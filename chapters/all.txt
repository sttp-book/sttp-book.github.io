# License

This book is licensed under [CC BY-NC-SA 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/). 
You are free to share, copy, and redistribute the material in any medium or format
as well as to adapt, remix, transform, and build upon the material under the following
terms:
(1) you must give appropriate credit, (2) you may not use the material for commercial purposes, (3) if you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original,
(4) you may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.

For commercial use, please contact us.

Main cover picture by [Agence Olloweb](https://unsplash.com/photos/d9ILr-dbEdg).
# Authors and Acknowledgements

This book is maintained by [Maurício Aniche](https://www.mauricioaniche.com), Assistant Professor in Software Engineering at Delft University of Technology.

_Dr. Maurício Aniche is an Assistant Professor in Software Engineering at Delft University of Technology, the Netherlands. His line of work focuses on how to make developers more productive during maintenance and testing. His research has been published in top-tier conferences (ICSE, FSE, ASE) and journals (TSE, EMSE). He always had a foot in industry. He worked as a software developer for VeriFone (3 years) and Locaweb (1 year), and as a software consultant and trainer for the group Caelum/Alura (6 years), one of the most respectable software engineering training centers in Brazil. He has given training and consultancy on software development and testing to 27 different companies, from 2010 to 2015. Moreover, he published three books focused on practitioners ("OOP and SOLID for ninjas", "Test-Driven Development in the real world", and "A Practical Guide on Software Testing"), which, altogether, have sold 10k copies. All these activities have given him a very particular vision on software engineering and testing should be done in practice._

This book would also not be possible without the help of many colleagues. A special thanks to:

* [Arie van Deursen](https://www.avandeursen.com), Full Professor in Software Engineering at Delft University of Technology, that has been giving CSE1110 for more than 10 years at Delft, and highly influenced me on how testing should be done. Arie is also responsible for the model-based testing and design-by-contracts chapter of this book.
* Dr. Annibale Panichella, my work colleague, for writing a chapter on mutation testing, and for providing me with great insights on AI for software testing.
* Azqa Nadeem, part of our CS teaching team, for writing the security testing, static testing, and fuzz testing chapters.
* Frank Mulder, part of our CS teaching team, for giving the book a full technical review, and for producing the web testing chapter.
* Wouter Polet, one of our many incredible TAs, for working on the initial transcripts of our lectures, and for working on this book's very first version.
* Dr. Steve Freeman, for the feedback he has provided to many parts of this book, especially related to interaction testing and mock objects. Steve's "Growing Object-Oriented Systems Guided by Tests" book has always been a great influence to me.
* Marsha Ginsberg, part of TU Delft staff, for giving the book a complete grammar check.
* The TU Delft's New Media Centre for patiently supporting me through recording, and for carefully editing the videos.
* For the many CSE1110 TAs that have helped us in improving the book. 
	* 2020 edition: Evaldas Latoškinas, Sára Juhošová, Dixit Sabharwal, Abel Mălan, Denise Toledo, Rembrandt Klazinga, Yoshi van den Akker.
* Finally, for our CSE1110 students that have fixed several typos, rephrased unclear paragraphs, proposed new exercises, and improve answers. The complete list of contributors can be found at https://github.com/sttp-book/sttp-book/graphs/contributors 

This book was also highly inspired by many books on software testing and software design. And while we cite references per chapter, in the following, we acknowledge the ones that inspired us the most:

* Pezzè, Mauro, and Michal Young. Software testing and analysis: process, principles, and techniques. John Wiley & Sons, 2008.
* Graham, Dorothy, Erik Van Veenendaal, and Isabel Evans. Foundations of software testing: ISTQB certification. Cengage Learning EMEA, 2008.
* Freeman, Steve, and Nat Pryce. Growing object-oriented software, guided by tests. Pearson Education, 2009.
* Langr, Jeff, Andy Hunt, and Dave Thomas. Pragmatic unit testing in Java 8 with JUnit. Pragmatic Bookshelf, 2015.
* Meszaros, Gerard. xUnit test patterns: Refactoring test code. Pearson Education, 2007.
* Kaner, Cem, Sowmya Padmanabhan, and Douglas Hoffman. The Domain Testing Workbook. Context Driven Press, 2013.
* Fowler, Martin. Testing bliki: https://martinfowler.com/tags/testing.html, Last access in 2020.
# Structure of the book

Testing software is a multifaceted activity, which requires developers to understand and apply a broad range of different techniques. This book is organised as follows:

* **Getting started with software testing**: Why is software testing such a challenging task? Why can't we find all the bugs in a software system? How expensive and/or feasible would it be if a developer decides to test all the possible scenarios in a software system? In this section, we explain to the reader the so-called _principles of software testing_. Moreover, we introduce the reader to the idea of automating the test cases with the help of JUnit, the standard Java testing framework. JUnit (and later other tools such as Mockito and Selenium) are used throughout this book. Finally, we convince the reader that, to achieve high-quality software systems, developers have to rely on both testing and design techniques (in what we call the _developer testing workflow_).

* **Testing techniques**: The main goal of software testing is to reveal bugs in the system under test. This section is entirely dedicated to techniques that aim at detecting those bugs as efficiently as possible. The chapters will cover specification-based techniques (i.e., deriving tests based on textual requirements of the system), structural techniques (i.e., deriving tests based on the structure of the source code), boundary testing (i.e., testing whether the system behaves correctly when inputs are near the boundaries of the input domain), model-based testing (i.e., deriving tests from more formal documentation, such as state machines or decision tables), and design-by-contracts and property-based testing (i.e., devising explicit contracts to methods and classes and ensuring that they behave correctly when these contracts are met).

* **Pragmatic software testing**: Testing becomes more complicated once the software systems under test get more complicated. And, on top of that, given how fast companies ship their software today, software testing is now a tool in the developers' belt. In this section, we cover techniques that help developers in writing their tests more effectively. Moreover, we discuss what design and architectural decisions developers can take to ease the testing process (the so-called design for testability). Finally, given test cases are implemented as code, we discuss best practices on how to write and maintain them.

* **Testing in context**: Context is king. In this chapter, we discuss some techniques and tools to test software systems in specific contexts, such as web applications and mobile applications. 

* **Non-functional testing**: Non-functional requirements are extremely important in some software systems. In this section, we cover different testing techniques to ensure that the security and performance requirements of the systems are met.

* **Intelligent testing**: Can machines also devise test cases? Up to this section, test cases were mostly devised by humans (and only later written as test code, so that the machine could run them). In this section, we discuss state-of-the-art ideas on how machines can also help developers in exploring systems and look out for crashes. We cover techniques such as mutation testing, fuzzing testing, and search-based software testing.

# Contribute to this book

We accept any kind of contribution, from typo fixes, rephrasing,
illustrations, videos, exercises,
code examples to full chapters. Just open a [Pull Request](https://github.com/sttp-book/sttp-book/pulls) in our [GitHub repository](https://github.com/sttp-book/sttp-book).
Or, if you feel you are not ready to contribute but want to see some content
here, just [open an issue](https://github.com/sttp-book/sttp-book/issues).

Note that everything you create will be published under the same [license](license.md) as the rest of the book. We will
add your name in the acknowledgements.
# Using this book

Feel free to use this book in your software testing course.
The slides we use in the background of our videos can be found
in our [video slides repository](https://github.com/sttp-book/video-slides).
We should release our accompanying lecture slides soon. _If you adopt
this book, please let us know!_

This book is currently being used by: 
* Delft University of Technology ([Dr. Maurício Aniche](https://www.mauricioaniche.com))
* University of Zurich ([Prof. Dr. Alberto Bacchelli](https://sback.it))
* PUC-RS/Brazil ([Prof. Dr. Bernardo Copstein](https://www.linkedin.com/in/bernardo-copstein-3226095))
# Mutation testing

*How do we know if we have tested enough?*

In the structural testing chapter, we discussed line coverage, branch coverage, and MC/DC.
In the model-based testing chapter, we discussed transition coverage and path coverage.
All these *adequacy criteria* measure how much of the program is exercised by the tests we devised.

However, these criteria alone might not be enough to determine the quality of the test cases.
In practice, we can exercise large parts of the system, while testing very little.

Consider the following class with a single method:

```java
public class Division {
  public static int[] getValues(int a, int b) {
    if (b == 0) {
      return null;
    }
    int quotient = a / b;
    int remainder = a % b;

    return new int[] {quotient, remainder};
  }
}
```

Imagine a tester writing the following test cases:

```java
@Test
public void testGetValues() {
  int[] values = Division.getValues(1, 1);
}

@Test
public void testZero() {
  int[] values = Division.getValues(1, 0);
}
```

While these tests give us 100% branch coverage, you probably also noticed that these tests will never fail as there are no **assertions**! 

## Fault Detection Capability

Let's discuss one more adequacy criterion: the **fault detection capability**.
It indicates the test's capability to reveal faults in the system under test.
The more faults a test can detect or, in other words, the more faults a test fails on, the higher its fault detection capability.
Using this criterion, we can indicate the quality of our test suite in a better way than with just the coverage metrics we have so far.

The fault detection capability does not just regard the amount of production code executed, but also the assertions made in the test cases.
For a test to be adequate according to this criterion, it has to have a meaningful **test oracle** (i.e., meaningful assertions).

The fault detection capability, as a test adequacy criterion, is the fundamental idea behind **mutation testing**.
In mutation testing, we change small parts of the code, and check if the tests can find the introduced fault.

In the previous example, we made a test suite that was not adequate in terms of its fault detection capability, 
because there were no assertions, and so the tests would never find any faults in the code.

Tests with assertions check if the result of the method is what we expect. We have a **test oracle** now.

```java
@Test
public void testGetValues() {
  int[] values = Division.getValues(1, 1);
  assertEquals(1, values[0]);
  assertEquals(0, values[1]);
}

@Test
public void testZero() {
  int[] values = Division.getValues(1, 0);
  assertNull(values);
}
```

To see how the values in a test case influence the fault detection capability, let's create two tests, where the denominator is not 0.

```java
@Test
public void testGetValuesOnes() {
  int[] values = Division.getValues(1, 1);
  assertEquals(1, values[0]);
  assertEquals(0, values[1]);
}

@Test
public void testGetValuesDifferent() {
  int[] values = Division.getValues(3, 2);
  assertEquals(1, values[0]);
  assertEquals(1, values[1]);
}
```

For the fault detection capability, we want to see if the tests detect any faults in the code.

This means we have to go back to the source code and introduce an error. 
We replace the division by a multiplication: a clear bug.

```java
public class Division {
  public static int[] getValues(int a, int b) {
    if (b == 0) {
      return null;
    }
    int quotient = a * b; // the bug was introduced here
    int remainder = a % b;

    return new int[] {quotient, remainder};
  }
}
```

If we run our tests with the buggy code, we see that the test with the values 1 and 1 (the `testGetValuesOnes()` test) still passes, but the other test (the `testGetValuesDifferent()`) fails.
This indicates that the second test has a higher fault detection capability.

Even though both tests exercise the method in the same way and execute the same lines, we see a difference in the fault detection capability. 
This is because of the different input values for the method and the different test oracles (assertions) in the tests. 
In this case, the input values and test oracle of the `testGetValuesDifferent()` test can detect the bug better.


{% set video_id = "QYbqz-gFWAk" %}
{% include "/includes/youtube.md" %}


## Hypotheses for Mutation Testing

The idea of mutation testing is to **assess the quality of the test suite**.
This is done by manipulating bits of the source code and running the tests with this manipulated source code.
If we have a good test suite, at least one of the tests will fail on this changed (i.e., buggy) code.
Following this procedure, we get a sense of the fault error capability of our test suite.

In mutation testing, we use **mutants**.
Mutants are modified programs which contain the defects, or faults that we introduce in the source code to be then used for determining the quality of the test suite.

A big question regarding the mutants is what their size should be.
We can change single operations, whole lines or even multiple lines of code.
What would work best?

Mutation testing and the answer to this question are based on the following two hypotheses:

* **The Competent Programmer Hypothesis (CPH)**:
Here, we assume that the program is written by a competent programmer.
More importantly, this means that given a certain specification, the programmer creates a program that is either correct, or it differs from a correct program by a combination of simple errors.

* **The Coupling Effect**:
The coupling effect hypothesis states that simple faults are coupled with more complex faults.
In other words, test cases that detect simple faults, will also detect complex faults.

Based on these two hypotheses, we can determine the size that the mutants should have.
Realistically, following the competent programmer hypothesis, the faults in the actual code will be small.
This indicates that the mutants' size should be small as well.
Considering the coupling effect, test cases that detect small errors will also be able to detect larger, more complex errors.


## Terminology

Before we talk about mutation testing in an in-depth way, we define some terms:

- **Mutant**: Given a program $$P$$, a mutant called $$P'$$ is obtained by introducing a *syntactic change* to $$P$$. A mutant is killed if a test fails when executed with the mutant.
- **Syntactic Change**: A small *change* in the code. Such a small change should make the code still valid, i.e., the code can still compile and run.
- **Change**: A change, or alteration, to the code that mimics typical human mistakes. We will see some examples of these mistakes later.

The example below illustrates mutation testing with these concepts.

Suppose we have a `Fraction` class with a method `invert()`.

```java
public class Fraction {
  private int numerator;
  private int denominator;

  // ...

  public Fraction invert() {
1.  if (numerator == 0) {
2.    throw new ArithmeticException("...");
    }
3.  if (numerator == Integer.MIN_VALUE) {
4.    throw new ArithmeticException("...");
    }
5.  if (numerator < 0) {
6.    return new Fraction(-denominator, -numerator);
    }
7.  return new Fraction(denominator, numerator);
  }
}
```

Here is a test suite for this method:

```java
@Test
public void testInvert(){
  Fraction f = new Fraction(1, 2);
  Fraction result = f.invert();
  assertEquals(2, result.getFloat(), 0.00001);
}

@Test
public void testInvertNegative(){
  Fraction f = new Fraction(-1, 2);
  Fraction result = f.invert();
  assertEquals(-2, result.getFloat(), 0.00001);
}

@Test
public void testInvertZero(){
  Fraction f = new Fraction(0, 2);
  assertThrows(ArithmeticException.class, () -> f.invert());
}

@Test
public void testInvertMinValue(){
  int n = Integer.MIN_VALUE;
  Fraction f = new Fraction(n, 2);
  assertThrows(ArithmeticException.class, () -> f.invert());
}
```

The test suite contains two tests for corner cases that throw an exception, and two "happy path" tests.
We will now determine the quality of our test suite using mutation testing.

First, we have to create a *mutant* by applying a *syntactic change* to the original method.
Keep in mind that, because of the two hypotheses, we want the syntactic change to be small: one operation/variable should be enough.
Moreover, the syntactic change is a *change*, hence it should mimic mistakes that could be made by a programmer.

For the first mutant we change line 6.
Instead of `-numerator` we just say `numerator`.

The mutant looks like as follows:

```java
public class Fraction {
  private int numerator;
  private int denominator;

  // ...

  public Fraction invert() {
1.  if (numerator == 0) {
2.    throw new ArithmeticException("...");
    }
3.  if (numerator == Integer.MIN_VALUE) {
4.    throw new ArithmeticException("...");
    }
5.  if (numerator < 0) {
      // the change was introduced here
6.    return new Fraction(-denominator, numerator);
    }
7.  return new Fraction(denominator, numerator);
  }
}
```

If we execute the test suite on this mutant, the `testInvertNegative()` test will fail, as `result.getFloat()` would be positive instead of negative.

Another mistake could be made in line 1.
When we studied boundary analysis, we saw that it is important to test the boundaries due to off-by-one errors.
We can make a syntactic change by introducing such an off-by-one error.
Instead of `numerator == 0`, in our new mutant we make it `numerator == 1`:

```java
public class Fraction {
  private int numerator;
  private int denominator;

  // ...

  public Fraction invert() {
1.  if (numerator == 1) {
2.    throw new ArithmeticException("...");
    }
3.  if (numerator == Integer.MIN_VALUE) {
4.    throw new ArithmeticException("...");
    }
5.  if (numerator < 0) {
6.    return new Fraction(-denominator, numerator);
    }
7.  return new Fraction(denominator, numerator);
  }
}
```

We see that, again, the test suite catches this error.
The test `testInvertZero()` will fail, as it expects an exception but none is thrown in the mutant. The test `testInvert()` will also fail since it has a 1 in the numerator which wrongly triggers an exception.


## Automation

Writing the mutations of our programs manually might take a lot of time. 
Moreover, we probably would only think of the cases that are already tested.
Like with test execution, we want to **automate the mutation process**.
There are various tools that generate mutants for mutant testing automatically, but they all use the same methodology.

First we need mutation operators.
A **mutation operator** is a grammatical rule that can be used to introduce a syntactic change.
This means that, if the generator sees a statement in the code that corresponds to the grammatic rule of the operator (e.g., `a + b`), then the mutation operator specifies how to change this statement with a syntactic change (e.g., turning it into `a - b`).

We can identify two categories of mutation operators:

- **Real fault based operators**: Operators that are very similar to defects seen in the past for the same kind of code. Such operators look like common mistakes made by programmers in similar code.
- **Language-specific operators**: Mutations that are made specifically for a certain programming language. For example, changes related to the inheritance feature we have in Java, or changes regarding pointer manipulations in C.

Most mutation testing tools include various basic mutation operators for real fault based operators.
Here are brief descriptions of some common mutation operators:

- **AOR - Arithmetic Operator Replacement**: Replaces an arithmetic operator by another arithmetic operator. Arithmetic operators are `+`, `-`, `*`, `/`, `%`.
- **ROR - Relational Operator Replacement**: Replaces a relational operator by another relational operator. Relational operators are `<=`, `>=`, `!=`, `==`, `>`, `<`.
- **COR - Conditional Operator Replacement**: Replaces a conditional operator by another conditional operator. Conditional operators are `&&`, `||`, `&`, `|`, `!`, `^`.
- **AOR - Assignment Operator Replacement**: Replaces an assignment operator by another assignment operator. Assignment operators include `=`, `+=`, `-=`, `/=`.
- **SVR - Scalar Variable Replacement**: Replaces each variable reference by another variable reference that has been declared in the code.


Here are examples for each of the mutation operators with first of all the original code, and then a mutant that could be produced by applying the mutation operator:

**Arithmetic Operator Replacement**

Original:
```java
int c = a + b;
```

Mutant example:
```java
int c = a - b;
```

**Relational Operator Replacement**

Original:
```java
if (c == 0) {
  return -1;
}
```

Mutant example:
```java
if (c > 0) {
  return -1;
}
```

**Conditional Operator Replacement**

Original:
```java
if (a == null || a.length == 0) {
  return new int[0];
}
```

Mutant example:
```java
if (a == null && a.length == 0) {
  return new int[0];
}
```

**Assignment Operator Replacement**

Original:
```java
c = a + b;
```

Mutant example:
```java
c -= a + b;
```

**Scalar Variable Replacement**

Original:
```java
public class Division {
  public static int[] getValues(int a, int b) {
    if (b == 0 || b == Integer.MIN_VALUE){
      return null;
    }

    int quotient = a / b;
    int remainder = a % b;

    return new int[] {quotient, remainder};
  }
}
```

Mutant example:
```java
public class Division {
  public static int[] getValues(int a, int b) {
    if (a == 0 || a == Integer.MIN_VALUE){
      return null;
    }

    int quotient = b / a;
    int remainder = quotient % a;

    return new int[] {remainder, a};
  }
}
```


In Java, there are a lot of language-specific operators.
We can, for example, change the inheritance of the class, remove an overriding method, or change some declaration types.
Without going into detail about these language-specific mutant operators, here are some examples:

- Access Modifier Change
- Hiding Variable Deletion
- Hiding Variable Insertion
- Overriding Method Deletion
- Parent Constructor Deletion
- Declaration Type Change

Of course, there are many more mutant operators that are used by mutant generators.
For now, you should at least have an idea what mutation operators are, how they work and what we can use them for.

{% set video_id = "KXQTWLyR5CA" %}
{% include "/includes/youtube.md" %}

## Mutation Analysis and Testing

Our goal is to use mutation testing to determine the quality of our test suite.
As we saw before, we have to create the mutants first and it is best to do this in an automated way with the help of mutation operators.
Then, we run the test suite against each of the mutants with an execution engine.
If any test fails when executed against a mutant, we say that the test suite kills the mutant.
This is good, because it shows that our test suite has some fault detection capability.
If none of the tests fail, the mutant stays alive.

This process is illustrated in the diagram below:

![Mutation Testing Process](img/mutation-testing/mutation_analysis_process.png)

When performing mutation testing, we count the number of mutants our test suite killed and the number of mutants that are still alive.
By counting the number of each of these mutant groups, we can give a value to the quality of our test suite.

We define the **mutation score** as:

$$\text{mutation score } = \frac{\text{killed mutants}}{\text{mutants}}$$

Assessing the quality of a test suite by computing its mutation score is called **mutation analysis**.

**Mutation testing** then means using this mutation analysis to improve the quality of the test suite by adding and/or changing test cases.

These concepts are highly interconnected.
Before mutation testing is carried out, you have to compute the mutation score.
This then indicates whether the test suite should be changed.
If the mutation score is low, there are a lot of mutants that are not killed by the test suite.
You then have to improve the test suite.

### Equivalent Mutants

Calculating the mutation score is challenging.
The mutation score increases when less mutants are alive.
This suggests that the best scenario is to have all the mutants killed by the test suite.
While this is indeed the best scenario, it is often unrealistic because some of the mutants may be impossible to kill.

Mutants that cannot be killed are called equivalent mutants.
An **equivalent mutant** is a mutant that always behaves in the same way as the original program.
If the mutant behaves like the normal code, it will always give the same output as the original program for any given input.
Clearly, this makes this mutant (which is basically the same program as the one under test) impossible to be killed by the tests.

Here, the equivalence is related to the definition of program equivalence.
Program equivalence roughly means that two programs are functionally equivalent when they produce the same output for every possible input.
This is also the equivalence between the normal code and an equivalent mutant.

Let's have a look at the following method (with some irrelevant parts left out).

```java
public void foo(int a) {
  int index = 10;
  while (...) {
    // ...
    index--;
    if (index == 0)
      break;
  }
}
```

Our mutation testing tool generates the following mutant using relational operator replacement:

```java
public void foo(int a) {
  int index = 10;
  while (...) {
    // ...
    index--;
    if (index <= 0)
      break;
  }
}
```

Note how the original code starts with `index` set to 10, decrements it at every iteration of the `while` loop, and breaks out of the loop when `index` equals 0.
The mutant works exactly the same, even though the condition is technically different.
The `index` is still decremented from 10 to 0.
Because `index` will never be negative, the `==` operator does the same as the `<=` operator.

The mutant produced by the generator is an equivalent mutant in this case.

Because of these equivalent mutants, we need to change the mutation score formula.
We do not want to take the equivalent mutants into account, as there is nothing wrong with the tests when they do not kill these mutants.

The new formula becomes:

$$\text{mutation score} = \frac{\text{killed mutants}}{\text{non-equivalent mutants}}$$

For the denominator, we used the number of non-equivalent mutants, instead of the total number of mutants.
To compute this new mutation score automatically, we need a way to determine automatically if a mutant is an equivalent mutant.
Unfortunately, we cannot do this automatically, as **detecting equivalent mutations is an undecidable problem**. We can never be sure that a mutant behaves the same as the original program for every possible input.

## Application

Mutation testing sounds like a great way to analyse and improve our test suites.
However, the question remains whether we can use mutation testing in practice.
For example, we can ask ourselves whether a test suite with a higher mutation score actually finds more errors.

A lot of research in software engineering tries to gain some insight into this problem.
So far all the studies about mutation testing have shown that mutants can provide a good indication of a test suite's fault detection capability, as long as the mutation operators are carefully selected and the equivalent mutants are removed.

More specifically, a study by Just et al. shows that mutant detection is positively correlated with real fault detection.
In other words, the more mutants a test suite detects, the more real faults the test suite can detect as well.
Even more interesting is that this correlation is independent of the coverage.
Furthermore, the correlation between mutant detection and fault detection is higher than the correlation between statement coverage and fault detection.
So, the mutation score provides a better measure for the fault detection capability than the test coverage.

## The costs of mutation testing

Mutation testing is not without its costs.
We have to generate the mutants, possibly remove the equivalent mutants, and execute the tests with each mutant.
This makes mutation testing quite expensive, i.e., it takes a long time to perform.

Assume we want to do some mutation testing.
We have:

- A code base with 300 Java classes
- 10 test cases for each class
- Each test case takes 0.2 seconds on average
- The total test execution time is then: $$300 \cdot 10 \cdot 0.2 = 600$$ seconds (10 minutes)

This execution time is for just the normal code.
For the mutation testing, we decide to generate on average 20 mutants per class.

We will have to execute the entire test suite of a class on each of the mutants.
Per class, we need $$20 \cdot 10 \cdot 0.2 = 40$$ seconds.
In total, the mutation testing will take $$300 \cdot 40 = 12000$$ seconds, or 3 hours and 20 minutes.

Because of this cost, researchers have tried to find ways to make mutation testing faster for a long time.
Based on some observations, they came up with the following heuristics.

* The first observation is that a test case can never kill a mutant if it does not cover the statement that changed (also known as the *reachability condition*).
Based on this observation, we only have to run the test cases that cover the changed statement.
This reduces the number of test cases to run and, with that, the execution time.
Furthermore, once a test case kills a mutant, we do not have to run the other test cases anymore.
This is because the test suite needs at least one test case that kills the mutant.
The exact number of test cases killing the same mutant does not really matter.

* A second observation is that mutants generated by the same operator and inserted at the same location are likely to be coupled with the same type of fault.
This means that when we use a certain mutation operator (e.g., Arithmetic Operator Replacement) and we replace the same statement with this mutant operator, we get two mutants that represent the same fault in the code.
It is then highly probable that if the test suite kills one of the mutants, it will also kill the others.
A heuristic that follows from this observation is to run the test suite against a subset of all the mutants (a technique also known as *do fewer*).
Obviously, when we run the test suite against a smaller number of mutations, the overall testing will take less time.
The simplest way of selecting the subset of mutants is by means of random sampling.
As the name suggests, we select random mutants to consider.
This is a very simple, yet effective way to reduce the execution time.

There are other heuristics to decrease the execution time, like e-selective or cluster mutants and operators, but we will not go into detail about those heuristics here.

{% hint style='tip' %}
Note that, for now, we talked about _first-order mutation_. Mutants are created by just a single mutation operator. In other words, each mutant contains just a single mistake. However, we can also consider _higher-order mutations_, where we apply more than a single mutation operator to generate a mutant exists. The idea is that, by combining different mutation operations, we end up with mutants that are harder to be killed.

For interested readers, we suggest the paper [_Higher Order Mutation Testing_](https://www.sciencedirect.com/science/article/abs/pii/S0950584909000688) by Jia and Harman.
{% endhint %}

## Tools

To perform mutation testing you can use one of many publicly available tools.
These tools are often made for specific programming languages.
One of the most mature mutation testing tools for Java is called [PIT](http://pitest.org).

PIT can be run from the command line, but it is also integrated into most popular IDEs (like Eclipse or IntelliJ).
Project management tools like Maven or Gradle can also be configured to run PIT.

As PIT is a mutation testing tool, it generates the mutants and runs the test suites against these mutants.
Then it generates easy to read reports based on the results.
In these reports, you can see the line coverage and mutation score per class.
Finally, you can also see more detailed results in the source code and check which individual mutants were kept alive. Try it out!

{% set video_id = "BEBhTtSZAlw" %}
{% include "/includes/youtube.md" %}

## Empirical studies

* [Parsai and Demeyer](https://link.springer.com/article/10.1007/s10009-020-00567-y) (2020): _We demonstrate that mutation coverage reveals additional weaknesses in the test suite compared to branch coverage and that it is able to do so with an acceptable performance overhead during project build._

## Exercises

**Exercise 1.**
"*Crimes* happen in a *city*. One way for us to know that the *police* is able to detect these *crimes* is to *simulate crimes* and see whether the *police* is able to detect them."

In the analogy above, we can replace crimes by bugs, city by software, and police by test suite. What should we replace **simulate crimes** by?

1. Mutation testing
1. Fuzzing testing
1. Search-based software testing
1. Combinatorial testing

**Exercise 2.**
Take a look at the method `min(int a, final int b, final int c)` from `org.apache.commons.lang3.math.NumberUtils`:

```java
    /**
     * <p>Gets the minimum of three {@code int} values.</p>
     *
     * @param a  value 1
     * @param b  value 2
     * @param c  value 3
     * @return  the smallest of the values
     */
    public static int min(int a, final int b, final int c) {
        if (b < a) {
            a = b;
        }
        if (c < a) {
            a = c;
        }
        return a;
    }
```

Which of the following mutation operators can be applied to the method in order to obtain a *mutant*?
1. Arithmetic Operator Replacement (`+`, `-`, `*`, `/`, `%`)
2. Relational Operator Replacement (`<=`, `>=`, `!=`, `==`, `>`, `<`)
3. Conditional Operator Replacement (`&&`, `||`, `&`, `|`, `!`, `^`)
4. Assignment Operator Replacement (`=`, `+=`, `-=`, `/=`)
5. Scalar Variable Replacement

Provide an upper-bound estimate for the number of possible mutants of the method above. Assume that our tool replaces every instance of an operator/variable independently.

{% hint style='working' %}
We need to develop more exercises for this chapter
{% endhint %}


## References

- Chapter 16 of the Software Testing and Analysis: Process, Principles, and Techniques. Mauro Pezzè, Michal Young, 1st edition, Wiley, 2007.
- Just, R., Jalali, D., Inozemtseva, L., Ernst, M. D., Holmes, R., & Fraser, G. (2014, November). Are mutants a valid substitute for real faults in software testing?. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (pp. 654-665). ACM.
# Intelligent testing

Up to this point, _automation_ meant the _automation of the text execution_. Once we manually devised the test cases, we wrote them down as JUnit tests, which were executed by the machine. 

While we studied several techniques to design test cases in a systematic manner, they all required some human effort. Our computers, however, can help a great deal in designing test cases. This is exactly what we are going to discuss in the following chapters.

We are going to work smarter rather than harder, and take a look at different testing techniques that perform software testing in an intelligent way:

- We first discuss **static testing**. In other words, how we can leverage program analysis to detect functional bugs and security vulnerabilities.

- We discuss **mutation testing**. Mutation testing helps us in measuring the fault capability detection of our test suites. After all, a test suite capable of detecting most of the bugs is better than a test suite that is only able to detect a few bugs.

- We discuss **fuzz testing**, where we will test the correctness of a program by automatically providing a (large number of) unexpected inputs. Fuzz testing is a common technique among security testers.

- Finally, in the **search-based testing** chapter, we will see how artificial intelligence or, more specifically, search algorithms, can help us in automatically devise test cases that fully cover the class under test.
# Search-based software testing

Imagine a computer program that looks at your code implementation and, based on what it sees, it automatically generates (JUnit) test cases that achieve 100% branch coverage. If this sounds like sci-fi to you, you should know it is not. In fact, automated test case generation tools are getting better by the day.

In this chapter, we will explore how it works. More specifically, we will discuss random and search-based test case generation. Finally, we will show you some tools. Note that _search-based software testing_ is a complex topic and requires an entire book on it. Here, we only illustrate the main ideas behind it.

## Random test case generation

Suppose you have the following `Triangle` program that identifies the type of the triangle, given the length of each side:

```java
public class Triangle {

    private final int a, b, c;

    enum TriangleType {
        EQUILATERAL, ISOSCELES, SCALENE
    }

    public Triangle(int a, int b, int c) {
        this.a = a;
        this.b = b;
        this.c = c;
    }

    // https://www.geeksforgeeks.org/program-to-find-the-type-of-triangle-from-the-given-coordinates/
    public TriangleType classify(int a, int b, int c) {
        if (a == b && b == c)
            return TriangleType.EQUILATERAL;
        else if (a == b || b == c)
            return TriangleType.ISOSCELES;
        else
            return TriangleType.SCALENE;
    }
}
```

For a program that automatically generate test cases, all it needs to do is:

1. Instantiate the class under test.
1. If the constructor has parameters, pass random values to it.
1. Invoke the method under test.
1. If the method has parameters, pass random values to it.
1. If the method returns a value, store it.
1. Check the output produced by the program and use it to write the assertion.
1. Measure the achieved (branch) coverage.
1. Repeat the procedure until the entire budget (e.g. a timeout) is consumed.

An example of a randomly generated test would be:

```java
@Test
void t1() {
    Triangle t = new Triangle(5, 7, 10);
    Triangle.TriangleType type = t.classify();
    assertThat(type).isEqualTo(Triangle.TriangleType.SCALENE);
}
```

Suppose the tool has a budget of 10 minutes, and stops afterwards. After 10 minutes, the tool might have generated hundreds (or even thousands) of test cases like that. Given that inputs are generated randomly, it is probable that the generated tests cover many branches of the program. 

A well-known tool (at least within the academic community) for Java programs that works precisely as described above is [Randoop](https://randoop.github.io/randoop/). According to its own manual: _Randoop can be used for two purposes: to find bugs in your program, and to create regression tests to warn you if you change your program's behavior in the future._ You can see the output of Randoop for the `Triangle` class here: https://github.com/sttp-book/code-examples/blob/master/src/test/java/tudelft/sbst/RegressionTest0.java.

## Search-based software testing

As you can imagine, randomly generating test cases might not work well (i.e. achieve high coverage) for complicated programs. Imagine that a specific branch of the program can only be achieved by a very specific input. Chances are that, randomly, that specific input will never be generated. Therefore, the questions is: _can we do better than random test case generation?_

One idea is to model the problem of "finding test cases" as an optimization problem. We will not go into details of what an optimization problem is, but let us use the `Triangle` as a case study again:

1. Suppose we are targeting the `classify()` method, and, more specifically, the `if (a == b || b == c)` branch.
1. Let us generate a set of random tests for the `classify()` method, similarly to what we have done above. Suppose the generated tests were T1=(`a=2`, `b=5`, `c=9`), T2=(`a=2`, `b=3`, `c=8`), up to T50.
1. Each of the randomly generated test cases would reach a different branch. If the target branch is exercised, the program simply returns the solution test case. If not, the program then analyzes the test cases in hands and how far they are from being a solution. If we look at T1, we see that, for `a==b`, we would need +3 in `a` or -3 in `b`; for `b==c`, we would need +4 in `b`, or -4 in `c`. If we look at T2, for `a==b`, we need +1 in `a`, or -1 in `b`. Note that T2 is almost there, but not yet! Also note that T2 is closer to be the test we are looking for than T1. Let us call _fitness_ the distance that the test is from being a solution.
1. The best test cases, i.e. the test cases closest to exercising the target branch, are then used to generate the "next generation" of test cases. That can be achieved by simple mutations in the values, e.g. adding a +1 in some of the inputs, and by crossover, e.g. getting the value of `a` from T1 and combining with the values of `b` and `c` of T2. 
1. The new test cases are then re-evaluated, i.e. their fitness is calculated. If a solution is found, the program stops. If not, it continues the search until the budget is exhausted.

Note that, little by little (or, after hundreds or even thousands of generations), the test cases get closer and closer to the target branch. For readers familiar with genetic algorithms, this is exactly what is going on here.

Clearly, there are several - theoretical and engineering - challenges to make all of this happen. And that is what software engineering researchers have been researching for many years in an area that is now known as _search-based software testing_. Note that search-based test case generation approaches are definitely more efficient than random generation, as it is able to use its budget more wisely! 

The most popular tool for search-based automated test case generation is [EvoSuite](http://www.evosuite.org).  

{% hint style='tip' %}
Currently, EvoSuite only works with Java 8 programs. Given that our code-examples use Java 11, we cannot easily run it there. EvoSuite developers are working hard to support newer versions of Java.
{% endhint %}

## The oracle problem

Determining whether the program produces the correct output for a given input is a hard problem. The problem has even a specific name: _the oracle problem_. 

Given that tools such as Randoop and EvoSuite cannot really know what the correct output is, it uses the output that the program gives as assertions. In this sense, tests that are produced by these tools do not reveal the functional bugs one would manually find. 

Nevertheless, we can assume other properties of the system that tools like EvoSuite can use to find a bug. For example, we can suppose that, whenever a given input makes a method to throw an Exception, that is a bug. Or whenever the system takes more than a specific number of seconds to respond, that is a bug! See [a list of errors that Randoop looks for](https://randoop.github.io/randoop/manual/index.html#kinds_of_errors): contracts over Object's equals(), hashCode(), clone(), toString(), Comparable's compare() and compareTo(), and repOk() to check the internal representation of the object (similarly to what we discussed in the design-by-contracts chapter).

## Automated test case generation in practice

Automated test case generation tools are getting more and more popular in industry. Facebook, for example, has been working on [Sapienz](https://engineering.fb.com/developer-tools/sapienz-intelligent-automated-software-testing-at-scale/), a search-based tool that looks for bugs/crashes in their mobile apps.

You can imagine such tools being plugged in continuous integration or nightly builds. Once the tool finds a crash, it reports back to developers, who then debug, fix the problem, and add the test case to the test suite of the program to ensure regressions will not happen. Automated test case generation tools are an interesting and complementary testing technique.

## Exercises

1. Run Randoop in any of the classes of our code examples repository. Check the `run-randoop.sh`.

## References

* Pacheco, Carlos, and Michael D. Ernst. "Randoop: feedback-directed random testing for Java." In Companion to the 22nd ACM SIGPLAN conference on Object-oriented programming systems and applications companion, pp. 815-816. 2007.

* Fraser, Gordon, and Andrea Arcuri. "Evosuite: automatic test suite generation for object-oriented software." In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering, pp. 416-419. 2011.

* Mao, Ke. Sapienz: Intelligent automated software testing at scale. https://engineering.fb.com/developer-tools/sapienz-intelligent-automated-software-testing-at-scale/
# Static testing

Static testing analyses the code characteristics without executing the application. It can be considered as an automated code review. It checks the style and structure of the code, and can be used to _statically_ evaluate all possible code paths in the System Under Test (SUT).
Static analysis can quickly find _low-hanging fruit_ bugs that can be found in the source code, e.g. using deprecated functions. Static analysis tools are scalable and generally require less time to set up. _PMD_, _Checkstyle_, _Checkmarx_ are some common static analysis tools.

The classical approach underlying static analysis is checking the code for potential structural and/or stylistic rule violations. A code checker typically contains a parser and an acceptable rule set. We look at the following techniques for static analysis:

1. Pattern matching via *Regular expressions*
2. Syntax analysis via *Abstract Syntax Trees*


## Pattern matching

Pattern matching is a code checking approach that searches for pre-defined patterns in code. A **Regular expression** or RegEx is a sequence of characters that represent a pattern. Simply put, a regex engine reads code character-by-character and upon every character-match, progresses through the regular expression until no more characters remain. Depending on the logic, either a positive/negative reaction is returned indicating whether the final state was an accepting/rejecting state, or all code snippets that matched the pattern are returned.

Regular expressions are usually represented using ***Finite State Automaton***. Each _node_ represents a state. We move from one state to the next by taking the _transition_ that matches the input symbol. Below you see a few examples of regular expressions and their corresponding finite state automata. The node with a black arrow is called the _starting state_. _Green states_ are accepting states. Any state other than accepting states is a rejecting state, e.g. _red_ and _gray states_ are rejecting states in the examples below. Note that while traversing the automaton, if the final state is not an accepting state, the string is considered rejected.

The automata for the regular expression '**bug**' is shown below. An input string `bug` will transition from left to right, until we end up in the green state. However, the string `bag` will move from first state to the second state, and then to the red state. Since there is no transition out of this state, we will stay here until the input finishes. Similarly, the string `bu` will also be rejected since its final state is not a green state.



![FSM for bug](img/static-testing/regex1.png)


Regex also have options to introduce wildcard characters. For example, the regular expression '**.\*bug**' results in the following automata. `.` denotes any possible character, and `*` denotes *0 or many times*. Hence, it accepts any string that ends in _bug_. The following strings will be accepted by this pattern: `bug`, `this is a bug`, `bad bug`, and `bugbug`. `bug!` will be rejected by this pattern. Note that this is a non-deterministic automata since there are two possible transitions for the symbol `b` in the first state.



![FSM for .*bug](img/static-testing/regex2.png)


The automata for '**.\*bug.\***' is given below. It will accept any string that contains `b`, `u`, `g` consecutively, at least once. In this case, even `bug!` will be accepted.


![FSM for .*bug.*](img/static-testing/regex3.png)

While regular expressions are a fast and powerful pattern matching technique, their biggest limitation is that they don't take semantics into account, ending up with many false positives. For example, consider the following code snippet. Suppose that the regular expression, `\s*System.out.println\(.*\);`, searches for all print statements in the code to remove them before deployment. It will find three occurrences in the code snippet, which are all FPs because the code is already disabled by a flag.

```java
boolean DEBUG = false;

if (DEBUG){
  System.out.println("Debug line 1");
  System.out.println("Debug line 2");
  System.out.println("Debug line 3");
}
```

## Syntax analysis

A more advanced code checking approach is syntax analysis. It works by deconstructing input code into a stream of characters, that are eventually turned into a Parse Tree. _Tokens_ are hierarchal data structures that are put together according to the code's logical structure.

![Parser pipeline](img/static-testing/lexer.png)


A **Parse Tree** is a concrete instantiation of the code, where each character is explicitly placed in the tree, whereas an **Abstract Syntax Tree (AST)** is an abstract version of the parse tree in which syntax-related characters, such as semi-colon and parentheses, are removed. An example of the AST of the code snippet above is given below.


![AST example](img/static-testing/ast-example.png)


A static analysis tool using syntax analysis takes as input (a) an AST, and (b) a rule-set, and raises an alarm in case a rule is violated.
For example, for a rule _allowing at most 3 methods_, and the following code snippet, the AST will be parsed and an error will be raised for violating the rule. Contrarily, a very complicated regular expression would be needed to handle the varying characteristics of the four available methods, potentially resulting in false positives.


![AST rule enforcement](img/static-testing/ast-usecase1.png)


Abstract Syntax Trees are used by compilers to find semantic errors &mdash; compile-time errors associated to the _meaning_ of the program. ASTs can also be used for program verification, type-checking, and translating the program from one language to another.  

## Performance of static analysis

 Typically, static analysis produces _sound_ results, i.e. zero false negatives. This is because the tools have access to the whole codebase, and they can track all the possible execution paths a program might take. So, if there are _any_ vulnerabilities, the tool should be able to find them. However, this comes at the cost of _Completeness_. Because it tracks all possible execution paths without seeing how the application behaves in action, some of the results might never be reached in an actual execution scenario, resulting in false positives.

 >Note that static analysis _can produce unsound results_ if a piece of code is added at runtime, because then the analysis will not be done on the new code-piece. This is one reason why the signatures that static analysis uses to detect bugs need to be kept up-to-date as the codebase evolves. However, this strategy will still not work for dynamically induced bugs.


{% hint style='tip' %} _Soundness_ and _Completeness_ are defined more extensively in the Security testing chapter. {% endhint %}

## Exercises

**Exercise 1.** Regular expressions _CANNOT DO_ which of the following tasks?
1. Matching patterns
2. Detect semantics
3. Define wild cards
4. Detect coding mistakes

**Exercise 2.** Given that a static analysis tool can view the entire codebase, what is the quality of results that the analysis will produce?
1. Sound and Complete
2. Sound but Incomplete
3. Unsound but Complete
4. Unsound and Incomplete

**Exercise 3.** Create an Abstract Syntax Tree for the following code snippet:
```java
(a + b) * (c - d)
```


## References

* Grune, D., Van Reeuwijk, K., Bal, H.E., Jacobs, C.J. and Langendoen, K., 2012. Modern compiler design. Springer Science & Business Media.
* Abstract Syntax Trees. https://en.wikipedia.org/wiki/Abstract_syntax_tree
* Sematic analysis. https://www.cs.usfca.edu/~galles/cs414S05/lecture/old2/lecture7.java.pdf
* Regular expressions in Java. https://www.tutorialspoint.com/java/java_regular_expressions.htm
# Fuzz testing

Fuzzing is a popular dynamic testing technique used for automatically generating complex test cases.
Fuzzers bombard the System Under Test (SUT) with randomly generated inputs in the hope to cause crashes.
A crash can either originate from *failing assertions*, *memory leaks*, or *improper error handling*.
Fuzzing has been successful in discovering [unknown bugs](https://lcamtuf.coredump.cx/afl/) in software.

{% hint style='tip' %}
Note that fuzzing cannot identify flaws that do not trigger a crash.
{% endhint %}

**Random fuzzing** is the most primitive type of fuzzing, where the SUT is considered as a completely black box, with no assumptions about the type and format of the input.
It can be used for exploratory purposes, but it takes a long time to generate any meaningful test cases.
In practice, most software takes some form of _structured input_ that is pre-specified, so we can exploit that knowledge to build more efficient fuzzers.


There are two ways of generating fuzzing test cases:

1. **Mutation-based Fuzzing** creates permutations from example inputs to be given as testing inputs to the SUT. These mutations can be anything ranging from *replacing characters* to *appending characters*. Since mutation-based fuzzing does not consider the specifications of the input format, the resulting mutants may not always be valid inputs. However, it still generates better test cases than the purely random case. _ZZuf_ is a popular mutation-based application fuzzer that uses random bit flipping. _American Fuzzy Lop (AFL)_ is a fast and efficient security fuzzer that uses genetic algorithms to increase code coverage and find better test cases.

2. **Generation-based Fuzzing**, also known as *Protocol fuzzing*, takes the file format and protocol specification of the SUT into account when generating test cases. Generative fuzzers take a data model as input that specifies the input format, and the fuzzer generates test cases that only alter the values while conforming to the specified format. For example, for an application that takes `JPEG` files as input, a generative fuzzer would fuzz the image pixel values while keeping the `JPEG` file format intact. _PeachFuzzer_ is an example of a generative fuzzer.

Compared to mutative fuzzers, generative fuzzers are _less generalisable_ and _more difficult to set up_ because they require input format specifications.
However, they produce higher-quality test cases and result in better code coverage.


## Maximising code coverage

One of the challenges of effective software testing is to generate test cases that not only _maximise the code coverage, but do so in a way that tests for a wide range of possible values_.
Fuzzing helps achieve this goal by generating wildly diverse test cases.
For example, [this blog post by Regehr](https://blog.regehr.org/archives/896) describes the use of fuzzing in order to optimally test an ADT implementation.

We want the fuzzer to generate these test cases in a reasonable time.
There are various ways in which we achieve maximal code coverage in less time:

1. Multiple tools
2. Telemetry as heuristics
3. Symbolic execution


### Multiple tools
A simple yet effective way to maximise code coverage is to use multiple fuzzing tools.
Each fuzzer performs mutations in a different way, so they can be run together to cover different parts of the search space in parallel.
For example, using a combination of a mutative and generative fuzzer can help to generate diverse test cases while also ensuring valid inputs.

### Telemetry as heuristics
If the code structure is known (i.e., in a white-box setting), telemetry about code coverage can help constrain the applied mutations.   
For example, for the `if` statement in the following code snippet, a heuristic based on ***branch coverage*** requires 3 test cases to fully cover it, while one based on ***statement coverage*** requires 2 test cases.
Using branch coverage ensures that all three branches are tested at least once.
While telemetry data does not directly help in generating valid test cases, it helps in selecting only those mutations that increase code coverage, for example. Code coverage metrics can also provide a stopping criteria for the fuzzer, reducing its run-time.

```java
public String func(int a, int b){
    a = a + b;
    b = a - b;
    String str = "No";
    if (a > 2)
        str = "Yes!";
    else if (b < 100)
        str = "Maybe!";
    return str;
}
```

### Symbolic execution
We can specify the potential values of variables that allow the program to reach a desired path, using so-called **symbolic variables**.
We assign symbolic values to these variables rather than explicitly enumerating each possible value.

We can then construct the formula of a **path predicate** that answers this question:
_Given the path constraints, is there any input that satisfies the path predicate?_.
We then only fuzz the values that satisfy these constraints.
A popular tool for symbolic execution is _Z3_.
It is a combinatorial solver that, when given path constraints, can find all combinations of values that satisfy the constraints.
The output of _Z3_ can be given as an input to a generative or mutative fuzzer to optimally test various code paths of the SUT.

The path predicate for the `else if` branch in the previous code snippet will be: $$((N+M \leq 2) \& (N < 100))$$. The procedure to derive it is as follows:

1. `a` and  `b` are converted into symbolic variables, such that their values are: $$a=N$$ and $$b=M$$.

1. The assignment statements change the values of `a` and `b` into $$a = N+M$$ and $$b = (N+M) - M = N$$.

1. The path constraint for the `if` branch is $$(N+M > 2)$$, so the constraint for the other branches will be its inverse: $$(N+M \leq 2)$$.

1. The path constraint for the `else if` branch is $$(N < 100)$$.

1. Hence, the final path predicate for the `else if` branch is the combination of the two: $$(N+M \leq 2) \& (N < 100)$$


Note that it is not always possible to determine the potential values of a variable because of the *halting problem* — answering whether a loop terminates with certainty is an undecidable problem. So, for a code snippet given below, Symbolic execution may give an imprecise answer.

```java
public void func(int a, boolean b){
    a = 2;
    while (b == true) {
        // some logic
    }
    a = 100;
}
```

{% hint style='tip' %}
For interested readers, we recommend the "fuzzing book": [https://www.fuzzingbook.org](https://www.fuzzingbook.org)!
{% endhint %}

## References

* Fuzzing for bug hunting. https://www.welcometothejungle.com/en/articles/btc-fuzzing-bugs
# Model-Based Testing
 
 
In software testing, a model is a simpler way to describe the program under test.
A model holds some of the attributes of the program for which the model is built.
Given that the model preserves some of the original attributes of the system under test, it can be used to analyse and test the system.
 
Why should we use models at all? A model gives us a structured way to understand how the program operates (or should operate).
Model-based testing makes use of models of the system to derive tests.
 
In this chapter, we show what a model is (or can be) and go over some of the models used in software testing.
The two models covered in this chapter are decision tables and state machines.
 
 
{% set video_id = "5yuFf4-4JnE" %}
{% include "/includes/youtube.md" %}
 
 
## Decision Tables
 
Decision tables are used to model how a combination of conditions should lead to a certain action.
These tables are easy to understand and can be validated by the client for whom the software is created.
Developers can use the decision tables to derive tests that verify the correct implementation of the requirements with respect to the conditions.
 
### Creating decision tables
 
A decision table is a table containing the conditions and the actions performed by the system based on these conditions. In this section, we discuss how to build them.
 
In general, a decision table looks like the following:
 
<table>
  <tr><th></th><th></th><th colspan="4">Variants</th></tr>
  <tr><td rowspan="2"><br><i>Conditions</i></td>
  <td>&lt;Condition1&gt;</td><td>T</td><td>T</td><td>F<br></td><td>F</td></tr>
  <tr><td>&lt;Condition2&gt;</td><td>T<br></td><td>F</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>&lt;Action&gt;</td><td>value1<br></td><td>value2</td><td>value3</td><td>value4</td></tr>
</table>
 
This table contains all the combinations of conditions explicitly.
Later we will look at ways to reduce the number of combinations in the table and in this way reduce the cost of testing all the combinations.
 
The selected conditions should always be independent of each other.
Also, in this type of decision table, the order of the conditions does not matter, e.g., making `<Condition2>` true and `<Condition1>` false or making `<Condition1>` false and after that `<Condition2>` true, should result in the same outcome.
(If the order *does* matter in some way, a state machine might be a better model. We cover state machines later in this chapter.)
 
Let us devise a first concrete decision table for an example program.
 
> **Requirement: Phone Subscription**
> 
> When choosing a phone subscription, there are a couple of options you could choose.
> Depending on these options a price per month is given.
> We consider the two options: international services and automatic renewal.
> International services increase the price per month.
> Automatic renewal decreases the price per month.
 
In the decision table, international services and
automatic renewal will be turned into conditions.
True then corresponds to a chosen option and false
corresponds to an option that is not chosen.
 
 
The decision table is as follows:
 
<table>
  <tr><th></th><th></th><th colspan="4">Variants</th></tr>
  <tr><td rowspan="2"><br><i>Conditions</i></td>
  <td>International</td><td>T</td><td>T</td><td>F<br></td><td>F</td></tr>
  <tr><td>Auto-renewal</td><td>T<br></td><td>F</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>price/month</td><td>30<br></td><td>32</td><td>10</td><td>15</td></tr>
</table>
 
You can see the different prices for the combinations of international services and automatic renewal.
 
 
 
**Don't care values:** In some cases, the value of a condition might not influence the action.
This is represented as a "don't care" value, or "dc".
 
Essentially, "dc" is a combination of two columns.
These two columns have the same values for the conditions and the same result, except the condition that had the dc value which has different values in the expanded form.
 
If the decision table is as follows:
 
<table>
  <tr><th></th><th></th><th colspan="3">Variants</th></tr>
  <tr><td rowspan="2"><br><i>Conditions</i></td>
  <td>&lt;Condition1&gt;</td><td>T</td><td>dc</td><td>F<br></td></tr>
  <tr><td>&lt;Condition2&gt;</td><td>dc</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>&lt;Action&gt;</td><td>value1<br></td><td>value1</td><td>value2</td></tr>
</table>
 
It can be expanded to:
 
<table>
  <tr><th></th><th></th><th colspan="5">Variants</th></tr>
  <tr><td rowspan="2"><br><i>Conditions</i></td>
  <td>&lt;Condition1&gt;</td><td>T</td><td>T</td><td>T<br></td><td>F</td><td>F</td></tr>
  <tr><td>&lt;Condition2&gt;</td><td>T<br></td><td>F</td><td>T</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>&lt;Action&gt;</td><td>value1<br></td><td>value1</td><td>value1</td><td>value1</td><td>value2</td></tr>
</table>
 
After expanding we can remove the duplicate columns.
We end up with the decision table below:
 
<table>
  <tr><th></th><th></th><th colspan="4">Variants</th></tr>
  <tr><td rowspan="2"><br><i>Conditions</i></td>
  <td>&lt;Condition1&gt;</td><td>T</td><td>T</td><td>F<br></td><td>F</td></tr>
  <tr><td>&lt;Condition2&gt;</td><td>T<br></td><td>F</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>&lt;Action&gt;</td><td>value1<br></td><td>value1</td><td>value1</td><td>value2</td></tr>
</table>
 
Let us now go back to the *Phone Subscription* example, and add another condition to it:
 
> A loyal customer receives the same discount as a
> customer who chooses the automatic renewal option.
> However, a customer is only entitled to the discount from one of the two options.
 
The new decision table is below:
 
<table>
  <tr><th></th><th></th><th colspan="6">Variants</th></tr>
  <tr><td rowspan="3"><br><i>Conditions</i></td>
  <td>International</td><td>F</td><td>F</td><td>F</td><td>T</td><td>T</td><td>T</td></tr>
  <tr><td>Auto-renewal</td><td>T</td><td>dc</td><td>F</td><td>T</td><td>dc</td><td>F</td></tr>
  <tr><td>Loyal</td><td>dc</td><td>T</td><td>F</td><td>dc</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>price / month</td><td>10</td><td>10</td><td>15</td><td>30</td><td>30</td><td>32</td></tr>
</table>
 
Note that when automatic renewal is true, the loyalty condition does not change the outcome and vice versa.
 
 
**Default behaviour**: Usually, $$N$$ conditions lead to $$2^N$$ combinations or columns.
However, the number of columns that are specified in the decision table can be smaller. Even if we expand all the dc values.
 
This is achieved by using a default action.
A default action means that if a combination of condition outcomes is not present in the decision table, the default action should be the result.
 
If we set the default charge rate to 10 per month the new decision table can be a bit smaller:
<table>
  <tr><th></th><th></th><th colspan="4">Variants</th></tr>
  <tr><td rowspan="3"><br><i>Conditions</i></td>
  <td>International</td><td>F</td><td>T</td><td>T</td><td>T</td></tr>
  <tr><td>Auto-renewal</td><td>F</td><td>T</td><td>dc</td><td>F</td></tr>
  <tr><td>Loyal</td><td>F</td><td>dc</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>price / month</td><td>15</td><td>30</td><td>30</td><td>32</td></tr>
</table>
 
 
{% set video_id = "1u1qfJ2IrpU" %}
{% include "/includes/youtube.md" %}
 
 
### Testing decision tables
 
We can derive tests based on the decision tables, in such a way that we test whether the expected logic is implemented correctly.
There are multiple ways to derive tests for a decision table:
 
- **All explicit variants:** Derive a test case for each column. The number of tests equals the number of columns in the decision table.
- **All possible variants:** Derive a test case for each possible combination of condition values. For $$N$$ conditions this leads to $$2^N$$ test cases. Often, this approach is unrealistic because of the exponential relationship between the number of conditions and the number of test cases.
- **Every unique outcome / All decisions:** One test case for each unique outcome or action. The number of tests depends on the actions in the decision table.
- **Each condition T/F:** Make sure that each condition is true and false at least once in the test suite. This often results in two tests: all conditions true and all conditions false.
 
One more way to derive test cases from a decision table is by using the **Modified Condition / Decision Coverage (MC/DC)**.
This is a combination of the last two ways of deriving tests demonstrated above.
 
We have already discussed MC/DC in the Structural-Based Testing chapter.
MC/DC has the two characteristics of All decisions and Each condition T/F with an additional characteristic that makes MC/DC special:
 
1. Each condition is at least once true and once false in the test suite;
2. Each unique action should be tested at least once;
3. Each condition should individually determine the action or outcome.
 
The third point is achieved by making two test cases for each condition.
In these two test cases, the condition being tested should have a different value, the outcome should be different, and the other conditions should have the same value in both test cases.
With this the condition that is being tested individually influences the outcome, as the other conditions stay the same and therefore do not influence the outcome.
 
By choosing the test cases efficiently, MC/DC needs fewer tests than all variants, while still exercising the important parts of the system.
With fewer tests it will take less time to write the tests and the test suite will be executed quicker.
 
 
In order to derive the tests, we expand and rearrange the decision table of the *Phone Subscription* example as follows:
 
<table>
  <tr><th></th><th></th><th>v1</th><th>v2</th><th>v3</th><th>v4</th><th>v5</th><th>v6</th><th>v7</th><th>v8</th></tr>
  <tr><td rowspan="3"><br><i>Conditions</i></td>
  <td>International</td><td>T</td><td>T</td><td>T</td><td>T</td><td>F</td><td>F</td><td>F</td><td>F</td></tr>
  <tr><td>Auto-renewal</td><td>T</td><td>T</td><td>F</td><td>F</td><td>T</td><td>T</td><td>F</td><td>F</td></tr>
  <tr><td>Loyal</td><td>T</td><td>F</td><td>T</td><td>F</td><td>T</td><td>F</td><td>T</td><td>F</td></tr>
  <tr><td><i>Action</i></td><td>price / month</td><td>30</td><td>30</td><td>30</td><td>32</td><td>10</td><td>10</td><td>10</td><td>15</td></tr>
</table>
 
First, we look at the first condition (v1) and we try to find pairs of combinations that would cover this condition according to MC/DC.
We look for combinations where only International Services and the price/month change.
The possible pairs are: {v1, v5}, {v2, v6}, {v3, v7} and {v4, v8}.
<!-- TODO FM: What do we mean by this sentence? Is the sentence correctly formulated now?
 Before Marsha's change, this was "Testing both combinations of any of these pairs would give MC/DC for the first condition." -->
Testing both of the combinations in any of these pairs would give MC/DC for the first condition.
 
Then with Automatic Renewal we find these pairs: {v2, v4}, {v6, v8}.
For this condition {v1, v3} and {v5, v7} are not viable pairs, because both combinations of each pair involve the same action.
 
The last condition, Loyal, gives the following pairs: {v3, v4}, {v7, v8}.
 
By choosing the test cases efficiently we should be able to achieve full MC/DC by choosing four of the combinations.
We want to cover all the actions in the test suite.
Therefore, we need at least v4 and v8.
With these decisions we also have covered the International Services condition.
We need one of v1, v2, v3 and one of v5, v6, v7.
To cover Loyal, we add v7 and to cover Automatic Renewal we add v2.
All the possible actions have now been covered.
 
For full MC/DC, we test the decisions: v2, v4, v7, v8.
 
 
{% set video_id = "TxAFPJx6yKI" %}
{% include "/includes/youtube.md" %}
 
 
### Implementing automated test cases for decision tables
 
Now that we know how to derive the test cases from the decision tables, it is time to implement them as automated test cases.
 
The most obvious way to test the combinations is to create a single test for each of the conditions.
Let us continue with the example for which we created the decision table earlier.
We write the tests for combinations v2 and v3.
Assuming that we can use some implemented methods in a `PhonePlan` class, the tests look like this:
 
```java
@Test
public void internationalAutoRenewalTest() {
  PhonePlan plan = new PhonePlan();
 
  plan.setInternational(true);
  plan.setAutoRenewal(true);
  plan.setLoyal(false);
 
  assertEquals(30, plan.pricePerMonth());
}
 
@Test
public void internationalLoyalTest() {
  PhonePlan plan = new PhonePlan();
 
  plan.setInternational(true);
  plan.setAutoRenewal(false);
  plan.setLoyal(true);
 
  assertEquals(30, plan.pricePerMonth());
}
```
 
{% hint style='tip' %}
Note that, in order to improve readability, we use setters to define whether a plan is for international services, automatic renewal or loyalty. The best way of designing the `PhonePlan` class would be to receive such parameters in the constructor.
This would force all clients of the class to provide these configurations at the moment of instantiation, preventing instances of `PhonePlan` to be in an inconsistent state.
 
We discuss more about such contracts in the design-by-contracts chapter.
{% endhint %}
 
 
In the example above, it should be noted that the different tests for the different combinations are similar.
The tests are the same but they operate on different values.
To avoid the code duplication that comes with this approach to implementing decision table tests, we can use parameterized tests (as we have done before):
 
```java
@ParameterizedTest
@CsvSource({
  "true, true, false, 30",   // v2
  "true, false, true, 30",   // v3
  "true, false, false, 32",  // v4
  "false, false, false, 15"  // v8
})
public void pricePerMonthTest(boolean international, boolean autoRenewal,
    boolean loyal, int price) {
  
  PhonePlan plan = new PhonePlan();
 
  plan.setInternational(international);
  plan.setAutoRenewal(autoRenewal);
  plan.setLoyal(loyal);
 
  assertEquals(price, plan.pricePerMonth());
}
```
 
{% set video_id = "tzcjDhdQfvM" %}
{% include "/includes/youtube.md" %}
 
### Non-binary choices and final guidelines

So far, all the decision tables we have discussed have boolean conditions (i.e., values in the cells were either `true` or `false`).
The decision tables can be generalized to contain non-boolean values (non-binary choices). In other words, instead of only `true` and `false`, the values can now also be, e.g., a number.

Take the decision table for the phone subscriptions, discussed earlier.
Initially, this only contained the international and auto-renewal conditions.
A non-boolean can be introduced to represent a data limit.
The amount of data can be unlimited (&infin;), 8 GB, or none (0 GB).

<table>
  <tr><th></th><th></th><th colspan="6">Variants</th></tr>
  <tr><td rowspan="3"><br><i>Conditions</i></td>
  <td>International</td><td>T</td><td>T</td><td>F<br></td><td>F</td><td>F</td><td>F</td></tr>
  <tr><td>Auto-renewal</td><td>T<br></td><td>F</td><td>T</td><td>T</td><td>F</td><td>F</td></tr>
  <tr><td>Data limit</td><td>&infin;</td><td>&infin;</td><td>8</td><td>0</td><td>8</td><td>0</td></tr>
  <tr><td><i>Action</i></td><td>price / month</td><td>32<br></td><td>30</td><td>12</td><td>10</td><td>17</td><td>15</td></tr>
</table>

With two boolean conditions and one condition with three possible values, the possible number of variants is $$2 \cdot 2 \cdot 3 = 12$$.
The decision table only shows $$6$$.
The unspecified columns implicitly represent invalid combinations.
For example, an international subscription without data would not be possible.

In small decision tables, non-boolean conditions can be useful.
However, with many non-boolean conditions, the amount of possible combinations in the decision table can be large.
Due to this combinatorial explosion of possible variants, the decision table becomes very hard to work with.
For example, a decision table with three conditions, each having $$4$$ possible values, results in $$4 \cdot 4 \cdot 4 = 64$$ possible combinations.
In such cases, other strategies like _pair-wise combinatorial testing_ offer better scalability. 

The testing techniques for decision tables with only boolean conditions can also be generalized to tables with non-boolean conditions.
For the MC/DC criteria (discussed in the structural testing chapter), this requires modifying one of its three properties: instead of making each condition `true` and `false` at least once in the test suite, each of its possible values should appear at least once in the test suite.

Finally, these are some general guidelines to keep in mind when designing decision tables:

1. **Keep conditions independent.** The order of the conditions should not matter, otherwise a state machine might be more fitting.
2. **Use DC values when possible.** This decreases the amount of variants, making the decision table easier to read and easier to understand.
3. **Variants with DC values should not overlap.** If they do, they should at least have the same action.
4. **Add a default column.** This provides an action for the variants that are not in the decision table, allowing the decision table to not include all possible variants.
5. **Consider non-boolean conditions if conditions are mutually exclusive.** Mutually exclusive means that the conditions can never be true at the same time. These conditions then probably encode a single non-boolean condition.
6. **If most conditions are non-boolean, consider using a different combinatorial testing technique instead**. One example would be pair-wise testing.

{% set video_id = "RHB_HaGfNjM" %}
{% include "/includes/youtube.md" %}
 
 
 
 
## State Machines
 
A state machine is a model that describes the software system by describing its states.
A system often has multiple states and various transitions between these states.
The state machine model uses these states and transitions to illustrate the system's behaviour.
 
The main focus of a state machine is, as the name suggests, the states of a system.
So, it is useful to think about what a state actually is.
The states in a state machine model describe where a program is in its execution. 
Transitions are actions that take the system from one state to another.
We can use as many states as we need to describe the system's behaviour well.
 
Besides states and transitions, a state machine has an initial state and events.
The initial state is the state that the system starts in.
From that state the system can transition to other states.
Each transition is paired with an event.
This event is usually one or two words that describe what has to happen to make the transition.
 
There are some agreements on how to make the models.
The notation we use is the Unified Modelling Language, UML.
For the state diagrams that means we use the following symbols:
 
- State: ![](img/model-based-testing/uml/state_symbol.svg)
- Transition: ![](img/model-based-testing/uml/transition_symbol.svg)
- Event: ![](img/model-based-testing/uml/event_symbol.svg)
- Initial state: ![](img/model-based-testing/uml/initial_state_symbol.svg)
 
For the following examples we model a (part of a) phone.
We start simply with a state machine that models the phone's ability to be locked or unlocked.
 
A phone that can be either locked or unlocked has two states: locked and unlocked.
Before there were face recognition and fingerprint sensors, you had to enter a password to unlock the phone.
A correct password unlocks the phone and if an incorrect password is given, the phone stays locked.
Finally, an unlocked phone can be locked again by pushing the lock button.
We can use these events in the state machine.
 
![](img/model-based-testing/examples/locked_unlocked_machine.svg)
 
In the diagram the initial state is `LOCKED`, because usually when someone starts using their phone, it is locked.
This should be represented in the initial state of the state machine.
 
Sometimes an event can lead to multiple states, depending on a certain condition.
To model this in the state machines, we use *conditional transitions*.
These transitions are only performed if the event happens and if the condition is true.
The conditions often depend on a certain value being used in the state machine.
To modify these values when a transition is taken in the state machine, we use actions.
Actions are associated with a transition and are performed when the system uses that transition to go into another state.
The notation for conditions and actions is as follows:
 
- Conditional transition: ![](img/model-based-testing/uml/conditional_symbol.svg)
- Action: ![](img/model-based-testing/uml/action_symbol.svg)
 
 
When a user types the wrong password four times in a row, the phone becomes blocked.
We use `n` in the model to represent the number of failed attempts.
Let us look at the conditional transitions that we need to model this behaviour.
 
![](img/model-based-testing/examples/blocked_condition_machine.svg)
 
When `n` (the number of failed unlock attempts) is smaller than 3, the phone stays in `LOCKED` state.
However, when `n` is equal to 3, the phone goes to `BLOCKED`.
Here we have an event, wrong password, that can lead to different states based on the condition.
 
In the previous state machine, `n` never changes.
This means that the phone will never go to its `BLOCKED` state, as that requires `n` to be equal to 3.
We can add actions to the state machine to make `n` change correctly.
 
![](img/model-based-testing/examples/blocked_complete_machine.svg)
 
These added actions are to set `n` to `n+1` when an incorrect password is given and to 0 when a correct password is given.
This way the state machine will be in the `BLOCKED` state when a wrong password is given four times in a row.
 
 
{% set video_id = "h4u9k-P3W0U" %}
{% include "/includes/youtube.md" %}
 
 
{% set video_id = "O1_oC-7I5E4" %}
{% include "/includes/youtube.md" %}
 
 
### Testing state-machines
 
Like with the decision tables, we want to use the state machine model to derive tests for the software system.
 
First of all, we look at what might be implemented incorrectly.
An obvious potential error is a transition to the wrong state.
This will cause the system to act incorrectly, so we want the tests to catch such errors.
Additionally, the conditions in conditional transitions and the actions in transition can be wrong.
Finally, the behaviour of a state should stay the same at all times.
This means that moving from and to a state should not change the behaviour of that state.
 
For state machines, we have a couple of test coverages.
In this chapter we go over three main ways to define test coverage:
 
- **State coverage:** each state has to be reached at least once
- **Transition coverage:** each transition has to be exercised at least once
- **Paths:** not exactly a way of describing test coverage, but we use paths to derive test cases
 
To achieve the state coverage, we generally bring the system into a state through transitions and then assert that the system is in that state.
To test a single transition (for transition coverage), more steps are needed:
 
1. Bring the system into the state that the transition being tested goes out of;
2. Assert that the system is in that state;
3. Trigger the transition event;
4. If there is an action, check if this action has happened;
5. Assert that the system is in the new state that the transition points to.
 
 
To achieve full state coverage, we need to arrive in each state once.
For the phone example we have three states so we can make three tests.
 
- Check that the system is `LOCKED` when it is started;
- Give the correct password and check that the system is `UNLOCKED`;
- Give an incorrect password four times and check that the system is `BLOCKED`.
 
With these three tests we achieve full state coverage, as the system is in each state at a certain point.
 
With the tests above, we have covered most of the transitions as well.
The only untested transition is the `lock button` from `UNLOCKED` to `LOCKED`.
To test this transition, we make the system `UNLOCKED` by giving the correct password.
Then we trigger the `lock button` and assert that the system is `LOCKED`.
 
 
### Paths and Transition trees
 
Besides the individual transitions, we can also test the combinations of transitions.
These combinations of transitions are called paths.
 
A logical thought might be: let's test all the paths in the state machine.
While this looks like a good objective, the number of paths will probably be too high.
Take a state machine that has a loop, i.e., a transition from state X to Y and a transition from state Y to X.
When creating paths, we can keep going back and forth between these two states.
This leads to an infinite number of paths.
Obviously, we cannot test all the paths, so we need to take a different approach.
 
The idea is that when using paths to derive test cases, we want each loop to be executed once.
This way we have a finite number of paths to be tested.
We derive these tests by using a transition tree, which spans the graph of the state machine.
Such a transition tree is created as follows:
 
1. The root node is the initial state of the state machine;
2. For each of the nodes at the lowest level of the transition tree:
  - If the state that the node corresponds to has not been covered before:
    For each of the outgoing transitions of this node's state:
      Add a child node that has the name of the state the transition points to. If this state is already in the tree, add or increment a number after the state's name to keep the node unique.
  - If any nodes were added: 
     Repeat from step 2. <!-- TODO: Marsha: this is already step 2. I find this whole point from point 1 not clear! -->
 
This is demonstrated in the example below.
 
To make the transition tree more interesting, we modify the phone's state machine to have an `OFF` state instead of a `BLOCKED` state.
See the state machine below:
 
![Phone state machine with off](img/model-based-testing/examples/phone_off_machine.svg)
 
The root node of the transition tree is the initial state of the state machine.
We append a number to make it easier to distinguish this node from other nodes of the same state.
 
![Root node transition tree](img/model-based-testing/examples/transition_tree/transition_tree_0.svg)
 
For each outgoing transition from the `OFF` state we add a child node to `OFF_0`.
 
![Two level transition tree](img/model-based-testing/examples/transition_tree/transition_tree_1.svg)
 
One node was added, so we continue by adding children to that node.
 
![Three level transition tree](img/model-based-testing/examples/transition_tree/transition_tree_2.svg)
 
The only state we have not seen yet is `UNLOCKED` in the `UNLOCKED_0` node.
Therefore, this is the only node we should add children to.
 
![Final phone transition tree](img/model-based-testing/examples/transition_tree/transition_tree_3.svg)
 
Now all the states of the nodes in the lowest layer have been visited so the transition tree is done.
 
 
 
From a complete transition tree, we can derive tests.
Each leaf node in the transition tree represents one path to test.
This path is shown by going from the root node to this leaf node.
In the tests, we assume that we are starting in the correct state.
Then, we trigger the next event that is needed for the given path and assume that we are in the next correct state.
These events that we need to trigger can be found in the state machine.
This process continues until the whole path has been followed.
 
 
In the transition tree of the previous example there are four leaf nodes: `OFF_1`, `OFF_2`, `LOCKED_1`, `LOCKED_2`.
We want a test for each of these leaf nodes, that follows the path leading to that node.
For `OFF_1` the test should 'move' the system from `OFF` to `LOCKED` and back to `OFF`.
Looking at the state machine this gives the events `home`, `long lock button`.
In the test we check that the system is `OFF`, then we trigger `home` and check that the system is `LOCKED`, then we trigger `long lock button` and assert that the system is `OFF`.
 
The tests for the other three paths can be derived in a similar fashion.
 
 
 
Using the transition tree, each loop that is in the state machine is executed once and while testing most of the important paths in the state machine, the number of tests is manageable.
 
{% set video_id = "pvFPzvp5Dk0" %}
{% include "/includes/youtube.md" %}
 
 
### Sneak paths and transition tables
 
In the previous section, we discussed transition trees and how to use them to derive tests.
These tests check if the system behaves correctly when following different paths in the state machine.
With this way of testing, we check if the existing transitions in a state machine behave correctly.
However, we do not check whether there are any more transitions; transitions that should not be there.
We call these paths, "sneak paths".
 
A **sneak path** is a path in the state machine that should not exist.
So, for example, we have state X and Y and the system should not be able to transition directly from X to Y.
If the system can in some way transition directly from X to Y, we have a sneak path.
We need to test to see if such sneak paths exist in the system.
To achieve this, we make use of transition tables.
 
A transition table is a table containing each transition that is in the state machine.
The transition is given by the state it is going out of, the event that triggers the transition, and the state the transition goes to.
 
 
We construct a transition table as follows:
 
- List all the state machine's states along the rows;
- List all events along the columns;
- For each transition in the state machine note its destination state in the correct cell of the transition table.
 
 
We take a look at the same state machine for which we created a transition table:
![](img/model-based-testing/examples/phone_off_machine.svg)
 
To make the transition table we list all the states and events in the table:
 
<table>
  <tr>
    <td>STATES</td>
    <td colspan="5">Events</td>
  </tr>
  <tr>
    <td></td>
    <td>home</td>
    <td>wrong password</td>
    <td>correct password</td>
    <td>lock button</td>
    <td>long lock button</td>
  </tr>
  <tr>
    <td>OFF</td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>LOCKED</td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>UNLOCKED</td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
</table>
 
Then we fill the table with the states that the transitions in the state machine point to:
 
<table>
  <tr>
    <td>STATES</td>
    <td colspan="5">Events</td>
  </tr>
  <tr>
    <td></td>
    <td>home</td>
    <td>wrong password</td>
    <td>correct password</td>
    <td>lock button</td>
    <td>long lock button</td>
  </tr>
  <tr>
    <td>OFF</td>
    <td>LOCKED</td>
    <td></td>
    <td></td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <td>LOCKED</td>
    <td></td>
    <td>LOCKED</td>
    <td>UNLOCKED</td>
    <td></td>
    <td>OFF</td>
  </tr>
  <tr>
    <td>UNLOCKED</td>
    <td></td>
    <td></td>
    <td></td>
    <td>LOCKED</td>
    <td>OFF</td>
  </tr>
</table>
 
We can see that there is, for example, a transition from `UNLOCKED` to `LOCKED` when the event `lock button` is triggered.
 
When we have the transition table, we have to decide the intended behaviour for the cells that are empty.
The default is to ignore the event and stay in the same state.
In some cases, one might want the system to throw an exception.
These decisions depend on the project and the customer's needs.
 
As discussed earlier, we can use the transition table to derive tests for sneak paths.
Usually, we want the system to remain in its current state when we trigger an event that has an empty cell in the transition table.
To test for all possible sneak paths, we create a test case for each empty cell in the transition table.
First of all, the test will bring the system to the state corresponding to the empty cell's row (you can use the transition tree to find a suitable path). Then the test triggers the event that corresponds to the empty cell's column. Finally, the test asserts that the system is in the same state as before triggering the event.
The number of 'sneak path tests' is equal to the number of empty cells in the transition table.
 
With these tests we can verify both existing and non-existing paths.
Together these techniques produce a good testing suite from a state machine.
 
{% set video_id = "EMZB2IZT8WA" %}
{% include "/includes/youtube.md" %}
 
 
 
### Super states and regions
 
Up till now, we have looked at simple and small state machines.
When the modelled system becomes large and complex, it is typical for the state machine to develop in the same way.
At some point the state machine will consist of so many states and transitions, that it becomes unclear and impractical to manage.
To resolve this issue and make a state machine more scalable we use **super states** and **regions**.
 
**Super state**: A super state is a state that consists of a state machine.
Basically, we wrap a state machine in a super-state which is used as a state in another state machine.
 
The notation of the super-state is as follows:
 
![Super state notation](img/model-based-testing/uml/super_state_symbol.svg)
 
As a super state is, in essence, a state machine that can be used as a state, we know what should be inside the super state.
The super state generally consists of multiple states and transitions, and it always has to have an initial state.
Any transition going into the super state goes to the initial state of the super state.
A transition going out of the super state means that if the event on this transition is triggered in any of the super state's states, the system transitions into the state this transition points to.
 
We can choose to show the super state fully or we can collapse it.
A collapsed super state is just a normal state in the state machine.
This state has the super state's name and the same incoming and outgoing transitions as the super state.
 
With the super states and the collapsing of super states we can modularise and combine state machines.
This allows us to shift the state machine's focus to different parts of the system's behaviour.
 
 
We can use a super state even in the small example of a phone's state machine.
The two states `LOCKED` and `UNLOCKED` both represent the system in some sort of `ON` state.
We can use this to create a super state called `ON`.
 
![Super state example](img/model-based-testing/examples/phone_super_state.svg)
 
Now we can also simplify the state machine by collapsing the super state:
 
![Collapsed super state example](img/model-based-testing/examples/phone_collapsed.svg)
 
 
 
**Regions**:
So far, we have had super states that contain one state machine.
Here, the system is in only one state of the super state at once.
In some cases, it may be useful to allow the system to be in multiple states at once.
This is achieved with regions.
 
A super state can be split into multiple regions.
These are orthogonal regions, meaning that the state machines in the regions are independent of each other; they do not influence the state machines in other regions.
Each region contains one state machine.
When the system enters the super state, it enters all the initial states of the regions.
This means that the system is in multiple states at once.
 
The notation of regions is: ![Region notation](img/model-based-testing/uml/region_symbol.svg)
 
Expanding regions is possible, but is highly impractical and usually not wanted, because expanding a region requires the creation of a state for each combination of states in the different regions.
This causes the number of states and transitions to explode quickly.
For this reason, we will not cover how to expand regions.
 
In general, it is best to use small state machines and to link these together using super states and regions.
 
So far when the phone was `ON`, we modelled the `LOCKED` and `UNLOCKED` state.
When the phone is on, it drains the battery.
The system keeps track of the level of the battery.
Let's assume that our phone has two battery levels: low and normal.
The draining of the battery and the transitions between the states of the battery run in parallel to the phone being locked or unlocked.
With parallel behaviour like this, we can use the regions in the state machine model, which looks like the following, with the new battery states and the regions:
 
![Region state machine example](img/model-based-testing/examples/phone_region.svg)
 
You see that we assume that the battery starts in the normal level state.
Therefore, when the system transitions to the `ON` state, it will be in both `LOCKED` and `NORMAL BATTERY` states at once.
 
{% set video_id = "D0IQxdjI0M0" %}
{% include "/includes/youtube.md" %}
 
 
 
### Implementing state-based testing in practice
 
You have now learnt a lot about state machines as a model.
One thing we have not looked at yet is how these state machines are represented in the actual code.
 
States are very common in programming.
Most classes in Object-Oriented-Programming correspond to their own small state machine.
 
In these classes, we distinguish two types of methods: **inspection** and **trigger** methods.
An **inspection** method only provides information about an object's state.
This information consists of the values or fields of an object.
The inspection methods only provide information. They do not change the state (or values) of an object.
**Trigger** methods bring the class into a new state.
This can be done by changing some of the class's values.
These trigger methods correspond to the events on the transitions in the state machine.
 
In a test, we want to bring the class to different states and assert that after each transition, the class is in the expected state.
In other words, a **test scenario** is basically a series of calls on the class's trigger methods.
Between these calls, we can call the inspection methods to check the state.
 
When a state machine corresponds to a single class, we can easily use the methods described above to test the state machine.
However, sometimes the state machine spans over multiple classes.
In that case, you might not be able to identify the inspection and trigger methods easily.
In this scenario, the state machines can even correspond to *end-to-end / system testing*.
Here, the flow of the entire system is under test, from input to output.
 
The system under test does not always provide a programming interface (API) to inspect the state or trigger the event for the transitions.
A common example of such a system is a web application.
In the end, the web application works through a browser.
To access the state or to trigger the transitions you would then need to use a dedicated tool to control the browser.
Using such a dedicated tool directly is not ideal, as you would have to specify each individual click on the web page to trigger events.
What we need is an abstraction layer on top of the system being tested.
This abstraction can be a Java class, with the methods that we need to be able to test the system as its state machine.
In this way, the abstraction layer will contain the inspection method to check the state and the trigger methods to perform the transitions.
 
With this small abstraction layer, we can formulate the tests clearly.
Triggering a transition is a single method call and checking the state also requires only one method call.
An example of such an abstraction layer is discussed in the chapter on web testing.
 
 
## Another example of a real-world model
 
Slack shared their internal flow chart that decides whether to send a notification of a message:
 
![How Slack decides to send notifications to users](img/model-based-testing/examples/slack.jpg)
 
 
## Exercises
 
 
**Exercise 1.**
The *ColdHot* air conditioning system has the following requirements:
 
- When the user turns it on, the machine is in an *idle* state. This is the initial state.
- If it is *too hot*, then, the *cooling* process starts. It goes back to *idle* when the defined *temperature is reached*.
- If it is *too cold*, then, the *heating* process starts. It goes back to *idle* when the defined *temperature is reached*.
- If the user *turns it off*, the machine is *off*. If the user *turns it on* again, the machine is back to *idle*.
 
Draw a minimal state machine to represent these requirements.
 
 
**Exercise 2.**
Derive the transition tree from the state machine of the assignment above.
 
 
**Exercise 3.**
Derive the transition table of the *ColdHot* state machine.
 
How many sneaky paths can be tested based on this transition table?
  
**Exercise 4.**
Draw the transition tree of the following state machine:

![](img/model-based-testing/exercises/statemachine-order.png)


**Exercise 5.**
With the transition tree you devised in the previous exercise and the state machine in that exercise, what is the transition coverage of a test that includes the following events: [order placed, order received, order fulfilled, order delivered]?
 
 
**Exercise 6.**
Devise the transition table of the state machine that was given in the exercise above.
(Note that the initial transition `Order placed` should not be represented in the transition table; this transition is there as a way to indicate the developer what "external action" has to happen for this state machine to start).
 
 
**Exercise 7.**
How many sneak paths are there in the state machine we used in the previous exercises?
(Again, ignore the initial `Order placed` transition.)
 
 
**Exercise 8.**
Consider the following decision table:
<table>
  <tr><th>Criteria</th><th colspan="6">Options</th></tr>
  <tr><td>C1: Employed for 1 year</td><td>T</td><td>F</td><td>F</td><td>T</td><td>T</td><td>T</td></tr>
  <tr><td>C2: Achieved last year's goal</td><td>T</td><td>dc</td><td>dc</td><td>F</td><td>T</td><td>F</td></tr>
  <tr><td>C3: Positive evaluation from peers</td><td>T</td><td>F</td><td>T</td><td>F</td><td>F</td><td>T</td></tr>
  <tr><td></td><td>10%</td><td>0%</td><td>5%</td><td>2%</td><td>6%</td><td>3%</td></tr>
</table>
 
Which decisions do we have to test for full MC/DC?
 
Use as few decisions as possible.
 
 
**Exercise 9.**
See the following generic state machine.
 
![](img/model-based-testing/exercises/generic_state_machine.svg)
 
Draw the transition tree of this state machine.
 
 
**Exercise 10.**
The advertisement (ad) feature is an important source of income for the company. Because of that, the life cycle of an ad needs to be better modelled.
Our product team defined the following rules:
 
* The life cycle of an ad starts with an 'empty' ad being created.
* The company provides information about the ad. More specifically, the company defines an image, a description, and how many times it should appear. When all this information is set, the ad then needs to wait for approval.
* An administrator checks the content of the ad. If it follows all the rules, the ad then waits for payment. If the ad contains anything illegal, it then goes back to the beginning of the process.
* As soon as the company makes the payment, the ad becomes available to users.
* When the target number of visualisations is reached, the ad is considered done. At this time, the company might consider running the campaign again, which moves the ad to wait for payment again. The company might also decide to end the campaign at that moment, which puts the ad in a final state.  
* While appearing for the users, if more than 10% of the users complain about the ad, the ad is then marked as blocked. The company then gets contacted. After understanding the case, the ad either starts to appear again, or is labelled as inappropriate. An inappropriate ad will never be shown again.
 
Devise a state diagram that describes the life cycle of an ad.
 
 
**Exercise 11.**
A microwave oven has the following requirements:
 
* Its initial state is `OFF`.
* When the user `turns it on`, the machine goes to an `ON` state.
* If the user selects `warm meal`, then, the `WARMING` process starts. It goes back to `ON` when the defined `time is reached`. A user may `cancel` it at any time, taking the microwave back to the `ON` state.
* If the user selects `defrost meal`, then, the `DEFROSTING` process starts. It goes back to `ON` when the defined `time is reached`. A user may `cancel` it at any time, taking the microwave back to the `ON` state.
* The user can `turn off` the microwave (after which it is `OFF`), but only if the microwave is not warming up or defrosting food.
 
Draw a minimal state machine to represent the requirements. For this question do not make use of super (OR) states.
Also, remember that, if a transition is not specified in the requirements, it simply does not exist, and thus, should not be represented in the state machine.
 
 
**Exercise 12.**
Devise a state transition tree for the microwave state machine.
 
**Exercise 13.**
Again, consider the state machine requirements for the microwave.
There appears to be some redundancy in the defrosting and warming up functionality, which potentially can be described using super states (also called OR-states).

Make use of super states and re-design the state machine.
 
 
**Exercise 14.**
See the requirement below:
 
```
Stefan works for Foodgram, a piece of software that enables users to send pictures of the dishes they prepare themselves. Foodgram's upload system has specific rules:
 
* The software should only accept images in JPG format.
* The software should not accept images that are bigger than 20MB.
* The software accepts images in both high and low resolution.
 
As soon as a user uploads a photo, the aforementioned rules are applied.
The software then either says *"Congratulations! Your picture was uploaded successfully"*, or *"Ooops, something went wrong!"*" (without any specific details about why it happened).
```
 
Create a decision table that takes the three conditions and their respective outcomes into account.
 
*Note: conditions should be modelled as Boolean decisions.*
 
 
 
 
 
 
 
 
 
**Exercise 15**
Twitter is a software system that enables users to share short messages with their friends.
Twitter's revenue model is ultimately based on advertisements (ads).
Twitter's system needs to decide when to show ads to its users, and which ones. For a given user a given ad can be *highly-relevant*, and the system seeks to serve the most relevant ads as often as possible without scaring users away.
 
To that end, assume that the system employs the following rules to decide whether a user *U* gets served an ad *A* at the moment user *U* opens their Twitter app:
 
* If the user *U* has not been active during the past two weeks, she will not get to see ad *A*;
* If the user *U* has already been served an ad during her last hour of activity, she will not get to see ad *A*;
* Furthermore, if the user *U* has over 1000 followers (an influencer), she will only get to see ad *A* if *A* is labelled as *highly-relevant* for *U*. Otherwise, user *U* will see *A* even if it is not *highly-relevant*.
 
We can model this procedure in a decision table, in various ways.
The complete table would have four conditions and 16 variants. We will try to create a more compact decision table. Note: We will use Boolean conditions only.
 
One way is to focus on the positive cases only, i.e., specify only the variants in which ad *A* is being served to user *U*. If you do not use 'DC' (don't care) values, how will the decision table look?
 
 
 
 
## References
 
* Chapter 4 of the Foundations of software testing: ISTQB certification. Graham, Dorothy, Erik Van Veenendaal, and Isabel Evans, Cengage Learning EMEA, 2008.
# Specification-Based Testing

In this chapter, we explore **specification-based testing** techniques. These use the *requirements* of the program (often written as text; think of user stories and/or UML use cases) as input for testing.

In simple terms, we devise a set 
of inputs, where each input tackles one part (or *partition*) 
of the program.

Given that specification-based techniques require no knowledge 
of how the software inside the "box" is structured.
For instance, it does not matter if the software was developed in Java or Python,
or what particular data structures were used in the implementation.
Therefore, these techniques are also referred to as **black box testing**.


## Partitioning the input space

Programs are usually too complex to be tested with just a single test case.
There are different cases in which the program is executed 
and its execution often depends on various factors, such as the input 
to the program.

Let's use a small program as an example. The specification below talks about a program that decides whether a given year is a leap year or not. 

> **Requirement: Leap year**
>
>Given a specific year as an input, the program should return *true* if 
>the provided year is a leap year and *false* if it is not.
>
>A year is a leap year if:
>
>- the year is divisible by 4;
>- and the year is not divisible by 100;
>- except when the year is divisible by 400 (because then it is a leap year)

To find a good set of test cases, often referred to as a *test suite*, 
we split the program into *classes*.
In other words, we divide the input space of 
the program in such a way that:
1) Each class is different, i.e. it is unique, where 
no two partitions represent/exercise the same behaviour;
2) We can easily verify whether the behaviour for a given input is correct or not.


By looking at the requirements above, we can derive the 
following classes/partitions:

* Year is divisible by 4, but not divisible by 100 = `leap year, TRUE`
* Year is divisible by 4, divisible by 100, divisible by 400 = `leap year, TRUE`
* Not divisible by 4 = `not leap year, FALSE`
* Divisible by 4, divisible by 100, but not divisible by 400 = `not leap year, FALSE`

Note how each class above exercises the program in different ways.


{% set video_id = "kSLbxmXcPPI" %}
{% include "/includes/youtube.md" %}



## Equivalence partitioning

The partitions above are not test cases that we can implement directly because
each partition might be instantiated by an infinite number of inputs. For example,
for the partition "year not divisible by 4", there are infinitely many numbers 
that are not divisible by 4 which we could use as concrete inputs to the program.
So how do we know which concrete input to instantiate for each of the partitions?

As we discussed earlier, each partition exercises the program in a certain way.
In other words, all input values from one specific partition will make the program 
behave in the same way.
Therefore, any input we select should give us the same result.
We assume that, if the program behaves correctly for one given input, 
it will work correctly for all other inputs from that class.
This idea of inputs being equivalent to each other 
is called **equivalence partitioning**.
Thus, it does not matter which precise input we select and one test case per 
partition will be enough.

Let’s now write some JUnit tests for the leap year problem. Remember that the name of a test method
in JUnit can be anything. It is good to name your test method after the 
partition that the method tests.

{% hint style='tip' %}
We discuss more about test code quality and best practices in writing test code in 
a future chapter.
{% endhint %}

The _Leap Year_ specification has been implemented by a developer in the following way:

```java
public class LeapYear {

  public boolean isLeapYear(int year) {
    if (year % 400 == 0)
      return true;
    if (year % 100 == 0)
      return false;

    return year % 4 == 0;
  }
}
```

With the classes we devised above, we have 4 test cases in total (i.e., one test case 
for each class/partition).
As any input can be used for a given partition, 
the following inputs will be used for the partitions:

- 2016, divisible by 4, not divisible by 100.
- 2000, divisible by 4, also divisible by 100 and by 400.
- 39, not divisible by 4.
- 1900, divisible by 4 and 100, not by 400.

Implementing this using JUnit gives the following code for the tests:

```java
public class LeapYearTests {

  private final LeapYear leapYear = new LeapYear();

  @Test
  public void divisibleBy4_notDivisibleBy100() {
    boolean leap = leapYear.isLeapYear(2016);
    assertTrue(leap);
  }

  @Test
  public void divisibleBy4_100_400() {
    boolean leap = leapYear.isLeapYear(2000);
    assertTrue(leap);
  }

  @Test
  public void notDivisibleBy4() {
    boolean leap = leapYear.isLeapYear(39);
    assertFalse(leap);
  }

  @Test
  public void divisibleBy4_and_100_not_400() {
    boolean leap = leapYear.isLeapYear(1900);
    assertFalse(leap);
  }
}
```

Note that each test method covers one of the partitions and the naming of the method refers to the partition it covers.

For those who are learning JUnit: Note that a new instance of the test class is created before each test,
so each test has a new `LeapYear` object. In this example, the `LeapYear` object has no state, so refreshing
the object under test is not significant, but this is good practice to observe in general.
In each test we first determine the result of the method.
After the method returns a value, we assert that this is the expected value.

{% set video_id = "mXmFiiifwaE" %}
{% include "/includes/youtube.md" %}


## Category-Partition Method

So far we have derived partitions by just looking at the specification of the program.
We basically used our experience and knowledge to derive the test cases.
In this chapter, we will discuss a more systematic way of deriving these partitions: the **Category-Partition** method.

This method provides us with a systematic way of deriving test cases, based on the characteristics of the input parameters. It also reduces the number of tests to a practical number.

Here are the steps we follow for this method and then an example to illustrate the process.

1. Identify the parameters, or the input for the program. For example, the parameters your classes and methods receive.
2. Derive characteristics of each parameter. For example, an `int year` should be a positive integer number between 0 and infinite. 
      - Some of these characteristics can be found directly in the specification of the program.
      - Others might not be found from specifications. For example, an input cannot be `null` if the method does not handle that well.

3. Add constraints in order to minimise the test suite.
      - Identify invalid combinations. For some characteristics it might not be possible to combine them with other characteristics.
      - Exceptional behaviour does not always have to be combined with all of the values of the other inputs. For example, trying a single `null` input might be enough to test that corner case.

4. Generate combinations of the input values. These are the test cases.

Let's apply the technique in the following program:

> **Requirement: Christmas discount**
> 
> The system should give a 25% discount on the cart when it is Christmas.
> The method has two input parameters: the total price of the products in the cart, and the date.
> When it is not Christmas, it just returns the original price; otherwise it applies the discount.

Following the category-partition method:

1. We have two parameters:
      - The current date
      - The total price

2. For each parameter we define the characteristics as:
      - Based on the requirements, the only important characteristic is that the date can be either Christmas or not.
      - The price can be a positive number, or in certain circumstances it may be 0. Technically the price can also be a negative number. This is an exceptional case, as you cannot pay a negative amount.

3. The number of characteristics and parameters is not too large in this case. As the negative price is an exceptional case, we can test this with just one combination, instead of with a date that is Christmas and a date that is not Christmas.

4. We combine the other characteristics to get the following test cases:
      - Positive price at Christmas
      - Positive price not at Christmas
      - Price of 0 at Christmas
      - Price of 0 not at Christmas
      - Negative price at Christmas

We now implement these test cases.
Each of the test cases corresponds to one of the partitions that we want to test.

{% set video_id = "frzRmafsPBk" %}
{% include "/includes/youtube.md" %}

## Walking example

> **Requirement: Chocolate bars**
>
> A package contains a certain number of chocolate bars in kilos.
> A package is composed of small bars (1 kilo each) and big bars (5 kilos each).
>
> Assuming that the package is always filled with the maximum number of big bars possible, return the number of small bars required to complete the package. 
> Return -1 if it is not possible to fill the package completely.
>
> The input of the program is: the number of available small bars, the number of available big bars, and the total number of kilos of the package.

A possible implementation for this program is as follows:

```java
public class ChocolateBars {

    public static final int CANNOT_PACK_BAG = -1;

    public int calculate(int small, int big, int total) {
        int maxBigBoxes = total / 5;
        int bigBoxesWeCanUse = Math.min(maxBigBoxes, big);
        total -= (bigBoxesWeCanUse * 5);

        if(small <= total)
            return CANNOT_PACK_BAG;
        return total;

    }
}
```

In this requirement, the partitions are less clear and it is essential to understand the problem fully
in order to derive the partitions.

One way to perform the analysis is to consider how the input variables affect the output variables. We observe that:

* There are three input variables: _number of small bars_, _number of big bars_, _number of kilos in a package_. They are all integers and values can range from 0 to infinity.
* Given a valid _number of kilos in a package_, the outcome is then based on the _number of big bars_ and _number of small bars_. This means we can only analyse the variables together, instead of separately.

We derive the following classes / partitions:

* **Need only small bars**. A solution that only uses small bars (and does not use big bars).
* **Need only big bars**. A solution that only uses the big bars (and does not use small bars).
* **Need small + big bars**. A solution that has to use both small and big bars.
* **Not enough bars**. A case for which there is no solution, because there are not enough bars.

We also derive an invalid class:

* **Not from the specs**: An exceptional case (e.g., negative numbers in any of the inputs).

For each of these classes, we can devise five concrete test cases:

* **Need only small bars**. small = 4, big = 2, total = 3
* **Need only big bars**. small = 5, big = 3, total = 10
* **Need small + big bars**. small = 5, big = 3, total = 17
* **Not enough bars**. small = 1, big = 1, total = 10
* **Not from the specs**: small = -1, big = -1, total = -1

In JUnit code:

```java
public class ChocolateBarsTest {
    private final ChocolateBars bars = new ChocolateBars();

    @Test
    void notEnoughBars() {
        assertEquals(-1, bars.calculate(1, 1, 10));
    }

    @Test
    void onlyBigBars() {
        assertEquals(0, bars.calculate(5, 3, 10));
    }

    @Test
    void bigAndSmallBars() {
        assertEquals(2, bars.calculate(5, 3, 17));
    }

    @Test
    void onlySmallBars() {
        assertEquals(3, bars.calculate(4, 2, 3));
    }

    @Test
    void invalidValues() {
      assertEquals(-1, bars.calculate(-1, -1, -1));
    }
}
```

This example shows a case where deriving good test cases becomes more challenging due to the
specifications being complex.

{% hint style='tip' %}
If you know some advanced features of JUnit, you might be wondering why we did not use something like parameterised tests. We will refactor this test code in a future chapter.
{% endhint %}

{% set video_id = "T8caAUwgquQ" %}
{% include "/includes/youtube.md" %}



## Exercises

**Exercise 1.**
What is an Equivalence Partition?


1. A group of results that is produced by one method.
2. A group of results that is produced by one input passed into different methods.
3. A group of inputs that all make a method behave in the same way.
4. A group of inputs that gives exactly the same output in every method.

**Exercise 2.**
We have a program called FizzBuzz.
It does the following:
> Given an integer `n`, return the string form of the number followed by `"!"`.
> If the number is divisible by 3 use `"Fizz"` instead of the number,
> and if the number is divisible by 5 use `"Buzz"` instead of the number,
> and if the number is divisible by both 3 and 5, use `"FizzBuzz"`

Examples:
* The integer 3 yields `"Fizz!"`
* The integer 4 yields `"4!"`
* The integer 5 yields `"Buzz!"`
* The integer 15 yields `"FizzBuzz"`

A novice tester is trying hard to devise as many tests as she can for
the FizzBuzz method.
She came up with the following tests:

- T1 = 15
- T2 = 30
- T3 = 8
- T4 = 6
- T5 = 25

Which of these tests can be removed while keeping a good test suite?

Which concept can we use to determine the tests that can be removed?

**Exercise 3.**
See a slightly modified version of the HashMap's `put` method Javadoc below. (Source code [here](http://developer.classpath.org/doc/java/util/HashMap-source.html)).

```java
/**
* Puts the supplied value into the Map,
* mapped by the supplied key.
* If the key is already in the map, its
* value will be replaced by the new value.
*
* NOTE: Nulls are not accepted as keys;
*  a RuntimeException is thrown when key is null.
*
* @param key the key used to locate the value
* @param value the value to be stored in the HashMap
* @return the prior mapping of the key, or null if there was none.
*/
public V put(K key, V value) {
  // implementation here
}
```

Apply the category/partition method.
What are the minimal and most suitable partitions?

**Exercise 4.**
Zip codes in country X are always composed of 4 numbers + 2 letters, e.g., `2628CD`.
Numbers are in the range `[1000, 4000]`.
Letters are in the range `[C, M]`.

Consider a program that receives two inputs: an integer (for the 4 numbers) and a string (for the 2 letters), and returns `true` (valid zip code) or `false` (invalid zip code).

The boundaries for this program appear to be straightforward:
- Anything below 1000 -> invalid
- [1000, 4000] -> valid
- Anything above 4000 -> invalid
- [A, B] -> invalid
- [C, M] -> valid
- [N, Z] -> invalid

Based on what you as a tester *assume* about the program, which invalid cases can you come up with?
Describe these invalid cases and how they might exercise the program based on your assumptions.

**Exercise 5.**
See a slightly modified version of the HashSet's `add()` Javadoc below.
Apply the category/partition method. What are the **minimal and most suitable partitions** for the `e` input parameter? 

```java
/**
* Adds the specified element to this set if it 
 * is not already present.
* If this set already contains the element, 
 * the call leaves the set unchanged
* and returns false.
*
* If the specified element is NULL, the call leaves the
* set unchanged and returns false.
*
* @param e element to be added to this set
* @return true if this set did not already contain 
 *   the specified element
*/
public boolean add(E e) {
    // implementation here
}
```


**Exercise 6.**
Which of the following statements **is false** about applying the category/partition method in the Java method below?

```java
/**
* Puts the supplied value into the Map, 
 * mapped by the supplied key.
* If the key is already in the map, its
* value will be replaced by the new value.
*
* NOTE: Nulls are not accepted as keys; 
 *  a RuntimeException is thrown when key is null.
*
* @param key the key used to locate the value
* @param value the value to be stored in the HashMap
* @return the prior mapping of the key, 
 *  or null if there was none.
*/
public V put(K key, V value) {
  // implementation here
}
```


1. The specification does not specify any details about the `value` input parameter, and thus, experience should be used to partition it, e.g., `value` being null and not null.

2. The number of tests generated by the category/partition method can grow quickly, as the chosen partitions for each category are later combined one-by-one. This is not a practical problem to the `put()` method because the number of categories and their partitions is small.

3. In an object-oriented language, besides using the method's input parameters to explore partitions, we should also consider the internal state of the object (i.e., the class's attributes), as it can also affect the behaviour of the method.

4. With the available information, it is not possible to perform the category/partition method, as the source code is required for the last step of the category/partition method: adding constraints.


**Exercise 7.**
Consider a `find` program that finds occurrences of a pattern in a file. The program has the following syntax:

```
find <pattern> <file>
```

A tester, after reading the specs and following the Category-Partition method, devised the following test specification:


* **Pattern size:** empty, single character, many characters, longer than any line in the file.
* **Quoting:** pattern is quoted, pattern is not quoted, pattern is improperly quoted.
* **File name:** good file name, no file name with this name, omitted.
* **Occurrences in the file:** none, exactly one, more than one.
* **Occurrences in a single line, assuming line contains the pattern:** one, more than one.

However, the number of combinations is too high now. What actions could we take to reduce the number of combinations?

**Exercise 8.**
What test cases should be created when taking both the partition of the input parameters *and* the internal state of the object into account?

```java
/**
* Adds the specified element to this set if it 
 * is not already present.
* If this set already contains the element, 
 * the call leaves the set unchanged
* and returns false.
*
* If the specified element is NULL, the call leaves the
* set unchanged and returns false.
*
* If the set is full, 
 * the call leaves the set unchanged and return false.
* Use private method `isFull` to know whether the set is already full.
*
* @param e element to be added to this set
* @return true if this set did not already contain 
 *   the specified element
*/
public boolean add(E e) {
    // implementation here
}
```


## References

* Graham, D., Van Veenendaal, E., & Evans, I. (2008). Foundations of software testing: ISTQB certification. Cengage Learning EMEA. Chapter 4.

* Pezzè, M., & Young, M. (2008). Software testing and analysis: process, principles, and techniques. John Wiley & Sons. Chapter 10.

* Ostrand, T. J., & Balcer, M. J. (1988). The category-partition method for specifying and generating functional tests. Communications of the ACM, 31(6), 676-686.

* Pacheco, C., & Ernst, M. D. (2007, October). Randoop: feedback-directed random testing for Java. In Companion to the 22nd ACM SIGPLAN conference on Object-oriented programming systems and applications companion (pp. 815-816).
# Domain testing examples

This chapter follows a problem-based approach. We first show a program requirement, and then, show how we would apply equivalent class analysis and boundary testing.

We will follow a common strategy when applying domain testing, highly influenced by how Kaner et al. do:

1. We read the requirement
2. We identify the input and output variables in play, together with their types, and their ranges.
3. We identify the dependencies (or independence) among input variables, and how input variables influence the output variable.
4. We perform equivalent class analysis (valid and invalid classes).
5. We explore the boundaries of these classes.
6. We think of a strategy to derive test cases, focusing on minimizing the costs while maximizing fault detection capability.
7. We generate a set of test cases that should be executed against the system under test.

See the videos for detailed explanations. See also the JUnit test cases we implemented for them in https://github.com/sttp-book/code-examples/tree/master/src/test/java/tudelft/domain.

{% set video_id = "6-SaTbc61eA" %}
{% include "/includes/youtube.md" %}

## Exercise 1: The Sum Of Integers

A program receives two numbers and returns the sum of these two integers. Numbers are between 1 *inclusive* and 99 *inclusive*.

| Variables         | Types     | Ranges    |
| ---               | ---       | ---       |
| X - First number  | Integer   | [1, 99]   |
| Y - Second number | Integer   | [1, 99]   |
| Sum - Output      | Integer   | [1, inf]  |

### Dependency among variables:

* X and Y are independent (X doesn't influence the range of Y, and vice-versa).
* X and Y are used to calculate Sum.

### Equivalence partitioning / Boundary analysis

| Variable  | Equivalence classes   | Invalid classes   | Boundaries    |
| ---       | ---                   | ---               | :---:         |
| X         | [1, 99]               | < 1               | *valid* 1 &#124; 0 *invalid*    |
|           |                       | > 99              | 99 &#124; 100 |
| Y         | [1, 99]               | < 1               | 1 &#124; 0    |
|           |                       | > 99              | 99 &#124; 100 |

### Strategy

* Variables are independent (X does not affect the range of Y, and vice-versa).
* 7 tests for X (3 partitions + 4 boundaries), in point for Y.
* 7 tests for Y (3 partitions + 4 boundaries), in points for X.
* Total of 14 tests

*In-point X = 50, In-point Y = 50.*

| Test cases    | X     | Y     | Sum       | Remark    |
| ---           | ---   | ---   | ---       | ---       |
| T1            | 50    | 50    | 100       | x &isinv; [1, 99], &Tab; *y remains fixed for T1-T7*|
| T2            | -100  | 50    | *invalid* | x < 1     |
| T3            | 250   | 50    | *invalid* | x > 99    |
| T4            | 0     | 50    | *invalid* | x = 0     |
| T5            | 1     | 50    | 51        | x = 1     |
| T6            | 99    | 50    | 149       | x = 99    |
| T7            | 100   | 50    | *invalid* | x = 100   |
| T8            | 50    | 50    | 100       | y &isinv; [1, 99], &Tab; *x remains fixed for T8-T14*|
| T9            | 50    | -100  | *invalid* | y < 1     |
| T10           | 50    | 250   | *invalid* | y > 99    |
| T11           | 50    | 0     | *invalid* | y = 0     |
| T12           | 50    | 1     | 51        | y = 1     |
| T13           | 50    | 99    | 149       | y = 99    |
| T14           | 50    | 100   | *invalid* | y = 100   |

**Questions:**

* Do we need T3 and T7? Or are they testing the same thing (i.e. > 99)? (Same applies to T10 and T14.) If we remove one of them, we end up with 12 tests.
* What would change if the requirement had something like sum < 167 ?



{% set video_id = "ElO9sKkG-2w" %}
{% include "/includes/youtube.md" %}

## Exercise 2: The Sum Of Integers, part 2

A program receives two numbers and returns the sum of these two integers. Numbers are between 1 *inclusive* and 99 *inclusive*.
Final sum should be <= 165.

| Variables         | Types     | Ranges    |
| ---               | ---       | ---       |
| X - First number  | Integer   | [1, 99]   |
| Y - Second number | Integer   | [1, 99]   |
| Sum - Output      | Integer   | [0, 165]  |

### Dependency among variables:

* X and Y are independent (X doesn't influence the range of Y, and vice-versa).
* X and Y are used to calculate Sum.
* X + Y <= 165

### Equivalence partitioning / Boundary analysis

| Variable  | Equivalence classes   | Invalid classes   | Boundaries    |
| ---       | ---                   | ---               | :---:         |
| X         | [1, 99]               | < 1               | *valid* 1 &#124; 0 *invalid*    |
|           |                       | > 99              | 99 &#124; 100 |
| Y         | [1, 99]               | < 1               | 1 &#124; 0    |
|           |                       | > 99              | 99 &#124; 100 |
| Sum       | [0, 165]              | > 165             | 165 &#124; 166    |

### Strategy

* Variables are independent (X does not affect the range of Y, and vice-versa).
* 7 tests for X (3 partitions + 4 boundaries), in point for Y,
* 7 tests for Y (3 partitions + 4 boundaries), in points for X.
* 2 tests for the boundary on Sum.
* Total of 16 tests

*In-point X = 50 (taking into consideration that X <= 165 - Y),
In-point Y = 50 (taking into consideration that Y <= 165 - X).*

| Test cases    | X     | Y     | Sum       | Remark    |
| ---           | ---   | ---   | ---       | ---       |
| T1            | 50    | 50    | 100       | x &isinv; [1, 99], &Tab; *y remains fixed for T1-T7*|
| T2            | -100  | 50    | *invalid* | x < 1     |
| T3            | 250   | 50    | *invalid* | x > 99    |
| T4            | 0     | 50    | *invalid* | x = 0     |
| T5            | 1     | 50    | 51        | x = 1     |
| T6            | 99    | 50    | 149       | x = 99    |
| T7            | 100   | 50    | *invalid* | x = 100   |
| T8            | 50    | 50    | 100       | y &isinv; [1, 99], &Tab; *x remains fixed for T8-T14*|
| T9            | 50    | -100  | *invalid* | y < 1     |
| T10           | 50    | 250   | *invalid* | y > 99    |
| T11           | 50    | 0     | *invalid* | y = 0     |
| T12           | 50    | 1     | 51        | y = 1     |
| T13           | 50    | 99    | 149       | y = 99    |
| T14           | 50    | 100   | *invalid* | y = 100   |
| T15           | 82    | 83    | 165       |           |
| T16           | 83    | 83    | *invalid* |           |


{% set video_id = "w2er-p_tyRc" %}
{% include "/includes/youtube.md" %}

## Exercise 3: Passing Grade

A student passes an exam if s/he gets a grade >= 5.0. Grades below that are a fail.

Grades range from [1.0, 10.0]. Assume the system doesn't allow for invalid grades (e.g., 0.9, 10.5).

### Variables

| Variables | Types   | Ranges     | Notes                |
|-----------|---------|------------|----------------------|
| grade     | float   | [1, 10]    | (no one gets a 0...) |
| pass      | boolean | true/false | output variable      |

### Dependencies among variables

- No dependencies among input variables (grade is the only one).
- Grade is used to decide the pass/fail.

### Equivalence partitioning / Boundary analysis

| Variable | Equivalence Classes | Invalid Classes | Boundaries |
|----------|---------------------|-----------------|------------|
| grade    | [1,5[               |                 | 1          |
|          |                     |                 | 4.9        |
|          |                     |                 | 5          |
|          | [5, 10]             |                 | 4.9        |
|          |                     |                 | 5          |
|          |                     |                 | 10         |

We do not need to test 0.9 and 10.1 because we assume that the system doesn't allow for them.

### Strategy

Boundaries seem to be enough.

| Test Case | Grade (input) | Pass (output) | Notes          |
|-----------|---------------|---------------|----------------|
| T1        | 1             | false         |                |
| T2        | 4.9           | false         |                |
| T3        | 5             | true          |                |
| T4        | 7.5           | true          | extra in-point |
| T5        | 10            | true          |                |

{% set video_id = "S_90_sYP7GA" %}
{% include "/includes/youtube.md" %}



## Exercise 4: Passing Concepts

The final grade of a student is calculated as follows:

* `1 <= grade < 5` => **`F`**
* `5 <= grade < 6` => **`E`**
* `6 <= grade < 7` => **`D`**
* `7 <= grade < 8` => **`C`**
* `8 <= grade < 9` => **`B`**
* `9 <= grade <= 10` => **`A`**

The system does not allow for invalid grades (e.g. 0.9, 10.5)

### Variables

| Variable | type | range | remark |
| -------- | ---- | ----- | ------- |
| Grade | float | [1, 10] | no one gets a 0 |
| Concept | Enumerate | [F, E, D, B, C, A] | output variable |

### Dependency among variables

*There are no dependencies among the input variables, since we only have one variable.

* The grade is used to decide the concept.

### Equivalence Partitioning/Boundary Analysis

| Variable | Equivalence classes | Invalid classes | Boundaries |
| -------- | ------------------- | --------------- | ---------- |
| Grade | [1,5[ | | |
| | | | 1 |
| | | | 5 |
| | | | 4.9 |
| | [5,6[ | | |
| | | | 4.9 |
| | | | 5 |
| | | | 6 |
| | | | 5.9 |
| | [6,7[ | | |
| | | | 5.9 |
| | | | 6 |
| | | | 7 |
| | | | 6.9 |
| | [7,8[ | | |
| | | | 6.9 |
| | | | 7 |
| | | | 8 |
| | | | 7.9 |
| | [8,9[ | | |
| | | | 7.9 |
| | | | 8 |
| | | | 9 |
| | | | 8.9 |
| | [9,10] | | |
| | | | 8.9 |
| | | | 9 |
| | | | 10 |



### Strategy

Test all boundaries, yielding 12 tests.


| Test case | Grade | Concept (output) |
| --------- | ----- | ---------------- |
| T1 | 1 | F |
| T2 | 4.9 | F |
| T3 | 5 | E |
| T4 | 5.9 | E |
| T5 | 6 | D |
| T6 | 6.9 | D |
| T7 | 7 | C |
| T8 | 7.9 | C |
| T9 | 8 | B |
| T10 | 8.9 | B |
| T11 | 9 | A |
| T12 | 10 | A |


{% set video_id = "YGomnAMt7RA" %}
{% include "/includes/youtube.md" %}


## Exercise 5: The MSc admission problem

A student can only join the MSc if :
* `ACT = 36` and `GPA >= 3.5`
* `ACT >= 35` and `GPA >= 3.6`
* `ACT >= 34` and `GPA >= 3.7`
* `ACT >= 33` and `GPA >= 3.8`
* `ACT >= 32` and `GPA >= 3.9`
* `ACT >= 31` and `GPA = 4.0`
*ACT is an integer between 0 and 36 (inclusive).*
*GPA are float variables between 0.0 and 4.0 (single decimal digit) (inclusive).*

### Variables
| Variables	| Types		| Ranges 	|
| ---		| ---		| ---		|
| ACT		| integer	| [0, 36]	|
| GPA		| float		| [0.0, 4.0]	|
| Decision	| boolean 	| true/false	|

### Dependency among variables
* ACT and GPA have a joint effect
* ACT and GPA are used to calculate Decision.

### Equivalence partitioning / Boundary Analysis

The conditions from the requirements are really close with each other. Because of that, finding values that cross the boundary between partitions is challenging. We also note that the boundary that matters here is from (student being approved) -> (student not being approved). So, for each partition, we should find its boundary that would force the student not being approved.

These boundary points can found through the following process. We first find the on-point of the boundary, for instance (35, 3.6), then discover the closest inputs that would make the student to fail. In this example, (34, 3.6) and (35, 3.5). Note that there exists one special case (= 36, >= 3.5). Since it contains an equality condition, we have two off-points to explore.

Also note that we are using in-points that are on-points too. This is less common, but in the case of this problem choosing anohter in-point might make the final outcome to still not change between the boundary tests we devised.
For example, if we choose a GPA in-point of 3.6, the 35 and 36 ACT will have the same outcome value (as the next rule will intervene).

| Variable	| Equivalence classes	| Boundaries	| Remark	|
| ---		| ---					| ---			| ---		|
|(ACT, GPA)	| (= 36, >= 3.5)		| (35, in)		| in-point GPA = 3.5 	|
|			| 						| (36, in)		| on-point			|
|			| 						| (37, in)		| 			|
|			| 						| (in, 3.5)		| in-point ACT = 36		|
|			| 						| (in, 3.4)		| 			|
|			| (>= 35, >= 3.6)		| (35, in)		| in-point GPA = 3.6 	|
|			| 						| (34, in)		| 			|
|			| 						| (in, 3.6)		| in-point ACT = 35		|
|			| 						| (in, 3.5)		| 			|
|			| (>= 34, >= 3.7)		| (34, in)		| in-point GPA = 3.7 	|
|			| 						| (33, in)		| 			|
|			| 						| (in, 3.7)		| in-point ACT = 34		|
|			| 						| (in, 3.6)		| 			|
|			| (>= 33, >= 3.8)		| (33, in)		| in-point GPA = 3.8 	|
|			| 						| (32, in)		| 			|
|			| 						| (in, 3.8)		| in-point ACT = 33		|
|			| 						| (in, 3.7)		| 			|
|			| (>= 32, >= 3.9)		| (32, in)		| in-point GPA = 3.9 	|
|			| 						| (31, in)		| 			|
|			| 						| (in, 3.9)		| in-point ACT = 32		|
|			| 						| (in, 3.8)		| 			|
|			| (>= 31, = 4.0)		| (31, in)		| in-point GPA = 4.0 	|
|			| 						| (30, in)		| 			|
|			| 						| (in, 4.0)		| in-point ACT = 31		|
|			| 						| (in, 3.9)		| 			|
|			| (< 0, < 0.0)			| (-1, in)		| in-point GPA = 0.1 	|
|			| 						| (in, -0.1)	| in-point ACT = 1		|
|			| (> 36, > 4.0)			| (37, in)		| in-point GPA = 4 		|
|			| 						| (in, 4.1)		| in-point ACT = 36		|



### Strategy
* There are 24 boundaries *(for conditions on valid inputs)*, but some are repeated. 14 boundary tests.
  * (37, 3.5) is an invalid path, so we can ignore. Therefore 13 test cases.

| Test cases	| ACT	| GPA	| Decision	|
| ---			| ---	| ---	| ---		|
| T1 			| 35 	| 3.5	| False		|
| T2 			| 36 	| 3.5	| True		|
| T3 			| 36 	| 3.4	| False		|
| T4 			| 35 	| 3.6	| True		|
| T5 			| 34 	| 3.7	| True		|
| T6 			| 34 	| 3.6	| False		|
| T7 			| 33 	| 3.8	| True		|
| T8 			| 32 	| 3.8	| False		|
| T9 			| 33 	| 3.7	| False		|
| T10 			| 32 	| 3.9	| True		|
| T11			| 31 	| 4.0	| True		|
| T12 			| 30 	| 4.0	| False		|
| T13 			| 31 	| 3.9	| False		|
| T14 			| -1 	| 4.0	| *Invalid*	|
| T15 			| 37 	| 3.5	| *Invalid*	|
| T16 			| 36 	| -0.1	| *Invalid*	|
| T17			| 36	| 4.1	| *Invalid* |


{% set video_id = "g9pk25_6MrY" %}
{% include "/includes/youtube.md" %}

## Exercise 6: The printing label

A printer prints mailing labels.
The first line is the name of the person.

The program builds the name from three fields: first name, middle name, and last name.
Each field can hold up to 30 characters.
The label can be up to 70 characters wide.

### Variables

| Variables     | Types   | Ranges  |
|---------------|---------|---------|
| Len. of FN    | integer | [1, 30] |
| Len. of MN    | integer | [0, 30] |
| Len. of LN    | integer | [0, 30] |
| Output length | integer | [1, 70] |

### Dependencies among variables

FN + MN + LN <= 68

(The difference of 2 (to 70) happens as the system needs to add an empty space in between names)

### Equivalence partitioning / Boundary analysis

| Variable     | Equivalence Classes | Invalid Classes | Boundaries | Notes                           |
|--------------|---------------------|-----------------|------------|---------------------------------|
| FN           | [1, 30]             | invalid string  | 0          | everybody has a first name      |
|              |                     |                 | 1          |                                 |
|              |                     |                 | 30         |                                 |
|              |                     |                 | 31         | same as >30                     |
| MN           | [0, 30]             | invalid string  | 0          | not everybody has a middle name |
|              |                     |                 | -1         |                                 |
|              |                     |                 | 30         |                                 |
|              |                     |                 | 31         |                                 |
| LN           | [0, 30]             | invalid string  | 0          | not everybody has a last name   |
|              |                     |                 | -1         |                                 |
|              |                     |                 | 30         |                                 |
|              |                     |                 | 31         |                                 |
| (FN, MN, LN) | FM + MN + LN <= 68  |                 | 68         |                                 |
|              |                     |                 | 69         |                                 |

### Strategy

* Each variable has 2 partitions, plus a restriction.

* Let's not combine "invalid strings" with them all. So: 3 tests for exceptional cases + 5 * 5 * 5 = 125 + 3 = 128.

* If we focus on the on-points and off-points, and in-points for others, we'd have 4 + 4 + 4 = 12 tests plus invalid cases: 15 tests.

* In-points always taking the FN + MN + LN <= 68 restriction into account.

* Two tests for the extra restriction: 17 tests.

| Test Case | FN             | MN             | LN             | (length) | output  |                          |
|-----------|----------------|----------------|----------------|----------|---------|--------------------------|
| T1        | 0              | 15             | 7              | 22       | invalid | FN boundaries            |
| T2        | 1              | 7              | 2              | 10       | valid   |                          |
| T3        | 30             | 3              | 9              | 42       | valid   |                          |
| T4        | 31             | 21             | 12             | 64       | invalid |                          |
| T5        | 15             | 0              | 15             | 30       | valid   | MN boundaries            |
| T6        | 20             | -1             | 7              | 26       | invalid |                          |
| T7        | 21             | 30             | 6              | 57       | valid   |                          |
| T8        | 22             | 31             | 3              | 56       | invalid |                          |
| T9        | 7              | 3              | 0              | 10       | valid   | LN boundaries            |
| T10       | 2              | 6              | -1             | 7        | invalid |                          |
| T11       | 9              | 18             | 30             | 57       | valid   |                          |
| T12       | 12             | 27             | 31             | 70       | invalid |                          |
| T13       | invalid string | 14             | 20             |          | invalid | invalid classes          |
| T14       | 11             | invalid string | 23             |          | invalid |                          |
| T15       | 9              | 19             | invalid string |          | invalid |                          |
| T16       | 23             | 23             | 22             | 68       | valid   | FN + MN + LN restriction |
| T17       | 23             | 23             | 23             | 69       | invalid |                          |

### Notes:

* We simplified the output by basically returning "valid" or "invalid". You might also wanna check the final name that was generated.
* Test cases with strings of length -1 might then not be possible.


{% set video_id = "JoHwLORk0cw" %}
{% include "/includes/youtube.md" %}

## Exercise 7: Tax Income

Your income is taxed as follows:*

* `0 <= Income < 22100` → **Tax = `0.15 x Income`**
* `22100 <= Income < 53500` → **Tax = `3315 + 0.28 * (Income - 22100)`**
* `53500 <= Income < 115000` → **Tax = `12107 + 0.31 * (Income - 53500)`**
* `115000 <= Income < 250000` → **Tax = `31172 + 0.36 * (Income - 115000)`**
* `250000 <= Income` → **Tax = `79772 + 0.396 * (Income - 250000)`**

### Variables, Types, Ranges

| Variable | Type | Range | Remark |
| -------- | ---- | ----- | ------ |
| Income | double | `[0, infinite]` | input |
| Tax | double | `[0, infinite]` | output |

### Dependency between variables

* Income is used to calculate Tax

### Equivalence partitioning / Boundary analysis

| Variable | Equivalence classes | Invalid classes | Boundaries | Remark |
| -------- | ------------------- | --------------- | ---------- | ------ |
| Income | [0, 22100[ | | -1 | negative number |
|  | | | 0 | |
|  | | | 22099 | 22099.99 is better! |
|  | | | 22100 | |
| | | | | |
| | [22100,53500[ | | 22099 | |
|  | | | 22100 | |
|  | | | 53499 | |
|  | | | 53500 | |
| | | | | |
| | [53500,115000[ | | 53499 | |
|  | | | 53500 | |
|  | | | 114999 | |
|  | | | 115000 | |
| | | | | |
| | [115000,250000[ | | 114999 | |
|  | | | 115000 | |
|  | | | 250000 | |
|  | | | 250001 | |
| | | | | |
| | [250000, infinite[ | | 249999 | |
|  | | | 250000 | |

### Strategy

Test the boundaries (removing duplicates) → 10 tests

### Test cases

|  #  | Income | Tax |
| --- | ------ | --- |
| T1 | -1 | *CANNOT CALC TAX* |
| T2 | 0 | 0 |
| T3 | 22099 | 3314.85 |
| T4 | 22100 | 3315 |
| T5 | 53499 | 12106.72 |
| T6 | 53500 | 12107 |
| T7 | 114999 | 31171.69 |
| T8 | 115000 | 31172 |
| T9 | 249999 | 79771.64 |
| T10 | 250000 | 79772 |

### Question

* Would you consider 22100 and 22099 a duplicate?

{% set video_id = "IaWioXqM1g4" %}
{% include "/includes/youtube.md" %}

### Another approach

One may argue that this functions is continuous in all of its boundaries, which makes the result the same for all
(`on`, `off`) pairs.

As, a consequence when looking just at the results at the boundaries we are not exercising them in the most efficient way.
The whole purpose of boundary testing is to minimize the costs while maximizing fault detection capability.

What can we do to make our test cases more efficient? We can observe that what is really changing when making
the transition from one partition to another is not the result of the function itself, but its derivative. Thus, we can
focus on looking at the derivative for `on` and `off` points.

Let's recall the definition of the derivative of function `f` at point `a`: `f'(a) = lim h -> 0 ((f(a+h) - f(a)) / h)`
To determine the derivative numerically we have to substitute `h` with sufficiently small number. For our case let `h = 0.01`.

We can now look at the boundary between first and second partition. `off = 22099.99` `on = 22100`

By substituting those values into the definition of the derivative, we can determine the derivative of `f` at `off` `on`:

1. `f'(off) = (f(off+h) - f(off)) / h = (f(22099.99 + 0.01) - f(22099.99)) / 0.01 = (f(22100) - f (22099.99)) / 0.01`
2. `f'(on) = (f(on+h) - f(a)) / h = (f(22100 + 0.01) - f(22100)) / 0.01 = (f(22100.01) - f (22100)) / 0.01`

Now we find the expected derivatives for the corresponding off and on points from the specification:

1. `f(off) = 0.15 * off` thus `f'(off) = 0.15`
2. `f(on) = 3315 + 0.28 * (on - 22100)` thus `f'(a) = 0.28`

Now in our tests we can check whether actual derivatives for on and off points are equal to the expectation. What is left
to do is to repeat the whole process for all other boundaries.

Note that if we decide to exercise the boundaries in this way, we have to remember to write the tests for the results too, as correct derivative does not imply correct function.
Maybe the developer, during implementation, instead of `3315 + 0.28 * (on - 22100)` wrote `315 + 0.28 * (on - 22100)`,
which by testing only for derivatives we will not spot.

This example shows that as a software testers we, not only have to identify the boundaries between partitions, but also
think about what is really changing when crossing those boundaries, in order to maximize the efficiency of our test cases.

## Exercise 8: The ATM

An ATM allows you to withdraw 20 to 200 euros (inclusive) in increments of 20.
For the example purposes, the program returns true or false, depending whether the amount required is valid.

### Variables
| Variable |  Type   | Range |
| -------- | ------- | ----- |
|  Amount  | integer | {20, 40, 60, 80, 100, 120, 140, 160, 180, 200}|
|  Valid   | boolean | `true`, `false`

### Dependency among variables

Valid depends on the Amount

### Equivalence Partitioning/Boundary Analysis
| Variable | Equivalence classes | Invalid classes | Boundaries |
| -------- | ------------------- | --------------- | ---------- |
|  Amount  | 20 | | 19 |
| | | | 21 |
| | 40 | | 39 |
| | | | 41 |
| | 60 | | 59 |
| | | | 61 |
| | 80 | | 79 |
| | | | 81 |
| | 100 | | 99 |
| | | | 101 |
| | 120 | | 119 |
| | | | 121 |
| | 140 | | 139 |
| | | | 141 |
| | 160 | | 159 |
| | | | 161 |
| | 180 | | 179 |
| | | | 181 |
| | 200 | | 199 |
| | | | 201 |
| | | -1 | |

Note that `-1` may be impossible to test.

### Strategy

* Black-box testing: maybe all the on-points, off-points and invalid points.   
* White-box testing: maybe just on-points, few-off points, invalid points.   
Why? If you know the implementation, you know how hard you need to test it.

| Test case | Amount | Valid (output) |
| --------- | ------ | -------------- |
| T1 | 20 | true |
| T2 | 40 | true |
| T3 | 60 | true |
| T4 | 80 | true |
| T5 | 100 | true |
| T6 | 120 | true | 
| T7 | 140 | true | 
| T8 | 160 | true |
| T9 | 180 | true |
| T10 | 200 | true |
| T11 | 19 | false |
| T12 | 41 | false |
| T13 | 61 | false |
| T14 | -1 | false |

{% set video_id = "A9sjzHGcpiA" %}
{% include "/includes/youtube.md" %}

## Exercise 9: Piecewise

The input domain of a function is a set of all points (x, y) that meet the criteria:

* `1 < x <= 10`
* `1 <= y <= 10`
* `y <= 14 - x`

### Variables
| Variable | Type | Range |
| -------- | ---- | ----- |
| x | integer | `1 < x <= 10`
| y | integer | `1 <= y <= 10`, `y <= 14 - x`

### Dependency among variables

* `x` and `y` are dependent, since the range in `y` varies according to `x`.

### Equivalence Partitioning/Boundary Analysis
| Variable | Equivalence classes | Invalid classes | Boundaries |
| -------- | ------------------- | --------------- | ---------- |
| x | `1 < x <= 10` | | |
| | | | `(1, in)` |
| | | | `(2, in)` |
| | | | `(10, in)` |
| | | | `(11, in)` |
| y | `1 <= y <= 10` | | |
| | | | `(in, 1)` |
| | | | `(in, 0)` |
| | | | `(in, 10)` |
| | | | `(in, 11)` |
| | `y <= 14 - x` | | |
| | | | `(4, 10)` |
| | | | `(5, 10)` |
| | | | `(10, 4)` |
| | | | `(11, 4)` |


### Strategy
Make tests for all 12 boundaries

| Test case |  x  |  y  | output |
| --------- | --- | --- | ------ |
| T1 | 1 | 5 | false |
| T2 | 2 | 5 | true |
| T3 | 10 | 2 | true |
| T4 | 11 | 2 | false |
| T5 | 3 | 1 | true |
| T6 | 3 | 0 | false |
| T7 | 3 | 10 | true |
| T8 | 3 | 11 | false |
| T9 | 4 | 10 | true |
| T10 | 5 | 10 | false |
| T11 | 10 | 4 | true |
| T12 | 11 | 4 | false |

### Another approach

We might look at the plot of the function. In the plot, we identify 5 boundaries (one at each of the extremes of the figure). As a tester, we can exercise these boundaries.

![Boundary testing chart](img/domain-testing/piecewise-boundary-chart.png)

| Test case |  x  |  y  | output |
| --------- | --- | --- | ------ |
| T1 | 1 | 1 | false |
| T2 | 2 | 1 | true |
| T3 | 10 | 1 | true |
| T4 | 11 | 1 | false |
| T5 | 2 | 0 | false |
| T6 | 2 | 10 | true |
| T7 | 2 | 11 | false |
| T8 | 4 | 10 | true |
| T9 | 4 | 11 | false |
| T10 | 10 | 4 | true |
| T11 | 10 | 5 | false |
| T12 | 11 | 4 | false |
| T13 | 1 | 10 | false |
| T14 | 5 | 10 | false |


{% set video_id = "2c8D_KGqZ-g" %}
{% include "/includes/youtube.md" %}

## Exercise 10: Chocolate bars

A package should store a total number of kilograms. There are small bars (1 kg each) and big bars (5 kg each).
* The input of the program is the number of small bars and big bars available and the total number of kilos to store.
* We should calculate the number of small bars to use, assuming we always use big bars before small bars. Output -1, if it can't be done.

| Variables			| Types		| Ranges	|
| ---				| ---		| ---		|
| Small bars 		| integer	| [0, inf]	|
| Big bars			| integer	| [0, inf]	|
| Total kilos		| integer	| [0, inf]	|
| Used small bars (output)	| integer	| [-1, inf]	|

### Dependency between variables
* Input variables are independent (they don't affect each other's range).
* Output variable depends on the three input values.
* **Constraint**: Use big bars before small bars.

*Analyse it from the perspective of the output variable: how can the input variables influence the result?*

### Equivalence partitioning / Boundary analysis

| Variable					| Equivalence classes	| Boundaries		|
| ---						| ---					| ---				|
| (small, big, total weight)| only big bars			| only big bars -> small and big bars 	|
|							| 						| only big bars -> not enough bars		|
|							| only small bars		| only small bars -> small and big bars	|
|							| 						| only small bars -> not enough bars	|
|							| small and big bars	| small and big bars -> only big bars	|
|							| 						| small and big bars -> only small bars	|
|							| 						| small and big bars -> not enough bars	|
|							| not enough bars		| not enough bars -> only big bars		|
|							| 						| not enough bars -> only small bars	|
|							| 						| not enough bars -> small and big bars	|

![Boundaries in the chocolate bars problem](img/boundary-testing/chocolate-boundaries.png)

### Strategy
* There are 4 equivalence classes and 10 boundaries, but many of these boundaries are actually the same.
  * For example, boundary 1 and 5 are in fact the same boundary. *Boundaries are not directional*

| Test cases	| (Small bars, Big bars, Total weight)	| Used small bars (output)	| Remark	|
| ---			| ---									| ---						| ---		|
| T2			| (10, 2, 10)							| 0							| small and big bars ->	|
| T1			| (10, 1, 10)							| 5							| only big bars			|
| T3			| (10, 1, 10)							| 5							| small and big bars ->	|
| T4			| (10, 0, 10)							| 10						| only small bars		|
| T5			| (5, 0, 5)								| 5							| only small bars ->	|
| T6			| (4, 0, 5)								| -1						| not enough bars		|
| T7			| (4, 2, 10)							| 0							| only big bars ->		|
| T8			| (4, 1, 10)							| -1						| not enough bars		|
| T9			| (3, 1, 8)								| 3							| small and big bars -> |
| T10			| (2, 1, 8)								| -1						| not enough bars (needed more small bars)	|
| T11			| (3, 1, 8)								| 3							| small and big bars ->	|
| T12			| (3, 0, 8)								| -1						| not enough bars (needed more big bars)	|

{% set video_id = "9ij_kqj78eA" %}
{% include "/includes/youtube.md" %}


## References

* Kaner, Cem, Sowmya Padmanabhan, and Douglas Hoffman. The Domain Testing Workbook. Context Driven Press, 2013.

* Kaner, Cem. What Is a Good Test Case?, 2003. URL: http://testingeducation.org/BBST/testdesign/Kaner_GoodTestCase.pdf
# Property-Based Testing

We briefly mentioned property checks when talking about design-by-contracts.
There, we used properties in assertions.
We can also use properties for test case generation instead of just assertions.

Given that these properties should always hold, we can test them with any input that we like.
We typically make use of a generator to try a large number of different inputs,
without the need of writing them all ourselves.

These generators often create a series of random input values for a test function.
The test function then checks if the property holds using an assertion.
For each of the generated input values, this assertion is checked. If we find an input value that makes the assertion to fail, we can affirm that the property does not hold.

The first implementation of this idea was called *QuickCheck* and was originally developed for Haskell.
Nowadays, most languages have an implementation of quick check, including Java.
The Java implementation we are going to use is [jqwik](https://jqwik.net).

{% hint style='tip' %}
Jqwik has several features to better support property-based tests. In this chapter, we only skim through some of them. We recommend readers to dive into jqwik's manual.
{% endhint %}

## Getting started with property-based tests

How does it work?

* First, we define properties.
Similar to defining test methods, we use an annotation on a method with an assertion to define a property: `@Property`.
QuickCheck includes a number of generators for various types.
For example, Strings, Integers, Lists, Dates, etc.

* To generate values, we add some parameters to the annotated method.
The arguments for these parameters will then be automatically generated by jqwik.
Note that the existing generators are often not enough when we want to test one
of our own classes; in these cases, we can create a custom generator which generates random values for this class.

* jqwik handles the number of generated inputs. After all, generating random values for the test input is tricky:
the generator might create too much data to efficiently handle while testing.

* Finally, as soon as jqwik finds a value that breaks the property, it
starts the shrinking process.
Using random input values can result in very large inputs.
For example, lists that are very long or strings with a lot of characters.
These inputs can be very hard to debug. Smaller inputs are preferable when it comes to testing.
When an input makes a property fail, jqwik tries to find a shrunken version of this input that still makes the property fail.
That way it gets the smallest part of a larger input that actually causes the problem.

As an example:
a property of Strings is that if we add two strings together, the length of the result should be the same as the sum of the lengths of the two strings summed.
We can use property-based testing and jqwik's implementation to make tests for this property.

```java
public class PropertyTest {

  @Property
  void concatenationLength(@ForAll String s1, @ForAll String s2) {
    String s3 = s1 + s2;

    Assertions.assertEquals(s1.length() + s2.length(), s3.length());
  }
}
```

`concatenationLength` had the `Property` annotation, so QuickCheck will generate random values for `s1` and `s2` and execute the test with those values.

Property-based testing changes the way we automate our tests.
We have only been automating the execution of our tests; the design and instantiation of test cases were always done by us, testers.
With property-based testing, by means of QuickCheck's implementation, we also automatically generate the inputs of the tests.


{% set video_id = "7kB6JaSH9p8" %}
{% include "/includes/youtube.md" %}


{% hint style='tip' %}
Note that, in the video, we still use the `@RunWith` annotation that was required in JUnit 4 (back then, we also used the _QuickCheck_ framework, and not _jqwik_. The code in this chapter was updated to _jqwik_, which natively supports JUnit 5. Nevertheless, the underlying idea is still the same.
{% endhint %}

## Other examples

Suppose the following requirement:

> Requirement: Passing grade
>
> A student passes an exam if s/he gets a grade >= 5.0. 
> Grades below that are a fail.
>
> Grades range from [1.0, 10.0].

We can identify two valid classes and one invalid class in the requirement: passing grades and non-passing grades, and grades outside the range.

When doing property-based testing, we declare these properties in form of jqwik's properties:

* The `fail` property: for all floats, ranging from 1 (inclusive) to 5.0 (exclusive), the program should return false.
* The `pass` property: for all floats, ranging from 5 (inclusive) to 10 (inclusive), the program should return true.
* The `invalid` property: for all invalid grades (which we define as any number below 0.9 or greater than 10.1), the program must throw an exception.

```java
public class PassingGradesPBTest {

    private final PassingGrade pg = new PassingGrade();

    @Property
    void fail(@ForAll @FloatRange(min = 1f, max = 5.0f, maxIncluded = false) float grade) {
        assertThat(pg.passed(grade)).isFalse();
    }

    @Property
    void pass(@ForAll @FloatRange(min = 5.0f, max = 10.0f, maxIncluded = true) float grade) {
        assertThat(pg.passed(grade)).isTrue();
    }

    @Property
    void invalid(@ForAll("invalidGrades") float grade) {
        assertThatThrownBy(() -> {
            pg.passed(grade);
        })
        .isInstanceOf(IllegalArgumentException.class);
    }

    @Provide
    private Arbitrary<Float> invalidGrades() {
        return Arbitraries.oneOf(
                Arbitraries.floats().lessOrEqual(0.9f),
                Arbitraries.floats().greaterOrEqual(10.1f));
    }
}

```

See another requirement, also used as an example in jqwik's website:

> Requirement: FizzBuzz
>
> The program must return 'Fizz' to multiples of 3, 
> 'Buzz' to multiples of 5,
> and 'FizzBuzz' to multiples of 3 and 5.
> The program must throw an exception for numbers below 0 (inclusive).

We can derive four clear properties:

* Property `fizz`: for all numbers divisible by 3, and not divisible by 5, the program returns "Fizz" (see the `divisibleBy3` provider method to understand how we feed values with such properties).
* Property `buzz`: for all numbers divisible by 5 (and not divisible by 3), the program returns "Buzz".
* Property `fizzbuzz`: for all numbers divisible by 3 and 5, the program returns "FizzBuzz".
* Property `noZeroesAndNegatives`: the program throws an exception for all numbers that are zero or smaller.

```java
public class FizzBuzzTest {

    private final FizzBuzz fb = new FizzBuzz();

    @Property
    boolean fizz(@ForAll("divisibleBy3") int i) {
        return fb.fizzbuzz(i).equals("Fizz");
    }

    @Property
    boolean buzz(@ForAll("divisibleBy5") int i) {
        return fb.fizzbuzz(i).equals("Buzz");
    }

    @Property
    boolean fizzbuzz(@ForAll("divisibleBy3and5") int i) {
        return fb.fizzbuzz(i).equals("FizzBuzz");
    }

    @Property
    void noZeroesAndNegatives(@ForAll("negative") int i) {
        assertThrows(IllegalArgumentException.class, () -> fb.fizzbuzz(i).equals("FizzBuzz"));
    }

    @Provide
    Arbitrary<Integer> divisibleBy3() {
        return Arbitraries.integers()
                .greaterOrEqual(1)
                .filter(i -> i % 3 == 0)
                .filter(i -> i % 5 != 0);
    }

    @Provide
    Arbitrary<Integer> divisibleBy5() {
        return Arbitraries.integers()
                .greaterOrEqual(1)
                .filter(i -> i % 5 == 0)
                .filter(i -> i % 3 != 0);
    }

    @Provide
    Arbitrary<Integer> divisibleBy3and5() {
        return Arbitraries.integers()
                .greaterOrEqual(1)
                .filter(i -> i % 3 == 0)
                .filter(i -> i % 5 == 0);
    }

    @Provide
    Arbitrary<Integer> negative() {
        return Arbitraries.integers().lessOrEqual(0);
    }
}
```

You may see other examples in our [code repository](https://www.github.com/sttp-book/code-examples).


## Exercises

1. Write property-based tests for all the exercises we discussed in the domain testing chapters and appendix.
# Testing techniques

In a simplified view of a software testing process, 
developers perform two distinct tasks: **designing 
test cases**, and **executing test cases**.

The first task, as we just mentioned, 
is about analysing and designing test cases. In simple words, the goal of this stage 
is to systematically devise different test cases that together will give us a certain level 
of confidence as to whether the software is ready to be shipped or not. 
Devising test cases is an activity that is often done by humans 
(although we will explore the state-of-the-art in software testing research, where machines also try to devise test cases for us). 
After all, it requires good knowledge of the requirements. 
In the _Roman Numeral_ example of a previous chapter, we devised three test cases during the test case design phase.

The second task is about executing the test cases we have devised. 
We often do this by running the software system, feeding it with the inputs we crafted, 
and checking whether the system responded in the way we expected. 
Although this phase can also be done by humans, this is an activity that we can easily automate. 
As we discussed before, we can (and should) write a program that runs our software and executes the test cases.

{% hint style='info' %} 
As a side note, in industry, the term "automated software testing" often relates 
to the automatic *execution* of test cases (that were manually devised). 
In academia, whenever a research paper says "automated software testing", 
it means automatically *creating* test cases (by means of artificial intelligence techniques, for instance). 
{% endhint %}


{% set video_id = "pPv37kPqvAE" %}
{% include "/includes/youtube.md" %}


When it comes to devising test cases, while our experience helps us a lot in finding bugs, it might not be enough:

* Experience-based testing is highly prone to mistakes. The developer might forget to test a corner case.
* It varies from person to person. Our goal is to define techniques such that any developer in the world is able to test any software.
* Without clear criteria, it is difficult to know when to stop testing. Our gut feelings might not be precise enough.

The following chapters aim at exploring different techniques to test a software system effectively, rigorously, and systematically, 
and how to automate as many steps as possible along the way. 
These techniques will rely on the different artefacts that are present during the 
software development process.

More specifically, we will discuss the following topics:

* **Specification-based testing**: Techniques to derive tests from textual requirements. Understanding the _category/partition method_ and _equivalence partitioning_.

* **Boundary testing**: Deriving tests that exercise the boundaries of our requirement.

* **Structural testing**: Test cases based on the structure of the source code.

* **Model-based testing**: Leveraging more formal documentation such as state machines and decision tables to derive tests.

* **Design-by-contracts**: Devising explicit contracts for methods and classes to ensure that they behave correctly when these contracts are (and are not) met.

* **Property-based testing**: Deriving properties of the system (similar to contracts) and using them to automatically generate test cases.



{% set video_id = "xyV5fZsUH9s" %}
{% include "/includes/youtube.md" %}



## References


* Yu, C. S., Treude, C., & Aniche, M. (2019, July). Comprehending Test Code: An Empirical Study. In 2019 IEEE International Conference on Software Maintenance and Evolution (ICSME) (pp. 501-512). IEEE. Chicago
# Boundary testing

Off-by-one mistakes are a common cause for bugs in software systems.
As developers, we have all made mistakes such as using a "greater than" operator (`>`) where it had to be a "greater than or equal to" operator (`>=`). 
Interestingly, programs with such a bug tend to work well for most of the provided inputs. They fail, however, when the input is "near the boundary of condition".

In this chapter, we explore **boundary testing** techniques.

## Boundaries in between classes/partitions

In the previous chapter, we studied specification-based techniques and, more specifically,
we understood the concept of classes/partitions.
When we devise classes, these have "close boundaries"
with the other classes. 
In other words, if we keep performing small changes 
to an input that belongs to some partition (e.g., by adding +1 to it), 
at some point this input will belong to another class. 
The precise point where the input changes from one class to another is what we call a *boundary*.
And this is precisely what boundary testing is about: to make the program behave correctly
when inputs are near a boundary.

More formally,
we can find such boundaries by finding a pair of consecutive 
input values $$[p_1,p_2]$$, where $$p_1$$ belongs to partition A, and $$p_2$$ belongs to partition B.

Let us apply boundary testing in a concrete example:

> **Requirement: Calculating the number of points of a player**
> 
> Given the score of a player and the number of remaining lives of the player, the program does the following:
> - If the player's score is below 50, then it always adds 50 points on top of the current points.
> - If the player's score is greater than or equals to 50, then:
>   - if the number of remaining lives is greater than or equal to 3, it triples the score of the player.
>   - otherwise, it adds 30 points on top of the current points.

A possible implementation for this method can be:

```java
public class PlayerPoints {

  public int totalPoints(int currentPoints, int remainingLives) {
    if(currentPoints < 50)
      return currentPoints+50;

    return remainingLives < 3 ? currentPoints+30 : currentPoints*3;
  }
}
```

When devising the partitions to test this method, a tester might come up with the following partitions:

1. **Less points**: Score < 50
2. **Many points but little lives**: Score >= 50 and remaining lives < 3
3. **Many points and many lives**: Score >= 50 and remaining lives >= 3

Those partitions would lead testers to devise at least three test cases, one per partition:

```java
public class PlayerPointsTest {

  private final PlayerPoints pp = new PlayerPoints();

  @Test
  void lessPoints() {
    assertEquals(30+50, pp.totalPoints(30, 5));
  }

  @Test
  void manyPointsButLittleLives() {
    assertEquals(300+30, pp.totalPoints(300, 1));
  }

  @Test
  void manyPointsAndManyLives() {
    assertEquals(500*3, pp.totalPoints(500, 10));
  }
}
```

However, a tester who is aware of boundaries also devises test cases that explore the boundaries of the domain.
Let us explore them: 

* **Boundary 1:** When the score is strictly smaller than 50, it belongs to partition 1. If the score is greater than or equal to 50, it belongs to partitions 2 and 3. Therefore, we observe the following boundary: when the score changes from 49 to 50, the partition it belongs to also changes (let us call this test B1).

* **Boundary 2:** Given a score that is greater than or equal to 50, we observe that if the number of remaining lives is smaller than 3, it belongs to partition 2; otherwise, it belongs
to partition 3. Thus, we just identified another boundary there (let us call this test B2).

We can visualise these partitions with their boundaries in a diagram.

![Partitions with their boundaries](img/boundary-testing/examples/partition_boundaries.svg)

In our example, the tester would then 
devise and automate test cases B1 and B2. Given that a boundary is composed of two different input values, note that
each boundary will require *at least* two test cases:

For B1:
* B1.1 = input={score=49, remaining lives=5}, output={99}
* B1.2 = input={score=50, remaining lives=5}, output={150}

For B2:
* B2.1 = input={score 500, remaining lives=3}, output={1500}
* B2.2 = input={score 500, remaining lives=2}, output={530}

An implementation using JUnit is shown below. Note that we have written just a single test for each pair of test cases. This makes the test more cohesive.
If there is a boundary bug, a single test will let us know.

```java
@Test
void betweenLessAndManyPoints() {
  assertEquals(49+50, pp.totalPoints(49, 5));
  assertEquals(50*3, pp.totalPoints(50, 5));
}

@Test
void betweenLessAndManyLives() {
  assertEquals(500*3, pp.totalPoints(500, 3));
  assertEquals(500+30, pp.totalPoints(500, 2));
}
```

{% hint style='tip' %}

You might have noticed that, for B1, in case of score < 50, `remaining lives` makes no difference.
However, for score >= 50, `remaining lives` does make a difference, as the output can vary according to its value. 
And for the B1.2 test case, we chose `remaining lives` = 5, which makes the
condition true. 
You might be wondering whether you also need to devise another test case, B1.3, where the remaining lives condition would be exercised as false. 

If you are looking to test all possible combinations, then the answer is yes. However, in longer
conditions, full of boundaries, the number of combinations might be too high, making it unfeasible for the developer
to test them all. Later in this chapter, we will learn
how to choose values for the "boundaries that we do not care about".

{% endhint %}

## On and off points

Given some initial intuition on how to analyse boundaries, let us define some
terminology:

- **On-point:** The on-point is the value that is exactly on the boundary. This is the value we see in the condition itself. 
- **Off-point**: The off-point is the value that is closest to the boundary and that flips the condition. If the on-point makes the condition true, the off point makes it false and vice versa. Note that when dealing with equalities or inequalities (e.g. $$x = 6$$ or $$x \neq 6$$), there are two off-points; one in each direction.
- **In-points**: In-points are all the values that make the condition true.
- **Out-points**: Out-points are all the values that make the condition false.

**Example:** Suppose we have a program that adds shipping costs when the total price is below 100.
The condition used in the program is $$x < 100$$.

* The on-point is $$100$$, as that is the value that is precisely in the condition.
* The on-point makes the condition false (100 is not smaller than 100), so the off-point should be the closest number that makes the condition true.
This will be $$99$$, as $$99 < 100$$ is true.
* The in-points are the values which are smaller than or equal to $$99$$. For example, 37, 42, 56.
* The out-points are all values which are larger than or equal to $$100$$. For example, 325, 1254, 101.

We show all these points in the diagram below.

![On- and off-points, in- and out-points](img/boundary-testing/examples/on_off_points.svg)

Let us now study a similar but slightly different condition: $$x \leq 100$$ (the only difference is that, in this one, we use "less than or equal to"):

- The on-point is still $$100$$: this is the value that is precisely in the condition.
- The condition is evaluated as true for the on-point. So, the off-point should be the closest number to the on-point, but making the condition false. The off-point is thus $$101$$.

![On-, off-, in- and out-points 2](img/boundary-testing/examples/on_off_points2.svg)

Note that, depending on the condition, an on-point can be either an in- or an out-point.

As a tester, you devise test cases for these different points: a test case for the 
on-point, a test case for the off-point, a test case for a single in-point (as all in-points
belong to the same equivalence partition), and a test case for a single out-point (as all
out-points also belong to the same equivalence partition).

{% hint style='tip' %}
Note that _on_ and _off_ points are also _in_ or _out points_. Therefore, tests that focus only on the _on_ and _off_ points would also be testing _in_ and _out_ points. In fact, some authors argue that testing boundaries is enough. Moreover, a test that exercises an in-point that is far away from the boundary might not have a strong fault detection capability. Why would we need them?

There is _no perfect answer_ here. We suggest:

* If the number of test cases is indeed too high, and it is just too expensive to do them all, prioritization is important, and we suggest testers to indeed **focus on the boundaries**.
* Far away in/out points are sometimes easier to be seen or comprehended by a tester who is still learning about the system under test, and exploring its boundaries (_exploratory testing_). Deciding whether to perform such a test is thus a decision that a tester should take, taking the costs into account.
{% endhint %}

## Revisiting the "chocolate bars" problem

Let's revisit the example from the a previous chapter. There, we had a program
where the goal was to return the number of bars needed in order to build some boxes of chocolates:


> **Chocolate bars**
> 
> A package should store a total number of kilos. 
> There are small bars (1 kilo each) and big bars (5 kilos each). 
> We should calculate the number of small bars to use, 
> assuming we always use big bars before small bars. Return -1 if it can't be done.
>
> The input of the program is thus the number of small bars, the number of big bars,
> and the total number of kilos to store.

And these were the classes we derived after applying the category/partition method:

* **Need only small bars**. A solution that only uses the provided small bars.
* **Need only big bars**. A solution that only uses the provided big bars.
* **Need small + big bars**. A solution that has to use both small and big bars.
* **Not enough bars**. A case in which it's not possible, because there are not enough bars.
* **Not from the specs**: An exceptional case.

As we saw in the previous chapter, the following code passed all the tests we derived:

```java
public class ChocolateBars {
    public static final int CANNOT_PACK_BAG = -1;

    public int calculate(int small, int big, int total) {
        int maxBigBoxes = total / 5;
        int bigBoxesWeCanUse = Math.min(maxBigBoxes, big);
        total -= (bigBoxesWeCanUse * 5);

        if(small <= total)
            return CANNOT_PACK_BAG;
        return total;
    }
}
```

However, the following input makes the program to fail: `(2,3,17)`! 

Note that the input `(2,3,17)` belongs to the **need small + big bars** partition. In this case,
the program should make use of all the big bars (there are 3 available) and then *all* the small bars available (there are 
2 available). Note that the buggy program would work if we had 3 available small bars (having `(3, 3, 17)` as input).

The bug lies on the `if` condition. It should be `if(small < total)` instead of
`if(small <= total)`:

```java
public class ChocolateBars {
    public static final int CANNOT_PACK_BAG = -1;

    public int calculate(int small, int big, int total) {
        int maxBigBoxes = total / 5;
        int bigBoxesWeCanUse = Math.min(maxBigBoxes, big);
        total -= (bigBoxesWeCanUse * 5);

        // we fixed the bug here!
        if(small < total)
            return CANNOT_PACK_BAG;
        return total;
    }
}
```

This bug is clearly an instance of a bug that should have been detected by boundary testing.
The problem is that this boundary is just less explicit from the requirements.

As we defined at the beginning of this chapter,
boundaries also happen when we are going from "one partition" to 
another. There is a "single condition" that we can use as clear source.
In these cases, what we should do is to devise test cases for a sequence of inputs that move
from one partition to another.

![Partitions and boundaries](img/boundary-testing/partition-boundary.png)

Let us focus on the bug caused by the `(2,3,17)` input:

* `(1,3,17)` should return *not possible* (1 small bar is not enough). This test case belongs to the **not enough bars** partition.
* `(2,3,17)` should return 2. This test case belongs to **need for small + big bars** partition.

The `(1,3,17)` and `(2,3,17)` inputs exercise precisely the boundary between the **not enough bars** and the **need for small + big bars** partitions. 

Let us now explore the boundaries between other partitions. The figure below shows which boundaries can happen (and that we should test):

![Boundaries in the chocolate bars problem](img/boundary-testing/chocolate-boundaries.png)

Looking at the **only big bars** partition, we should find inputs that transition from this partition to another one:

* `(10, 1, 10)` returns 5. This input belongs to the **need small + big bars** partition.
* `(10, 2, 10)` returns 0. This input belongs to the **need only big bars** partition.

Finally, with the **only small bars** partition:

* `(3, 2, 3)` returns 3. We need only small bars here, and therefore, this input belongs to the **only small bars** partition.
* `(2, 2, 3)` returns -1. We can't make the boxes. This input belongs to the **Not enough bars** partition.

A partition might have boundaries with more than just a single other partition. 
The **only small bars** partition has boundaries not only with the **not enough bars** partition (as we saw above), but also with the **only big bars** partition:

* `(4, 2, 4)` returns 4. We need only small bars here, and therefore, this input belongs to the **only small bars** partition.
* `(4, 2, 5)` returns 0. We need only big bars here, and therefore, this input belongs to the **only big bars** partition.

A lesson we learn from this example is that boundary bugs may not 
only emerge out of "clear `if` conditions" we
see in the implementation. Boundary bugs also happen in more subtle
interactions among partitions.

{% set video_id = "uP_SpXtHxoQ" %}
{% include "/includes/youtube.md" %}



## Automating boundary testing with JUnit (via parameterised tests)

You might have noticed that in the domain matrix we always have a certain number of input values and, implicitly, an expected output value.
We could just implement the boundary tests by making a separate method for each test, or by
grouping them per boundary, as we have been doing so far.

However, the number of test methods can quickly become large and unmanageable.
Moreover, the code in these test methods will be largely the same,
as they all have the same structure, only with different input and output values.

Luckily, JUnit offers a solution where we can generalise the implementation of a test
method, and run it with different inputs and expected outputs: **parameterised Tests**.
As the name suggests, with a parameterised test, developers 
can define a test method with parameters.
To define a parameterised test, you make use of the `@ParameterizedTest` annotation, 
instead of the usual `@Test` annotation.

For each parameter you want to pass to the "template test method", you define a 
parameter in the method's parameter list (note that so far, all our JUnit methods had
no parameters). For example, a test method `t1(int a, int b)` receives two parameters,
`int a` and `int b`. The developer uses these two variables in the body of the test
method, often in places where the developer would have a hard-coded value.

The next step is to feed JUnit with a list of inputs which will be passed
to the test method.
In general, these values are provided by a `Source`.
Here, we will make use of a `CsvSource`.
With it, each test case is given as a comma-separated list of input values.
To execute multiple tests with the same test method, 
the `CsvSource` expects list of strings, where each string represents 
the input and output values for one test case.
The `CsvSource` is an annotation itself, so in an implementation 
it would look like the following: `@CsvSource({"value11, value12", "value21, value22", "value31, value32", ...})`


```java
@ParameterizedTest(name = "small={0}, big={1}, total={2}, result={3}")
    @CsvSource({
      // The total is higher than the amount of small and big bars.
      "1,1,5,0", "1,1,6,1", "1,1,7,-1", "1,1,8,-1",
      // No need for small bars.
      "4,0,10,-1", "4,1,10,-1", "5,2,10,0", "5,3,10,0",
      // Need for big and small bars.
      "0,3,17,-1", "1,3,17,-1", "2,3,17,2", "3,3,17,2",
      "0,3,12,-1", "1,3,12,-1", "2,3,12,2", "3,3,12,2",
      // Only small bars.
      "4,2,3,3", "3,2,3,3", "2,2,3,-1", "1,2,3,-1"
    })
    void boundaries(int small, int big, int total, int expectedResult) {
        int result = new ChocolateBars().calculate(small, big, total);
        Assertions.assertEquals(expectedResult, result);
    }
```

Some developers prefer not to pass a list of CSV/strings. For those, JUnit provides a `@MethodSource` option, which allows developers to provide the input for the parameterised test through a method. The developer simply needs to define a method that returns a `Stream<Arguments>` (and set the name of this method in the `@MethodSource` annotation). See the implementation below:

```java
public class ChocolateBarsTest {

    @ParameterizedTest(name = "small={0}, big={1}, total={2}, result={3}")
    @MethodSource("generator")
    void boundaries(int small, int big, int total, int expectedResult) {
        int result = new ChocolateBars().calculate(small, big, total);
        Assertions.assertEquals(expectedResult, result);
    }

    private static Stream<Arguments> generator() {
      return Stream.of(
        // The total is higher than the amount of small and big bars.
        Arguments.of(1,1,5,0),
        Arguments.of(1,1,6,1),
        Arguments.of(1,1,7,-1),
        Arguments.of(1,1,8,-1),
        // No need for small bars.
        Arguments.of(4,0,10,-1),
        Arguments.of(4,1,10,-1),
        Arguments.of(5,2,10,0),
        Arguments.of(5,3,10,0),
        // Need for big and small bars.
        Arguments.of(0,3,17,-1),
        Arguments.of(1,3,17,-1),
        Arguments.of(2,3,17,2),
        Arguments.of(3,3,17,2),
        Arguments.of(0,3,12,-1),
        Arguments.of(1,3,12,-1),
        Arguments.of(2,3,12,2),
        Arguments.of(3,3,12,2),
        // Only small bars.
        Arguments.of(4,2,3,3),
        Arguments.of(3,2,3,3),
        Arguments.of(2,2,3,-1),
        Arguments.of(1,2,3,-1)
      );

    }
}
```

You can see all these implementation in our GitHub repository: https://github.com/sttp-book/code-examples/tree/master/src/test/java/tudelft/chocolate. 


{% set video_id = "fFksNXJJfiE" %}
{% include "/includes/youtube.md" %}



## The CORRECT way

The book *Pragmatic Unit Testing in Java 8 with JUnit*, by Langr, Hunt, and Thomas, has an interesting discussion about boundary conditions.
Authors call it the **CORRECT** way, as each letter represents one boundary condition to consider:

* **Conformance:**
  * Many data elements must conform to a specific format. Example: e-mail addresses (always name@domain). If you expect an e-mail address, and you do not receive one, your software might crash.
  * Required action: Test what happens when your input is not in conformance with what is expected.

* **Ordering:**
  * Some inputs might come in a specific order. Imagine a system that receives different products to be inserted in a basket. The order of the data might influence the output. What happens if the list is ordered? Unordered?
  * Required action: Make sure our program works even if the data comes in an unordered manner (or return an elegant failure to user, avoiding the crash).

* **Range:**
  * Inputs should usually be within a certain range. Example: Age should always be greater than 0 and smaller than 125.
  * Required action: Test what happens when we provide inputs that are outside of the expected range.

* **Reference:**
  * In OOP systems, objects refer to other objects. Sometimes the relationships between the objects are extensive and there may be external dependencies. What happens if these dependencies do not behave as expected?
  * Required action: When testing a method, consider:
    * What it references outside its scope
    * What external dependencies it has
    * Whether it depends on the object being in a certain state
    * Any other conditions that must exist

* **Existence:**
  * Does "something" really exist? What if it does not? Imagine you query a database, and your database returns an empty result. Will our software behave correctly?
  * Required action: Does the system behave correctly when something that is expected to exist, does not?


* **Cardinality:**
  * In simple words, our loop performed one step less (or more) than it should.
  * Required action: Test loops in different situations, such as when it actually performs zero iterations,
  one iterations, or many. (Loops are further discussed in the structural testing chapter).

* **Time**
  * Systems rely on dates and times. What happens if the system receives inputs that are not
  ordered in regards to date and time?
  * Timeouts: Does the system handle timeouts well?
  * Concurrency: Does the system handle concurrency well?


{% set video_id = "oxNEUYqEvzM" %}
{% include "/includes/youtube.md" %}


## Equivalent classes and boundary analysis altogether

We discussed _equivalent class analysis_ and _boundary testing_. In practice, testers combine both, in what they call _domain testing_.

We suggest the following strategy when applying domain testing, highly influenced by how Kaner et al. do:

1. We read the requirement
2. We identify the input and output variables in play, together with their types, and their ranges.
3. We identify the dependencies (or independence) among input variables, and how input variables influence the output variable.
4. We perform equivalent class analysis (valid and invalid classes).
5. We explore the boundaries of these classes.
6. We think of a strategy to derive test cases, focusing on minimizing the costs while maximizing fault detection capability.
7. We generate a set of test cases that should be executed against the system under test.

See a series of [domain testing examples](/chapters/testing-techniques/domain-testing.html) in our appendix.


## Exercises

**Exercise 1.**
We have the following method.

```java
public String sameEnds(String string) {
  int length = string.length();
  int half = length / 2;

  String left = "";
  String right = "";

  int size = 0;
  for(int i = 0; i < half; i++) {
    left += string.charAt(i);
    right = string.charAt(length - 1 - i) + right;

    if (left.equals(right))
      size = left.length();
  }

  return string.substring(0, size);
}
```

Perform boundary analysis on the condition in the for-loop: `i < half`, i.e. what are the on- and off-point and the in- and out-points?
You can give the points in terms of the variables used in the method.





**Exercise 2.**
Perform boundary analysis on the following equality: `x == 10`.
What are the on- and off-points?


**Exercise 3.**
A game has the following condition: `numberOfPoints <= 570`.
Perform boundary analysis on the condition.
What are the on- and off-point of the condition?
Also give an example for both an in-point and an out-point.



**Exercise 4.**
Regarding **boundary analysis of inequalities** (e.g., `a < 10`), which of the following statements **is true**?

1. There can only be a single on-point which always makes the condition true.
2. There can be multiple on-points for a given condition which may or may not make the condition true.
3. There can only be a single off-point which may or may not make the condition false.
4. There can be multiple off-points for a given condition which always make the condition false.


**Exercise 5.**
A game has the following condition: `numberOfPoints > 1024`. Perform a boundary analysis.


**Exercise 6.**
Perform boundary analysis on the following decision: `n % 3 == 0 && n % 5 == 0`.
What are the on- and off-points?


**Exercise 7.**
Which one of the following statements about the **CORRECT** principles is **true**?

1. We assume that external dependencies are already on the right state for the test (REFERENCE).
1. We test different methods from the same class in an isolated way in order to avoid order issues (TIME).
1. Whenever we encounter a loop, we always test whether the program works for 0, 1, and 10 iterations (CARDINALITY).
1. We always test the behaviour of our program when any expected data does not exist (EXISTENCE).


**Exercise 8.**
We have a program called <ins>IsCat</ins>.
It works as follows:
> Given an list of prerequisites, it returns either the string "Cat" or the string "Doge".
> If the number of legs is an even number, it has a tail, the number of lives left is between [1, 9] inclusive, it has sharp nails and the sounds it produces is "miauw", it is a cat.
> In any other case, it is a doge.

First, do boundary analysis on the inputs.
Think of on and off points for each of the conditions (while picking in points for the others).
Next, appply the category/partition method.
What are the minimal and most suitable partitions?

```java
public class FelineValidator {
    public static final String INVALID_CAT = "doge";

    /**
     *  This function checks whether a certain animal is a cat.
     *  Given the following prerequisites:
     *    If the number of legs is an even number,
     *    it has a tail,
     *    the number of lives left is between [1, 9] inclusive,
     *    it has sharp nails, and
     *    the sounds it produces is "miauw" ...
     *  it is a cat.
     *  In any other case, it is a doge.
     *
     * @param numberOfLegs
     * @param hasTail
     * @param numberOfLives
     * @param hasSharpNails
     * @param sound
     * @return String
     */
    public String isCat(int numberOfLegs, boolean hasTail, int numberOfLives, boolean hasSharpNails, String sound) {
        if (!(numberOfLegs % 2 == 0)) return INVALID_CAT;
        if (!hasTail) return INVALID_CAT;
        if (!(numberOfLives >= 1 && numberOfLives <= 9)) return  INVALID_CAT;
        if (!hasSharpNails) return INVALID_CAT;
        if (!(sound.matches("miauw"))) return INVALID_CAT;

        return "cat";
    }
}
```



## References

* Jeng, B., & Weyuker, E. J. (1994). A simplified domain-testing strategy. ACM Transactions on Software Engineering and Methodology (TOSEM), 3(3), 254-270.

* Chapter 7 of Pragmatic Unit Testing in Java 8 with Junit. Langr, Hunt, and Thomas. Pragmatic Programmers, 2015.

* * Kaner, Cem, Sowmya Padmanabhan, and Douglas Hoffman. The Domain Testing Workbook. Context Driven Press, 2013.

* * Kaner, Cem. What Is a Good Test Case?, 2003. URL: http://testingeducation.org/BBST/testdesign/Kaner_GoodTestCase.pdf
# Structural Testing

In a previous chapter, we discussed how to test software using requirements as the main element to guide the testing. 
In this chapter, we will use the source code itself as a source of information to create tests. 
Techniques that use the structure of the source code as a way to guide the testing, are called **structural testing** techniques.


Understanding structural testing techniques means understanding the different *coverage criteria*. These coverage criteria relate closely to *test coverage*, a concept that many developers know. By test coverage, we mean the amount (or percentage) of production code that is exercised by the tests.

We will cover the following coverage criteria:

- Line coverage (and statement coverage)
- Block coverage
- Branch/Decision coverage
- Condition (Basic and Condition+Branch) coverage
- Path coverage
- MC/DC coverage

Watch a summary of one of our lectures in structural testing:

{% set video_id = "busfqNkpgKI" %}
{% include "/includes/youtube.md" %}

## Why do we need structural testing?

In a nutshell, for two reasons: 
1) to systematically derive tests from source code;
2) to know when to stop testing;

As a tester, when performing specification-based testing, 
your goal was clear: to derive classes out of the requirement specifications, and then to derive test cases for each of the
classes. You were satisfied once all the classes and boundaries
were systematically exercised.

The same idea applies to structural testing. First, it gives
us a systematic way to devise tests. As we will see, a tester
might focus on testing all the lines of a program; or focus
on the branches and conditions of the program. Different
criteria produce different test cases. 

Second, to know
when to stop. It is easy to imagine that the number of possible paths in a mildly complex piece of code is just too large, and exhaustive testing is impossible. Therefore, having clear criteria on when to stop helps testers in understanding
the costs of their testing.


## Line (and statement) coverage

As the name suggests, when determining the line coverage, we look at the number of lines of code that are covered by the tests (more specifically,
by at least one test).

See the following example: 

> **Requirement**: Black-jack
>
> The program receives the number of points of two blackjack players.
> The program must return the number of points of the winner.
> In blackjack, whoever gets closer to 21 points wins. 
> If a player goes over 21 points,
> the player loses. If both players lose, the program must return 0.

See the following implementation for the requirement above:

```java
public class BlackJack {
  public int play(int left, int right) {
1.  int ln = left;
2.  int rn = right;
3.  if (ln > 21)
4.    ln = 0;
5.  if (rn > 21)
6.    rn = 0;
7.  if (ln > rn)
8.    return ln;
9.  else
10.   return rn;
  }
}
```

Let us now devise and implement two test cases for this method:

```java
public class BlackJackTests {
  @Test
  void bothPlayersGoTooHigh() {
    int result = new BlackJack().play(30, 30);
    assertThat(result).isEqualTo(0);
  }

  @Test
  void leftPlayerWins() {
    int result = new BlackJack().play(10, 9);
    assertThat(result).isEqualTo(10);
  }
}
```

The first test executes lines 1-7, 9, and 10 as both values are higher than 21.
This means that, after the `bothPlayersGoTooHigh` test, 9 out of the 10 lines are covered. Thus, line coverage is $$\frac{9}{10}\cdot100\% = 90\%$$.

Line 8 is therefore the only line that the first test does not cover.
The second test, `leftPlayerWins`, complements the first test, and executes lines 1-3, 5, 7 and 8.
Both tests together now achieve a line coverage of $$100\%$$, as together they cover
all the 10 different lines of the program.

More formally, we can compute line coverage as: 

$$\text{line coverage} = \frac{\text{lines covered}}{\text{lines total}} \cdot 100\%$$


{% set video_id = "rkLsvlPlOHc" %}
{% include "/includes/youtube.md" %}


## Why is line coverage problematic?

Using lines of code as a way to determine line coverage is a simple and straightforward idea.
However, counting the covered lines is not always a good way of calculating the coverage.
The number of lines in a piece of code depends on the 
decisions taken by the programmer who writes the code. 

Let us look again at the Black Jack example.
The `play` method can also be written in 6 lines, instead of 10:

```java
public int play(int left, int right) {
1.  int ln = left;
2.  int rn = right;
3.  if (ln > 21) ln = 0;
4.  if (rn > 21) rn = 0;
5.  if (ln > rn) return ln;
6.  else return rn;
}
```

The `leftPlayerWins` test covered $$\frac{6}{10}$$ lines in the previous
implementation of the `play` method.
In this new implementation, it covers lines 1-5, or $$\frac{5}{6}$$ lines.
The line coverage went up from $$60\%$$ to $$83\%$$, 
while testing the same method with the same input.

This urges for a better representation of source code. One that is 
independent of the developers' personal code styles.

{% hint style='tip' %}
Some coverage tools measure coverage at statement level. Statements are the unique instructions that your
JVM, for example, executes. This is a bit better, as splitting one line of code in two would not make a difference, but it is still not good enough.
{% endhint %}

{% set video_id = "iQECMbKLez0" %}
{% include "/includes/youtube.md" %}



## Blocks and Control-Flow Graph

A **control-flow graph** (or CFG) is a representation of all paths that might be traversed during the execution of a piece of code. 
It consists of *basic blocks*, *decision blocks*, and *arrows/edges* that connect these blocks.

Let us use the Black Jack implementation to illustrate the difference between them:

```java
public class BlackJack {
  public int play(int left, int right) {
1.  int ln = left;
2.  int rn = right;
3.  if (ln > 21)
4.    ln = 0;
5.  if (rn > 21)
6.    rn = 0;
7.  if (ln > rn)
8.    return ln;
9.  else
10.   return rn;
  }
}
```

A basic block is composed of "the maximum number of statements that are executed together no matter what happens". In the code above, lines 1-2 are always executed together. Basic blocks are often represented by a square.

At this moment, our control-flow graph looks like the following:

![Black Jack - CFG part 1](img/structural-testing/examples/bj-p1.png)


A decision block, on the other hand, represents all the statements in the source
code that can create different branches. See line 3: `if (ln > 21)`. This `if`
statement creates a decision moment in the application: based on the condition, it is decided which code block will be executed next. Decision blocks are often represented by diamonds. This decision block happens right 
after the basic block we created above, and thus, they are connected by means of
an edge.

![Black Jack - CFG part 2](img/structural-testing/examples/bj-p2.png)

A basic block has always a single outgoing edge. A decision block, 
on the other hand, always has
two outgoing edges (indicating where you go in case of the decision being evaluated to `true`, and where you go in case the decision is evaluated to `false`).

In case of the decision block being evaluated to `true`, line 4 is executed, and the
program continues to line 5.
Otherwise, it proceeds straight to line 5, which is another decision block:

![Black Jack - CFG part 3](img/structural-testing/examples/bj-p3.png)

When you repeat the approach up to the end of the program, you end up with the
following CFG:

![Black Jack - CFG part 4](img/structural-testing/examples/bj-p4.png)

Let us see an example of a more complex CFG:

> **Requirement**: Counting words
>
> Given a sentence, the program should count the number of words 
> that end with either an "s" or an "r".
> A word ends when a non-letter appears.

A possible implementation for this program is:

```java
public class CountLetters {
  public int count(String str) {
1.  int words = 0;
2.  char last = ' ';
3.  for (int i = 0; i < str.length(); i++) {
4.    if (!Character.isLetter(str.charAt(i))
5.        && (last == 's' || last == 'r')) {
6.      words++;
7.    }
8.    last = str.charAt(i);
9.  }
10. if (last == 'r' || last == 's')
11.   words++;
12. return words;
  }
}
```

The corresponding CFG:

![Control flow graph example](img/structural-testing/examples/CFG-branch-example.svg)

Note that we split the `for` loop into three blocks: the variable initialisation, the decision block, and the increment.


**Control-Flow Graphs in other languages.** As you can see, this CFG representation is quite generic. Even when you use a different programming language to write the same program, you might end up with the same CFG. We can devise control-flow graphs for programs in any programming language. For example, see the piece of
Python code below:

```python
# random_ads is a list of ads.
# an ad contains three attributes:
# * available: true/false indicating whether the ad 
#   is still available.
# * reached: true/false indicating 
#   whether the number of paid prints was reached.
# * prints: an integer indicating the 
#   number of times that the ad was printed.
def validate_ads(random_ads):
01. valid_ads = []
02. invalid_ads = []

03. for random_ad in random_ads:
04.   if random_ad.available and not random_ad.reached:
05.     valid_ads.append(random_ad)
06.   else:
07.     invalid_ads.append(random_ad)

08. for valid_ad in valid_ads:
09.   valid_ad.prints += 1

10. return valid_ads, invalid_ads
```

A CFG for this piece of code would look like:

![CFG in Python](img/structural-testing/examples/cfg-python.png)

We applied the same idea we have seen for Java programs in a Python program. The notions of basic and decision blocks are the same. A small difference to note is in the *foreach* loop (which is simply written using the `for` keyword in Python). Given that *foreach* loops do not follow the same format as traditional `for` loops, we modelled it differently: the *foreach* loop is fully represented by a single decision block (i.e., no blocks for the increment, or condition). As with any decision blocks, it has two outcomes, `true` and `false`.


## Block coverage

We can use the control-flow graph to derive tests.
A first idea would be to use *blocks* as a coverage criterion, in the same way we did with lines, but instead of aiming at covering
100% of the lines, we aim at covering 100% of the blocks.

The formula that measures block coverage is similar to
the line coverage formula:

$$\text{block coverage} = \frac{\text{blocks covered}}{\text{blocks total}} \cdot 100\%$$

Note that blocks do not depend on how the developer wrote the code. Thus, it does not suffer from
having different coverage numbers due to different
programming styles.

For the `CountLetters` program, a test T1 = "cats and dogs" exercises all the blocks, and thus,
reaches 100% block coverage (follow the input in the control-flow graph and see all the blocks being executed):

```java
@Test
void multipleWords() {
  int words = new CountLetters().count("cats|dogs");
  assertEquals(2, words);
}
```



## Branch/Decision coverage

Complex programs often rely on lots of complex conditions (e.g., `if` statements composed of many conditions).
When testing these programs, aiming at 100% line or block coverage might not be enough to cover all the cases we want.
We need a stronger criterion.

Branch coverage (or decision coverage) works similar to line and statement coverage, except with branch coverage we count (or aim at covering) all the possible decision outcomes.


A test suite will achieve 100% branch (or decision) coverage when tests exercise all the possible outcomes of decision blocks:

$$\text{branch coverage} = \frac{\text{decision outcomes covered}}{\text{decision outcomes total}} \cdot 100\%$$

Decisions (or branches) are easy to identify in a CFG. 
Arrows with either `true` or `false` (i.e., both the arrows going out of a decision block) are branches, and therefore must be exercised.

Let's aim at 100% branch coverage for the Count Letter's `count` implementation above: 

```java
public class CountLettersTests {
  @Test
  void multipleMatchingWords() {

    int words = new CountLetters()
        .count("cats|dogs");

    assertEquals(2, words);
  }

  @Test
  void lastWordDoesntMatch() {

    int words = new CountLetters()
        .count("cats|dog");

    assertEquals(1, words);
  }
}
```

* The first test (by providing `cats|dogs` as input) covers all the branches in the left part of the CFG.
The right part covers the top `false` branch, because at some point `i` will be equal to `str.length()`.
The word "dogs" ends with an 's', so it also covers the `true` branch on the right side of the CFG.
This gives the test $$\frac{5}{6} \cdot 100\% = 83\%$$ branch coverage.

* The only branch that is now not covered is the `false` branch at the bottom right of the CFG.
This branch is executed when the last word does not end with an 'r' or an 's'.
The second test executes this branch, by providing the string `cats|dog` as input. Thus, the two tests together achieve a branch/decision coverage of $$100\%$$.

{% hint style='tip' %}
In the video, we use _squares_ to represent decision blocks. We did it just because otherwise the control-flow graph would not fit in the video. When doing control-flow graphs, please use _diamonds_ to represent decision blocks.
{% endhint %}

{% set video_id = "XiWtG8PKH-A" %}
{% include "/includes/youtube.md" %}


## (Basic) condition coverage

Branch coverage gives two branches for each decision, no matter how complicated or complex the decision is.
When a decision gets complicated, i.e., it contains more than one condition like `a > 10 && b < 20 && c < 10`, 
branch coverage might not be enough to test all the possible outcomes of all these decisions. 

For example, suppose one aims at testing the decision above.
A test T1 (a=20, b=10, c=5), which makes the condition `true`, and a test T2 (a=5, b=10, c=5), which makes the condition `false`, already fully cover this decision block, in terms of branch coverage. 
However, these two tests do not cover all the
possibilities/different combinations for this decision to be evaluated to `false`; e.g., T3 (a=20, b=30, c=5), etc.

When using *condition coverage* as a criterion, we split each compound condition into multiple decision blocks. This means each of the conditions will be tested separately, and not only the "big decision block".

It is common to then re-design the CFG and make sure each decision block is now composed of a single condition.
With the new CFG in hands (and with it new edges to explore), it works the same as branch coverage. The formula is basically the same, but now there are more decision outcomes to count:

$$\text{condition coverage} = \frac{\text{conditions outcome covered}}{\text{conditions outcome total}} \cdot 100\%$$

We achieve 100% condition coverage when all of the outcomes of
all the conditions in our program have been exercised.
In other words, whenever all the conditions have been `true` and `false` at least once.


Once again we look at the program that counts the words ending with an 'r or an 's'. Let us now focus on achieving 100% (basic) condition coverage.

We start by building a more granular CFG:

![Control Flow Graph example with conditions](img/structural-testing/examples/CFG-condition-example.svg)

You can see that this new CFG has more decision blocks than the previous one (six instead of three).

The `multipleMatchingWords` test now covers 7 out of 12 different decision outcomes.
Condition coverage is thus $$\frac{7}{12} \cdot 100\% = 58\%$$.
This is significantly less than the $$83\%$$ branch coverage that we obtain from the same `multipleMatchingWords` test, 
showing how many more tests one would need 
to achieve 100% condition coverage.


## Condition + Branch coverage

Let's think carefully about condition coverage. If we only focus on exercising the individual conditions themselves, but do not
think of the overall decision, we might end up in a situation like the one below.

Imagine the following program and its respective CFG:

```java
void hello(int a, int b) {
  if(a > 10 & b > 20) {
    System.out.println("Hello");
  } else {
    System.out.println("Hi");
  }
}
```

![Example of why condition+branch coverage is needed, when compared to basic condition coverage](img/structural-testing/examples/cond_plus_branch.png)


A test `T1 = (20, 10)` causes the first condition `a > 10` to be `true`, and the
second condition `b > 20` to be `false`. A test `T2 = (5, 30)` makes the first condition `false`, and the second condition `true`. Note that T1 and T2 together achieve 100% **basic condition** coverage. After all, both conditions `a` and `b` have been exercised as both `true` and `false`. 

However, the final outcome of the entire decision was `false` in both tests. We never saw this program printing "Hello". We found a case where
we achieved 100% basic condition coverage, but only 50% branch coverage. This is not a smart testing strategy. This is why looking only at the conditions themselves while ignoring the overall outcome of the decision block is called
**basic condition coverage**.

In practice, whenever we use condition coverage, we actually perform **branch + condition coverage**. In other words, we make sure
that we achieve 100% condition coverage (i.e., all the outcomes of all conditions are exercised) and 100% branch coverage (all the outcomes
of the compound decisions are exercised).

The formula to calculate branch+condition coverage is as follows. Note how this formula gives us a clear differentiation between basic condition and decision+condition coverage:

$$\text{C/DC coverage} = \frac{\text{conditions outcome covered + decisions outcome covered}}{\text{conditions outcome total + decisions outcome total}} \cdot 100\%$$


{% hint style='tip' %}
While there is some confusion among the different terms, in this book, whenever we mention condition coverage or full condition coverage, we mean condition+branch coverage.

In addition, another common criterion is the _Multiple Condition Coverage_, or MCC. To satisfy the MCC criterion, a condition needs to be exercised in _all_ its possible combinations. That would imply in 2^N tests, given N conditions.
{% endhint %}

{% set video_id = "oWPprB9GBdE" %}
{% include "/includes/youtube.md" %}


## Path coverage

With branch+condition coverage, we looked at each condition and branch individually. Such a criterion gives testers more branches to generate tests, especially when compared to the first criterion we discussed (line coverage).

However, although we are testing each condition to be evaluated as `true` and `false`, this does not ensure testing of *all the paths* that a program can have.

Path coverage does not consider the conditions individually. Rather, it considers the (full) combination of the conditions in a decision.
Each of these combinations is a path. You might see a path as a unique way to traverse the CFG.

The calculation is the same as the other coverages: 

$$\text{path coverage} = \frac{\text{paths covered}}{\text{paths total}} \cdot 100\%$$

See the following example that focus on a small piece of the `count` method:

```java
if (!Character.isLetter(str.charAt(i)) 
        & (last == 's' | last == 'r')) {
    words++;
}
```

The decision in this if-statement contains three conditions and can be generalised to `(A & (B | C))`, with:
* A = `!Character.isLetter(str.charAt(i))`
* B = `last == 's'`
* C = `last == 'r'`

To get $$100\%$$ path coverage, we would have to test all the possible combinations of these three conditions.

We make a truth table to find the combinations:

| Tests | A | B | C | Outcome |
|-------|---|---|---|---------|
| 1     | T | T | T | T       |
| 2     | T | T | F | T       |
| 3     | T | F | T | T       |
| 4     | T | F | F | F       |
| 5     | F | T | T | F       |
| 6     | F | T | F | F       |
| 7     | F | F | T | F       |
| 8     | F | F | F | F       |


This means that, for full path coverage, we would need 8 tests just to cover this `if` statement.
It is a large number for just a single statement. 

While this seems similar to the MCC criterion we quickly discussed above, imagine programs that rely on loops:

```java
boolean shouldRun = true;
while(shouldRun) {
  something();
  something2();

  shouldRun = something3();
}
```

To satisfy all the criteria we studied so far, we would need to exercise the `shouldRun` as being true and false. That does not happen with path coverage. To satisfy path coverage, we would need to test all the possible paths that can happen. The unbounded loop might make this program to iterate an infinite number of times. Imagine now a program with two unbounded loops together. How many different possible paths does this program have?


By aiming at achieving path coverage of our program, testers can indeed come up with good tests.
However, the main issue is that achieving 100% path coverage might not always be feasible or too costly.
The number of tests needed for full path coverage will grow exponentially with the number of conditions in a decision.


{% set video_id = "hpE-aZYulmk" %}
{% include "/includes/youtube.md" %}

## Lazy vs eager operators (and how they affect test case design)

Note that we have been avoiding lazy (short-circuit) operators (i.e., && and ||), on purpose, to make sure all conditions of the expression are evaluated. This allows us to devise test cases for each possible combination we see in the decision table. However, that might not be the case if we use lazy operators. Let's take as an example the same expression, but now using lazy operators: `(A && (B || C))`

We make the truth table to find the combinations:

| Tests | A | B  | C  | Outcome |
|-------|---|----|----|---------|
| 1     | T | T  | dc | T       |
| 2     | T | F  | T  | T       |
| 3     | T | F  | F  | F       |
| 4     | F | dc | dc | F       |

('dc' represents "don't care" values.)

For this particular example, if the A is false, then the rest of the expression will be not evaluated anymore, because the result of the entire statement will be automatically false. Moreover, for the second part of the expression, if B is true, then the entire proposition `(B || C)` is already true, so we "don't care" about the value of the C.

Generically speaking, it might be not possible to devise test cases for all the combinations. As a tester, you just have to take such constraints into consideration.


## Loop boundary adequacy

The section raised an interesting problem:
in terms of coverage criteria, what to do when we have loops? When there is a loop, the block inside of the loop might be executed many times, making testing more complicated.

Think of a `while(true)` loop which can be non-terminating. If we wanted to be rigorous about it, we would have to test the program where the loop block is executed one time, two times, three times, etc. Imagine a `for(i = 0; i < 10; i++)` loop with a `break` inside of the body. We would have to test what happens if the loop body executes one time, two times, three times, ..., up to ten times.
It might be impossible to exhaustively test all the combinations.

How can we handle long-lasting loops (a loop that runs for many iterations), or unbounded loops (where we do not know how many times it will be executed)? 

Given that exhaustive testing is impossible,
testers often rely on the **loop boundary adequacy criterion**
to decide when to stop testing a loop. A test suite satisfies this criterion if and only if for every loop:

* A test case exercises the loop zero times;
* A test case exercises the loop once;
* A test case exercises the loop multiple times.

The idea behind the criterion is to make sure the program
is tested when the loop is never executed (does the program
behave correctly when the loop is simply 'skipped'?), when it only iterates once (as we empirically know that algorithms may not handle single cases correctly), and many times.

Pragmatically speaking, the main challenge comes when devising
the test case for the loop being executed multiple times.
Should the test case force the loop to iterate for 2, 5, or 10 times?
That requires a good understanding of the program/requirement itself. 
Our suggestion for testers is to rely on specification-based techniques. With an optimal understanding of the specs, one should be able to devise good tests for the particular loop.


## MC/DC (Modified Condition/Decision Coverage)

Modified condition/decision coverage (MC/DC) looks at the combinations of conditions like path coverage does.
However, instead of aiming at testing all the possible combinations, we follow a process in order to identify the "important" combinations. The goal of focusing on these important combinations is to manage the large number of test cases that one needs to devise when aiming for 100% path coverage.

The idea of MC/DC is to *exercise each condition 
in a way that it can, independently of the other conditions,
affect the outcome of the entire decision*. 
In short, this means that every possible condition of each parameter must have influenced the outcome at least once.

If we take the decision block from path coverage example, `A && (B || C)`, MC/DC dictates that:
* For condition A:
  * There must be one test case where `A=true` (say T1). 
  * There must be one test case where `A=false` (say T2).
  * T1 and T2 (which we call _independence pairs_) must have different outcomes (e.g., T1 makes the entire decision to evaluate to true, and T2 makes the entire decision to evaluate to false, or the other way around).
  * In both test cases T1 and T2, variables B and C should be the same.
* For condition B:
  * There must be one test case where `B=true` (say T3). 
  * There must be one test case where `B=false` (say T4).
  * T3 and T4 have different outcomes.
  * In both test cases T3 and T4, variables A and C should be the same.
* For condition C:
  * There must be one test case where `C=true` (say T5). 
  * There must be one test case where `C=false` (say T6).
  * T3 and T4 have different outcomes,
  * In both test cases T3 and T4, variables A and B should be the same.
    
Cost-wise, a relevant characteristic of MC/DC coverage is that, supposing that conditions only have binary outcomes (i.e., `true` or `false`), the number of tests required to achieve 100% MC/DC coverage is, on average, $$N+1$$, where $$N$$ is the number of conditions in the decision. 
Note that $$N+1$$ is definitely smaller than all the possible combinations ($$2^N$$)!

Again, to devise a test suite that achieves 100% MC/DC coverage, we should devise $$N+1$$ test cases that, when combined, 
exercise all the combinations independently from the others.

The question is how to select such test cases. See the example below.

Imagine a program that decides whether an applicant should be admitted to the 'University of Character':
```java
void admission(boolean degree, boolean experience, boolean character) {
    if (character && (degree || experience)) {
        System.out.println("Admitted");
    } else {
        System.out.println("Rejected");
    }
}
```

The program takes three booleans as input (which, generically speaking, is the same as the `A && (B || C)` we just discussed): 
* Whether the applicant has a good character (`true` or `false`),
* Whether the applicant has a degree (`true` or `false`),
* Whether the applicant has experience in a field of work (`true` or `false`).

If the applicant has good character _and_ either a degree _or_ experience in the field, he/she will be admitted.
In any other case the applicant will be rejected.

To test this program, we first use the truth table for `A && (B || C)` to see all the combinations and their outcomes.
In this case, we have 3 decisions and $$2^3$$ is 8, therefore we have tests that go from 1 to 8:

| Tests | Character | Degree | Experience | Decision |
|-------|:---------:|:------:|:----------:|----------|
| 1     | T         | T      | T          | T        |
| 2     | T         | T      | F          | T        |
| 3     | T         | F      | T          | T        |
| 4     | T         | F      | F          | F        |
| 5     | F         | T      | T          | F        |
| 6     | F         | T      | F          | F        |
| 7     | F         | F      | T          | F        |
| 8     | F         | F      | F          | F        |

Our goal will be to apply the MC/DC criterion to these test cases,
and select $$N+1$$, in this case $$3+1=4$$, tests.
In this case, the 4 four tests that satisfy that MC/DC coverage is {2, 3, 4, 6}.

How did we find them?
We go test by test, condition by condition.

We start with selecting the pairs of combinations (or tests) for the `Character` parameter.

* In test 1, we see that `Character`, `Degree`, and `Experience` are all `true` and the `Decision` (i.e., the outcome of the entire boolean expression) is also `true`. We now look for another test in this table, where only the value of `Character` is the opposite of the value in test 1,
but the others (`Degree` and `Experience`) are still the same. This means we have to look for a test where `Character = false`, `Degree = true`, `Experience = true`, and `Decision = false`. This combination appears in test 5. 

  Thus, we just found a pair of tests (again, called _independence pairs_), $$T_1$$ and $$T_5$$, where `Character` is the only parameter which changed and the outcome (`Decision`) changed as well.
  In other words, a pair of tests where the `Character` **independently** influences the outcome (`Decision`). Let's keep the pair $$\{T_1, T_5\}$$ in our list of test cases.

* We could have stopped here and moved to the next variable. After all, we already found an independence pair for `Character`. However, finding them all might help us in reducing the number of test cases at the end, as you will see. So let us continue and we look at the next test. In test 2, `Character = true`, `Degree = true`, `Experience = false`, and `Decision = true`. We repeat the process and search for a test where `Character` is the opposite of the value in test 2, but `Degree` and `Experience` remain the same (`Degree = true`, `Experience = false`). This set appears in test 6.

    This means we just found another pair of tests, $$T_2$$ and $$T_6$$, where `Character` is the only parameter which changed and the outcome (`Decision`) changed as well.

* Again, we repeat the process for test 3 (`Character = true`, `Degree = false`, `Experience = true`) and find that the `Character` parameter in test 7 (`Character = false`, `Degree = false`, `Experience = true`) is the opposite of the value in test 3 and changes the outcome (`Decision`). 

* For test 4 (`Character = true`, `Degree = false`, `Experience = false`). Its pair is test 8 (`Character = false`, `Degree = false`, `Experience = false`). Now, the outcome of both tests is the same (`Decision = false`). This means that the pair $$\{T_4, T_8\}$$ does not show how `Character` can independently affect the overall outcome; after all, `Character` is the only thing that changes in these two tests, but the outcome is still the same.

As we do not find another suitable pair when repeating the process for tests 5, 6, 7 and 8, we move on from the `Character` parameter to the `Degree` parameter. We repeat the same process, but now we search for the opposite value of parameter `Degree` whilst `Character` and `Experience` stay the same.

* For test 1 (`Charater = true`, `Degree = true`, `Experience = true`), we search for a test where (`Charater = true`, `Degree = false`, `Experience = true`). This appears to be the case in test 3. However, the outcome for both test cases stay the same. Therefore, $$\{T_1, T_3\}$$ does not show how the `Degree` parameter can independently affect the outcome.

* After repeating all the steps for the other tests we find only $$\{T_2, T_4\}$$ to have different values for the `Degree` parameter where the outcome also changes.

* Finally we move to the `Experience` parameter. As with the `Degree` parameter, there is only one pair of combinations that will work, which is $$\{T_3, T_4\}$$. 

We highly recommend carrying out the entire process yourself to get a feeling of how the process works!

We now have all the pairs for each of the parameters:
- `Character`: {1, 5}, {2, 6}, {3, 7}
- `Degree`: {2, 4}
- `Experience`: {3, 4}

Having a single independence pair per variable (`Character`, `Degree` and `Experience`) is enough. After all, we want to minimise the total number of tests, and we know that we can
achieve this with $$N+1$$ tests.

We do not have any choices with conditions `Degree` and `Experience`, as we found only one pair of tests for each parameter.
This means that we have to test combinations 2, 3 and 4.

Lastly, we need to find the appropriate pair of A. Note that any
of them would fit. However, we want to reduce the total amount
of tests in the test suite (and again, we know we only need 4 in this case).

If we were to pick either test 1 or test 5 we would have to include either test 5 or test 1 as well, 
as they are their opposites, but therefore unneccesarily increasing our number of tests.
In order to keep our test cases in accordance to $$N+1$$ or in this case $$3+1$$, thus 4 test cases we can 
either add test 6 or test 7, as their opposites (test 2 or 3) are already included in our test cases.
Randomly, we pick test 6.

{% hint style='tip' %}
You can indeed have more than one set of tests that achieve 100% MC/DC. All solutions are equally acceptable.
{% endhint %}

Therefore, the tests that we need for 100% MC/DC coverage are {2, 3, 4, 6}.
These are the only 4 tests we need. This is indeed cheaper when compare to the 8 tests we would need for path coverage.

Let us now discuss some details about the MC/DC coverage:

* We have applied what we call unique-cause MC/DC criteria. We identify an independence pair (T1, T2), where only a single condition changes between T1 and T2, as well as the final outcome. That might not be possible in all cases. For example, `(A and B) or (A and C)`. Ideally, we would demonstrate the independence of the first A, B, the second A, and C. It is however impossible to change the first A and not change the second A. Thus, we can not demonstrate the independence of each A in the expression. In such cases, we then allow A to vary, but we still fix all other variables (this is what is called masked MC/DC).

* It might not be possible to achieve MC/DC coverage in some expressions. See `(A and B) or (A and not B)`. While the independence pairs (TT, FT) would show the independence of A, there are no pairs that show the independence of B. While logically possible, in such cases, we recommend the developer to revisit the (degenerative) expression as it might had been poorly designed. In our example, the expression could be reformulated to simply `A`.

* Mathematically speaking, $$N+1$$ is the minimum number of tests required for MC/DC coverage (and $$2 * N$$ the theoretical upper bound). However, empirical studies indeed show that $$N+1$$ is often the required number of tests.


{% set video_id = "HzmnCVaICQ4" %}
{% include "/includes/youtube.md" %}




## Criteria subsumption

You might have noticed that the criteria we studied became more rigorous and demanding throughout this chapter. We started our discussion with line coverage. Then we discussed branch coverage, and we noticed that we could generate more tests if we focused on branches. Then, we discussed branch + condition coverage, and we noticed that we could generate even more tests, if we also focused on the conditions.

There is a relationship between these criteria. Some strategies **subsume** other strategies. 
Formally, a strategy X subsumes strategy Y if all elements that Y exercises are also exercised by X. You can see in the figure below the relationship between the coverage criteria we studied.

![Criteria subsumption](img/structural-testing/subsumption.png)<!--{width=50%}-->

For example, in the picture, one can see that branch coverage subsumes line coverage. This means that 100% of branch coverage always implies 100% line coverage. However, 100% line coverage does not imply 100% branch coverage. Moreover, 100% of branch + condition coverage always implies 100% branch coverage and 100% line coverage.

## The effectiveness of structural testing

A common question among practitioners is whether
structural testing or, in their words, test coverage,
matters.

While researchers have not yet found a magical coverage
number that one should aim for, they have been finding
interesting evidence pointing towards the benefits
of performing structural testing.

We quote two of these studies:

* Hutchins et al.: "Within the limited domain of our experiments, test sets achieving coverage levels over 90% usually showed significantly better fault detection than randomly chosen test sets of the same size. In addition, significant improvements in the effectiveness of coverage-based tests usually occurred as coverage increased from 90% to 100%. However, the results also indicate that 100% code coverage alone is not a reliable indicator of the effectiveness of a test set."
* Namin and Andrews: "Our experiments indicate that coverage is sometimes correlated with effectiveness when [test suite] size is controlled for, and that using both size and coverage yields a more accurate prediction of effectiveness than [test suite] size alone. This in turn suggests that both size and coverage are important to test suite effectiveness."

For interested readers, an extensive literature review on the topic can be found in
Zhu, H., Hall, P. A., & May, J. H. (1997). Software unit test coverage and adequacy. ACM computing surveys (csur), 29(4), 366-427.

## Structural testing vs structural coverage

A common misconception among practitioners to *confuse structural testing with structural coverage*.

Structural testing means *leveraging the structure of the source code to systematically exercise the system under test*. When compared to specification-based testing, we note that structural testing is: 

- More objective. In other words, it does not depend on the opinions and experience of the tester. While different testers might come up with different specification-based tests, they would come with similar structural tests.

- Implementation-aware. Implementations can vary from the specifications. After all, there are so many ways one can implement a program. Structural testing enables testers to explore the precise implementation.

On the other hand, structural testing is a _check and balance_ (as Chilenski puts it) on the specification-based tests. Structural testing confirms and complements the tests that we derived before.

It is common to see developers running their coverage tools and writing tests for the outputs they observe. Developers that are mostly focused on (simply) achieving high _structural coverage_ are missing the main point of structural testing. 

Again, structural testing should complement your requirements-based testing. As Chilenski suggests (see Figure 3 in his paper), the first step of a tester should be to derive test cases out of any requirements-based technique. Once requirements are fully covered, testers then perform structural testing to cover what is missing from the structural-point of view. Any divergences should be brought back to the requirements-based testing phase: _why did we not find this class/partition before?_ Once requirements and structure are covered, one can consider the testing phase done.

Therefore, do not aim at 100% coverage. Use structural testing to complement your specification-based tests.

## Exercises

For the first couple of exercises we will use the following code:

```java
public boolean remove(Object o) {
01.  if (o == null) {
02.    for (Node<E> x = first; x != null; x = x.next) {
03.      if (x.item == null) {
04.        unlink(x);
05.        return true;
         }
       }
06.  } else {
07.    for (Node<E> x = first; x != null; x = x.next) {
08.      if (o.equals(x.item)) {
09.        unlink(x);
10.        return true;
         }
       }
     }
11.  return false;
}
```

This is the implementation of JDK8's LinkedList remove method. Source: [OpenJDK](http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/LinkedList.java).

**Exercise 1.**
Give a test suite (i.e. a set of tests) that achieves $$100\%$$ **line** coverage on the `remove` method.
Use as few tests as possible.

The documentation on Java 8's LinkedList methods, that may be needed in the tests, can be found in its [Javadoc](https://devdocs.io/openjdk~8/java/util/linkedlist).


**Exercise 2.**
Create the control-flow graph (CFG) for the `remove` method.



**Exercise 3.**
Look at the CFG you just created. Which of the following sentences **is false**?

1. A minimal test suite that achieves 100% basic condition coverage has more test cases than a minimal test suite that achieves 100% branch coverage.

2. The method `unlink()` is for now treated as an 'atomic' operation, but also deserves specific test cases, as its implementation might also contain decision blocks.

3. A minimal test suite that achieves 100% branch coverage has the same number of test cases as a minimal test suite that achieves 100% full condition coverage.

4. There exists a single test case that, alone, is able to achieve more than 50% of line coverage.







**Exercise 4.**
Give a test suite (i.e. a set of tests) that achieves $$100\%$$ **branch** coverage on the `remove` method.
Use as few tests as possible.

The documentation on Java 8's LinkedList methods, that may be needed in the tests, can be found in its [Javadoc](https://devdocs.io/openjdk~8/java/util/linkedlist).



**Exercise 5.**
Consider the decision `(A or C) and B` with the corresponding decision table:

<table>
    <tr><th>Decision</th><th>A</th><th>B</th><th>C</th><th>(A | C) & B</th></tr>
    <tr><td>1</td><td>T</td><td>T</td><td>T</td><td>T</td></tr>
    <tr><td>2</td><td>T</td><td>T</td><td>F</td><td>T</td></tr>
    <tr><td>3</td><td>T</td><td>F</td><td>T</td><td>F</td></tr>
    <tr><td>4</td><td>T</td><td>F</td><td>F</td><td>F</td></tr>
    <tr><td>5</td><td>F</td><td>T</td><td>T</td><td>T</td></tr>
    <tr><td>6</td><td>F</td><td>T</td><td>F</td><td>F</td></tr>
    <tr><td>7</td><td>F</td><td>F</td><td>T</td><td>F</td></tr>
    <tr><td>8</td><td>F</td><td>F</td><td>F</td><td>F</td></tr>
</table>

What is the set with the minimum number of tests needed for $$100\%$$ MC/DC (Modified Condition / Decision Coverage)?


----

For the next three exercises use the code below.
This method returns the longest substring that appears at both the beginning and end of the string without overlapping.
For example, `sameEnds("abXab")` returns `"ab"`.

```java
public String sameEnds(String string) {
01. int length = string.length();
02. int half = length / 2;

03. String left = "";
04. String right = "";

05. int size = 0;
06. for (int i = 0; i < half; i++) {
07.   left = left + string.charAt(i);
08.   right = string.charAt(length - 1 - i) + right;

09.   if (left.equals(right)) {
10.     size = left.length();
      }
    }

11. return string.substring(0, size);
}
```

This code is based on the [same ends problem](https://codingbat.com/prob/p131516).

**Exercise 6.**
Draw the control-flow graph (CFG) of the source code above.




**Exercise 7.**
Give a test case (by the input string and expected output) that achieves 100% line coverage.





**Exercise 8.**
Given the source code of the `sameEnds` method. Which of the following statements is **not correct**?

1. It is possible to devise a single test case that achieves 100% line coverage and 100% decision coverage.
2. It is possible to devise a single test case that achieves 100% line coverage and 100% (basic) condition coverage.
3. It is possible to devise a single test case that achieves 100% line coverage and 100% decision + condition coverage.
4. It is possible to devise a single test case that achieves 100% line coverage and 100% path coverage.


----


Now consider this piece of code for the FizzBuzz problem.
Given an integer `n`, it returns the string form of the number followed by `"!"`.
So the integer 8 would yield `"8!"`.
Except if the number is divisible by 3 it returns "Fizz!" and if it is divisible by 5 it returns "Buzz!".
If the number is divisible by both 3 and 5 it returns "FizzBuzz!"
Based on a [CodingBat problem](https://codingbat.com/prob/p115243).

```java
public String fizzString(int n) {
1.  if (n % 3 == 0 && n % 5 == 0)
2.       return "FizzBuzz!";
3.  if (n % 3 == 0)
4.      return "Fizz!";
5.  if (n % 5 == 0)
6.      return "Buzz!";
7.  return n + "!";
}
```

**Exercise 9.**
Assume we have two test cases with an input integer: T1 = 15 and T2 = 8.

What is the branch+condition coverage these test cases give combined?

What is the decision coverage?



----

The next couple of exercises use Java's implementation of the LinkedList's `computeIfPresent()` method.

```java
public V computeIfPresent(K key, BiFunction<? super K, ? super V, ? extends V> rf) {
01. if (rf == null) {
02.   throw new NullPointerException();
    }
  
03. Node<K,V> e;
04. V oldValue;
05. int hash = hash(key);
06. e = getNode(hash, key);
07. oldValue = e.value;
  
08. if (e != null && oldValue != null) {
  
09.   V v = rf.apply(key, oldValue);
  
10.   if (v != null) {
11.     e.value = v;
12.     afterNodeAccess(e);
13.     return v;
      }
      else {
14.     removeNode(hash, key, null, false, true);
      }
    }
15. return null;
}
```

**Exercise 10.**
Draw the control-flow graph (CFG) of the method above.


**Exercise 11.**
How many tests do we need **at least** to achieve $$100\%$$ line coverage?


**Exercise 12.**
How many tests do we need **at least** to achieve $$100\%$$ branch coverage?


**Exercise 13.**
Which of the following statements concerning the subsumption relations between test adequacy criteria **is true**:


1. MC/DC subsumes statement coverage.
2. Statement coverage subsumes branch coverage.
3. Branch coverage subsumes path coverage.
4. Basic condition coverage subsumes branch coverage.


**Exercise 14.**
A test suite satisfies the loop boundary adequacy
criterion if for every loop L:

1. Test cases iterate L zero times, once, and more than once.
2. Test cases iterate L once and more than once.
3. Test cases iterate L zero times and one time.
4. Test cases iterate L zero times, once, more than once, and N, where N is the maximum number of iterations.


**Exercise 15.**
Consider the expression `((A and B) or C)`.
Devise a test suite that achieves $$100\%$$ *Modified Condition / Decision Coverage* (MC/DC).


**Exercise 16.**
Draw the truth table for expression `A and (A or B)`.

Is it possible to achieve MC/DC coverage for this expression?
Why (not)?

What feedback should you give to the developer, that used this expression, about your finding?




## References

* Chapter 4 of the Foundations of software testing: ISTQB certification. Graham, Dorothy, Erik Van Veenendaal, and Isabel Evans, Cengage Learning EMEA, 2008.

* Chapter 12 of the Software Testing and Analysis: Process, Principles, and Techniques. Mauro Pezzè, Michal Young, 1st edition, Wiley, 2007.

* Zhu, H., Hall, P. A., & May, J. H. (1997). Software unit test coverage and adequacy. ACM computing surveys (csur), 29(4), 366-427.

* Hayhurst, K., Veerhusen, D., Chilenski, J., Rierson, L. A Practical Tutorial on Modified Condition/Decision Coverage, 2001. URL: https://shemesh.larc.nasa.gov/fm/papers/Hayhurst-2001-tm210876-MCDC.pdf. Short version: https://www.cs.odu.edu/~mln/ltrs-pdfs/NASA-2001-20dasc-kjh.pdf.

* Chilenski, J. J. (2001). An investigation of three forms of the modified condition decision coverage (MCDC) criterion. Office of Aviation Research. http://www.tc.faa.gov/its/worldpac/techrpt/ar01-18.pdf

* Cem Kaner on Code Coverage: http://www.badsoftware.com/coverage.htm

* Arie van Deursen on Code Coverage: http://avandeursen.com/2013/11/19/test-coverage-not-for-managers/

* Hutchins, M., Foster, H., Goradia, T., & Ostrand, T. (1994, May). Experiments of the effectiveness of data flow-and control flow-based test adequacy criteria. In Proceedings of the 16th international conference on Software engineering (pp. 191-200). IEEE Computer Society Press.

* Namin, A. S., & Andrews, J. H. (2009, July). The influence of size and coverage on test suite effectiveness. In Proceedings of the eighteenth international symposium on Software testing and analysis (pp. 57-68). ACM.

# Design by Contracts


*Can code test itself?*
In this chapter, we discuss what design by contracts and
property-based testing are.

## Self Testing

A self testing system is, in principle, a system that tests itself.
This may sound a bit weird, so let us take a step back first.

The way we tested systems so far was by creating separate classes for the tests.
The production code and test code were completely separated.
The test suite (consisting of the test classes) exercises and then observes the system under test to check whether it is acting correctly.
If the system does something that is not expected by the test suite, the test suite fails.
The code in the test suite is completely redundant.
It does not add any behaviour to the system.

With self-testing, we "move" part of the test suite into the system itself.
These assertions we insert into production code allows the system to check if it is running correctly by itself.
We do not have to run the test suite, but instead the system can check (part of) its behaviour during its normal execution.
Like with the test suite, if anything is not acting as expected, an error will be thrown.
In software testing, the self-tests are used as an additional check in the system complementary to the test suite.

### Assertions

The simplest form of this self-testing is the *assertion*.
An assertion is a Boolean expression at a specific point in a program
which will be true unless there is a bug in the program.
In other words,
an assertion states that a certain condition has to be true 
at the time the assertion is executed.

In Java, to make an assertion we use the `assert` keyword:

```java
assert <condition> : "<message>";
```

The `assert` keyword checks if the `<condition>` is true.
If it is, nothing happens.
The program just continues its execution as everything is according to plan.
However, if the `<condition>` yields false, the `assert` throws an `AssertionError`.

Let us walk through an example.
Take the implementation of a Stack, of which we just show the `pop` method:

```java
public class MyStack {
  public Element pop() {
    assert count() > 0 : "The stack does not have any elements to pop";

    // ... actual method body ...

    assert count() == oldCount - 1 : "Size of stack did not decrease by one";
  }
}
```

In this method, we check if a condition holds at the start: the stack should have at least one element.
Then, after the actual method, we check whether the count is now one lower than before popping.

These conditions are also known as *pre- and post-conditions*.
We cover these in the following section.

{% hint style='tip' %}
The assertion's message is optional, but can be very helpful 
for debugging.
Always include a message that describes what is going wrong if the assertion is failing.
{% endhint %}


In Java, asserts can be enabled or disabled.
If the asserts are disabled, they will never throw an `AssertionError` even if their conditions are false.
Java's default configuration is to disable the assertions.


To enable the asserts, we have to run Java with a special argument in one of these two ways: `java -enableassertions` or `java -ea`.
When using Maven or IntelliJ, the assertions are enabled automatically when running tests.
With Eclipse or Gradle, we have to enable it ourselves.


{% set video_id = "Tnm0qwsEiyU" %}
{% include "/includes/youtube.md" %}


Note how assertions play the role of test oracles here, but in a slightly
different way. Just to remember: oracles inform us
whether the software behaved correctly.

So far, we only used "value comparison" as oracle. Given an input that
we devised, we knew what output to expect. For example, in a program that
returns the square root of a number, given `n=4`, the value we expect as output
is `2`.
Instead of focusing on a specific instance, like we did so far, property checks, like the ones we are going to see in the remainder of this chapter, are more general rules (properties) that we assert on our code. For example, a program that, given $$n$$, returns $$n^2$$, has a property that it should never return a negative number.

Assertions also serve as an extra safety measure.
If it is crucial that a system runs correctly, we can use the asserts to add some additional testing during the system's execution.

Note that assertions do not replace unit tests.
Assertions are often of a more general nature.
We still need the specific cases in the unit tests. A combination of
both is what we desire.


{% set video_id = "OD0BY8GQs9M" %}
{% include "/includes/youtube.md" %}


## Pre- and Post-conditions

We briefly mentioned pre- and post-condition in an example.
Let us formalise the idea and see how to create good pre- and post-conditions and their influence on the code that we are writing.

Tony Hoare pioneered reasoning about programs with assertions, proposing what is now called **Hoare Triples**.
A Hoare Triple consists of a set of pre-conditions $$\{ P \}$$, a program $$A$$ and a set of post-conditions $$\{ Q \}$$
We can express the Hoare Triple as follows: $$\{ P \}\ A\ \{ Q \}$$.
This can be read as: if we know that $$P$$ holds, and we execute $$A$$, then, we end up in a state where $$Q$$ holds.
If there are no pre-conditions, i.e., no assumptions needed for the execution of $$A$$, we can simply set $$P$$ to true.

In a Hoare Triple, the $$A$$ can be a single statement or a whole program.
We will take $$A$$ to be a method.
As such, $$P$$ and $$Q$$ are the pre- and post-condition of the method $$A$$ respectively.
Now we can write the Hoare Triple as: $$\{ \mathit{pre-conditions} \}\ \mathit{method}\ \{ \mathit{post-conditions} \}$$.

### Pre-conditions

When writing a method, each condition that needs to hold for the method to successfully execute can be a pre-condition.
These pre-conditions can be turned into assertions.


Suppose a class that keeps track of one's favourite books.
Let us define some pre-conditions for the `merge` method.
The method adds the given books to the list of favourite books and then sends some notification to, e.g., a phone.

```java
public class FavoriteBooks {
  List<Book> favorites;

  // ...

  public void merge(List<Book> books) {
    favorites.addAll(books);
    pushNotification.booksAdded(books);
  }
}
```

When creating pre-conditions, testers have to think about what needs to hold to let the method execute.
We can focus on the parameter `books` first.
It cannot be `null`, because then the `addAll` method will throw a `NullPointerException`.
It should also not be empty, because then no books will be added to the favourites.
Finally, there has to be at least a single book that is not yet in the favourites list.

Now let us take a look at `favorites`.
This also cannot be `null` because then it is not possible to call `addAll` on favourites.

This gives us 4 pre-conditions and, thus, 4 assertions in the method:

```java
public class FavoriteBooks {
  // ...

  public void merge(List<Book> books) {
    assert books != null : "The list of books is null";
    assert favorites != null : "The favorites list is null";
    assert !books.isEmpty() : "There are no books in the given list";
    assert !favorites.containsAll(books) : "All books in the given list are already in favorites";

    favorites.addAll(books);
    pushNotification.booksAdded(books);
  }
}
```

The number of assumptions made before a method can be executed (and, with that, the number of pre-conditions) is a design choice.

One might want to *weaken the pre-conditions*, so that the method accepts/is able to handle more situations.
To that aim, we can remove a pre-condition as the method itself can handle the situation where the pre-condition would be false.
This makes the method more generally applicable, but also increases its complexity.
The method always has to check some extra things to handle the cases that could have been pre-conditions.
Finding the balance between the number of pre-conditions and complexity of the method is part of designing the system.


We can remove some of the pre-conditions of the `merge` method by adding some if-statements to the method.
First, we can try to remove the `!books.isEmpty()` assertions.
This means that the method `merge` has to handle empty `books` lists itself.

```java
public class FavoriteBooks {
  // ...

  public void merge(List<Book> books) {
    assert books != null : "The list of books is null";
    assert favorites != null : "The favorites list is null";
    assert !favorites.containsAll(books) : "All books in the given list are already in favorites";

    if (!books.isEmpty()) {
      favorites.addAll(books);
      pushNotification.booksAdded(books);
    }
  }
}
```

By generalising the `merge` method, we have removed one of the pre-conditions.

We can even remove the `!favorites.containsAll(books)` assertion by adding some more functionality to the method.

```java
public class FavoriteBooks {
  // ...

  public void merge(List<Book> books) {
    assert books != null : "The list of books is null";
    assert favorites != null : "The favorites list is null";

    List<Book> newBooks = books.removeAll(favorites);

    if (!newBooks.isEmpty()) {
      favorites.addAll(newBooks);
      pushNotification.booksAdded(newBooks);
    }
  }
}
```

Note that, although we increased the complexity of the method by removing some of its pre-conditions and dealing with these cases in the implementation, the method is now also easier to be called by clients. After all, the method has less pre-conditions to be considered.



### Post-conditions

The pre-conditions of a method hold when the method is called. At the end of the method, its **post-conditions** should hold.
In other words, the post-conditions formalise the effects that a method guarantees to have when it is done with its execution.


The `merge` method of the previous examples does two things.
First, it adds the new books to the `favorites` list.
Let us turn this into a Boolean expression, so we can formulate this as a post-condition.

```java
public class FavoriteBooks {
  // ...

  public void merge(List<Book> books) {
    assert books != null : "The list of books is null";
    assert favorites != null : "The favorites list is null";

    List<Book> newBooks = books.removeAll(favorites);

    if (!newBooks.isEmpty()) {
      favorites.addAll(newBooks);
      pushNotification.booksAdded(newBooks);
    }

    assert favorites.containsAll(books) : "Not all books were added to favorites";
  }
}
```

The other effect of the method is the notification that is sent.
Unfortunately, we cannot easily formalise this as a post-condition.
In a test suite, we would probably mock the `pushNotification` and then use `Mockito.verify` to verify that `booksAdded` was called.


It is important to realise that these post-conditions only have to hold if the pre-conditions held when the method was called. **In other words, if the method's pre-conditions were not fully satisfied, the method might not guarantee its post-conditions.**

You also saw in the example that we could not really write assertions for
some of the post-conditions of the method. 
Post-conditions (and pre-conditions for that matter) might not cover all the possible effects; however,
hopefully they do cover a relevant subset of the possible behaviour.


For complex methods, the post-conditions also become more complex.
That is actually another reason for keeping the method simple.
Suppose a method with multiple return points. That might require the developer to define different post-conditions for each of its different outcomes:

```java
if (A) {
  // ...
  if (B) {
    // ...
    assert PC1;
    return ...;
  } else {
    // ...
    assert PC2;
    return ...;
  }
}
// ...
assert PC3;
return ...;
```

The method above has three conditions and three different return statements.
This also gives us three post-conditions.
In the example, if `A` and `B` are true, post-condition 1 should hold.
If `A` is true but `B` is false, post-condition 2 should hold.
Finally, if `A` is false, post-condition 3 should hold.

The placing of these post-conditions now becomes quite important, so the whole method is becoming rather complex with the post-conditions.
Refactoring the method so that it has just a single return statement
with a general post-condition is advisable.
Otherwise, the post-condition essentially becomes a disjunction of propositions.
Each return statement forms a possible post-condition (proposition) and the method guarantees that one of these post-conditions is met.


### How do weak pre-conditions affect the post-conditions?

Based on what we saw about pre- and post-conditions, we can come up with a few
rules:

* The weaker the pre-conditions, the more situations a method
is able to handle, and the less thinking the client needs to do.
However, with weak pre-conditions, the method will always have to do
the checking.

* The post-conditions are only guaranteed if the pre-conditions held; if not,
the outcome can be anything. With weak pre-conditions the method might have
to handle different situations, leading to multiple post-conditions guarded
by conditions over the inputs or the program state.


{% set video_id = "LCJ91VSS3Z8" %}
{% include "/includes/youtube.md" %}



## Invariants

We have seen that pre-conditions should hold before a method's execution and post-conditions should hold after a method's execution.
Now we move to conditions that always have to hold, before and after a method's execution.
These conditions are called **invariants**.
An invariant is thus a condition that holds throughout the entire lifetime of a system, an object, or a data structure.

In terms of implementation, a simple way of using invariants is by creating a "checker" method.
This method will go through the data structure/object/system and will assert whether the invariants hold.
If an invariant does not hold, it will then throw an `AssertionError`.
For simpler invariants, it is common to see a Boolean method that checks the invariants of the structure/object/system.


Suppose we have a binary tree data structure.
An invariant for this data structure would be that when a parent points to a child, then the child should point to this parent.

We can make a method that checks if this representation is correct in the given binary tree.

```java
public void checkRep(BinaryTree tree) {
  BinaryTree left = tree.getLeft();
  BinaryTree right = tree.getRight();

  assert (left == null || left.getParent() == tree) &&
      (right == null || right.getParent() == tree) :
      "A child does not point to the correct parent";

  if (left != null) {
    checkRep(left);
  }
  if (right != null) {
    checkRep(right);
  }
}
```

In `checkRep()`, we first check if the children nodes of the current node are pointing to this node as parent.
Then, we continue by checking the child nodes the same way we checked the current node.


### Class invariants

In Object-Oriented Programming, these checks can also be applied at the class-level.
This gives us **class invariants**.
A class invariant ensures that its conditions will be true throughout the entire lifetime of the object.

The first time a class invariant should be true is right after its construction (i.e., when the constructor method is done).
The class invariant should also hold after each public method invocation.
Moreover, methods can assume that, when they start, the class invariant holds.

A private method invoked by a public method can leave the object with the class invariant being false.
However, the public method that invoked the private method should then fix this and end with the class invariant again being true.

This is all formalised by Bertrand Meyer as: _"The class invariant indicates that a proposition P can be a class invariant if it holds after construction, and before and after any call to a public method assuming that the public methods are called with their pre-conditions being true."_


To implement a simple class invariant in Java, we can use the Boolean method that checks if the representation is okay.
We usually call this method `invariant`.
We then assert the return value of this method after the constructor, and before and after each public method.
In these public methods, the only pre-conditions and post-conditions that have to hold additionally are the ones that are not in the invariant.

Let us return to the `FavoriteBooks` with the `merge` method.
We had a pre-condition saying that `favorites != null`.
Given that this should always be true, we can turn it into a class invariant.
Additionally, we can add the condition that `pushNotification != null`.

```java
public class FavoriteBooks {
  List<Book> favorites;
  Listener pushNotification;

  public FavoriteBooks(...) {
    favorites = ...;
    pushNotification = ...;

    // ...

    assert invariant() : "Invariant does not hold";
  }

  protected boolean invariant() {
    return favorites != null && pushNotification != null;
  }

  public void merge(List<Book> books) {
    assert invariant() : "Invariant does not hold";

    // Remaining pre-conditions
    assert books != null : "The list of books is null";

    List<Book> newBooks = books.removeAll(favorites);

    if (!newBooks.isEmpty()) {
      favorites.addAll(newBooks);
      pushNotification.booksAdded(newBooks);
    }

    // Remaining post-conditions
    assert favorites.containsAll(books) : "Not all books were added to favorites";

    assert invariant() : "Invariant does not hold";
  }

}
```

Note that the `invariant` method checks the two conditions.
We call `invariant` before and after `merge` and we only assert the pre- and post-conditions that are not covered in the `invariant` method.
We also assert the result of the `invariant` method at the end of the constructor.

{% hint style='tip' %}
When handling more complicated invariants, we can split the invariant into more methods.
Then we use these methods in the `invariant` method.
{% endhint %}

{% set video_id = "T5kwU91W07s" %}
{% include "/includes/youtube.md" %}


## Design by Contracts

Suppose a client system and a server system. The client makes use of the server's API. 
The client and server are bound by a *contract*.
The server does its job as long as its methods are used properly by the client.
This relates strongly to the pre- and post-conditions that we discussed earlier.
The client has to use the server's methods in a way that their pre-conditions hold.
The server then guarantees that the post-conditions will hold after the method call, i.e., makes sure the method delivers what it promises.

Note how the pre- and post-conditions of the server forms a contract
between the server and the client.
Designing a system following such contracts is called what
we call **Design by Contracts**.
In such design, contracts are represented by interfaces.
These interfaces are used by the client and implemented by the server.
The following UML diagram illustrates it:

![Design by Contracts UML diagram](img/design-by-contracts/dbc_uml.svg)

### Subcontracting

Imagine now an interface. This interface has its own pre- and post-conditions.
Now, what happens to these conditions when we create another implementation of this interface (i.e., a class that implements the interface, or a class that extends some base class)?

In the UML diagram above, we see that the implementation can have different pre-, post-conditions, and invariants than its base interface.

In terms of pre-conditions, the new implementation must be able to work with the pre-conditions that were specified in the interface.
After all, the interface is the only thing the client sees of the system.
The implementation cannot add any pre-conditions to the server's pre-conditions.
In terms of strength, we now know that $$P'$$ has to be **weaker** than (or as weak as) $$P$$.

The post-condition works the other way around.
The implementation must do at least the same work as the interface, but is allowed to do a bit more.
Therefore, $$Q'$$ should be **stronger** than (or as strong as) $$Q$$.

Finally, the interface guarantees that the invariant always holds.
Then, the implementation should also guarantee that at least the interface's invariant holds.
So, $$I'$$ should be **stronger** than (or as strong as) $$I$$.

In short, using the notation of the UML diagram:

- $$P'$$ **weaker** than $$P$$
- $$Q'$$ **stronger** than $$Q$$
- $$I'$$ **stronger** than $$I$$

The subcontract (the implementation) requires no more and ensures no less than the actual contract (the interface).

{% set video_id = "aA29jZYdJos" %}
{% include "/includes/youtube.md" %}


### Liskov Substitution Principle

The subcontracting follows the general notion of behavioural subtyping, proposed by Barbara Liskov.
The behavioural subtyping states that if we have a class `T` and this class has some sub-classes,
the clients or users of class `T` should be able to choose any of `T`'s sub-classes.
This notion of behavioural subtyping is now known as the **Liskov Substitution Principle (LSP)**.


In other words, the LSP states that if you use a class, you should be able to replace this class by one of its subclasses.
The sub-contracting we discussed earlier is just a formalisation of this principle.
Proper class hierarchies follow the Liskov Substitution Principle.
Keep the LSP in mind when designing and implementing a software system.

How can we test that our classes follow the LSP?
To test the LSP, we have to make some test cases for the public methods of the super class and execute these tests with all its subclasses.
We could just create the same tests to each of the subclasses' test suites.
However, this leads to a lot of code duplication in the test code, which we would like to avoid.


In Java, the List interface is implemented by various sub-classes, such as `ArrayList` and `LinkedList`.
Creating the tests for each of the sub-classes separately will result in the following structure.

![Test classes architecture](img/design-by-contracts/examples/subclass_test.svg)

The ArrayList and LinkedList will behave the same for the methods defined in List.
Therefore, there will be duplicate tests for these methods.


To avoid this code duplication, we can create a test suite just for the super class.
This test suite tests just the public methods of the super class.
The tests in this test suite should then be executed for each 
of the sub-classes of the super class.
This can be done by making the test classes extend the "super test class".
The "sub test class" will have all the common tests defined in the "super test class" and its own specific tests.

This is how the test suite looks like:

![Parallel Class Hierarchy](img/design-by-contracts/examples/parallel_architecture.svg)

Here, the `ArrayListTest` and `LinkedListTest` extend the `ListTest`.
List is an interface, so the `ListTest` should be abstract.
We cannot instantiate a `List` itself, so we should not be able to execute the `ListTest` without a test class corresponding to one of List's subclasses.
`ListTest` contains all the common tests of `ArrayListTest` and `LinkedListTest`.
`ArrayListTest` and `LinkedListTest` can contain their specific tests.

In the example, one can see that the hierarchy of the test classes is similar to the hierarchy of the classes they test.
Therefore, we say that we use a **parallel class hierarchy** in our test classes.

Implementation-wise,
how do we make sure that the "super test class" executes its tests in the correct subclass?
This depends on the test class that is executed and the subclass it is testing.
If we have class `T`, with subclasses `A` and `B`, then when executing tests for `A` we need the test class of `T` to use an instance of `A`; when executing tests for `B`, we need the test class of `T` to use an instance of `B`.

One way to achieve this behaviour is by using the *Factory Method* design pattern,
which works as follows:
In the test class for the interface level (the "super test class"), we define an abstract method that returns an instance with the interface type.
By making the method abstract, we force the test classes of the concrete implementations to override it.
We return the instance of the specific subclass in the overridden method.
This instance is then used to execute the tests.


We want to use the Factory Method design pattern in our tests for the `List`.
We start by the interface level test class.
Here, we define the abstract method that gives us a List.

```java
public abstract class ListTest {

  protected final List list = createList();

  protected abstract List createList();

  // Common List tests using list
}
```

Then, for this example, we create a class to test the `ArrayList`.
We have to override the `createList` method and we can define any tests specific for the `ArrayList`.

```java
public class ArrayListTest extends ListTest {

  @Override
  protected List createList() {
    return new ArrayList();
  }

  // Tests specific for the ArrayList
}
```

Now, the `ArrayListTest` inherits all the `ListTest`'s tests, so these will be executed when we execute the `ArrayListTest` test suite.
Because the `createList()` method returns an `ArrayList`, the common test classes will use an `ArrayList`.

{% set video_id = "GQ5NTiigkb0" %}
{% include "/includes/youtube.md" %}



## Exercises


**Exercise 1.**
See the code below:

```java
public Square squareAt(int x, int y) {
  assert x >= 0;
  assert x < board.length;
  assert y >= 0;
  assert y < board[x].length;
  assert board != null;

  Square result = board[x][y];

  assert result != null;
  return result;
}
```

What assertion(s), if any, can be turned into a class invariant?



**Exercise 2.**
Consider the piece of code in the previous example.
Suppose we remove the last assertion (line 10), which states that the result can never be null.

Are the existing pre-conditions of the `squareAt` method enough to ensure the property in the original line 10?
What can we add to the class (other than the just removed post-condition) to guarantee this property?



**Exercise 3.**
Your colleague works on a drawing application.
He has created a Rectangle class.
For rectangles, the width and height can be different from each other, but can't be negative numbers.

Your colleague also defines the Square class.
Squares are a special type of rectangle: the width and height should be equal and also can't be negative.
Your colleague decides to implement this by making Square inherit from Rectangle.

The code for the two classes is the following.

```java
class Rectangle {
  protected int width;
  protected int height;

  protected boolean invariant() { return width > 0 && height > 0; }

  public Rectangle(int width, int height) {
    assert width > 0;
    assert height > 0;
    this.width = width;
    this.height = height;
    assert invariant()
  }

  public int area() {
    assert invariant();
    int a = width * height;
    assert a > 0;
    return a;
  }

  public void resetSize(int width, int height) {
    assert width > 0;
    assert height > 0;
    this.width = width;
    this.height = height;
    assert invariant();
  }
}

class Square extends Rectangle {
  public Square(int x) {
    assert x > 0;
    super(x, x);
  }

  @Override
  public void resetSize(int width, int height) {
    assert width == height;
    assert width > 0;
    super.resetSize(width, height);
  }
}
```

Inspired by Bertrand Meyer's design by contracts, he also uses asserts to make sure contracts are followed. He explicitly defines pre-conditions and post-conditions in various methods of the base Rectangle class and the derived Square class.

A second colleague comes in and expresses concerns about the design.
How can you use the assertions provided to discuss the correctness of this design?

Is the second colleague's concern justified?
What principle is violated, if any?
Explain with the assertions shown in the code.


**Exercise 4.**
You run your application with assertion checking enabled. 
Unfortunately, it reports an assertion failure signalling a class invariant violation in one of the libraries your application makes use of.

Assume that the contract of the library in question is correct, and that all relevant pre-conditions are encoded in assertions as well.

Can you fix this problem? 
If so, how? 
If not, why?




**Exercise 5.**
HTTP requests return a status code which can have several values. As explained on [Wikipedia](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes):

- A 4xx status code "is intended for situations in which the error seems to have been caused by the client". 
A well known example is the 404 (Page not found) status code.
- A 5xx status code "indicates cases in which the server is aware that it has encountered an error or is otherwise incapable of performing the request."
A well known example is the 500 (Internal Server Error) status code.

What is the best correspondence between these status codes and pre- and post-conditions?




**Exercise 6.**
A method M belongs to a class C and has a pre-condition P and a post-condition Q. 
Now, suppose that a developer creates a class C' that extends C, and 
creates a method M' that overrides M. 
Which one of the following statements correctly explains the relative
strength of the pre (P') and post-conditions (Q') of the overridden method M'?


1. P' should be equal or weaker than P, and Q' should be equal or stronger than Q.
2. P' should be equal or stronger than P, and Q' should be equal or stronger than Q.
3. P' should be equal or weaker than P, and Q' should be equal or weaker than Q.
4. P' should be equal or stronger than P, and Q' should be equal or weaker than Q.

**Exercise 7.**
Which of the following is a valid reason to use assertions in your code?


1. To verify expressions with side effects.
1. To handle exceptional cases in the program.
1. To conduct user input validation.
1. To make debugging easier.

**Exercise 8.**
Can static methods have invariants? Explain.


## References

* C2 Wiki, What are assertions? http://wiki.c2.com/?WhatAreAssertions

* Mitchell, R., McKim, J., & Meyer, B. (2001). Design by contract, by example. Addison Wesley Longman Publishing Co., Inc..

* Meyer, B. (2002). Design by contract. Prentice Hall.

* Liskov, B. H., & Wing, J. M. (1994). A behavioural notion of subtyping. ACM Transactions on Programming Languages and Systems (TOPLAS), 16(6), 1811-1841.

* "Polymorphic Server Test" in Binder, R. V. (1994). Object-oriented software testing. Communications of the ACM, 37(9), 28-30.

* Regehr, John. Use of Assertions. https://blog.regehr.org/archives/1091
# Software testing automation (with JUnit)

Before we dive into the different testing techniques, let us first get used
to software testing automation frameworks. In this book, we will use JUnit, as
all our code examples are written in Java. If you are using a different programming language in your daily work, note that testing frameworks in other languages offer similar functionalities.

We will now introduce an example program and then use it to demonstrate how to write JUnit tests. 

{% hint style='tip' %}
All the production and test code used in this book can be found in the [code examples](https://github.com/sttp-book/code-examples/) repository.
{% endhint %}

> **Requirement: Roman numerals**
>
> Implement a program that receives a string as a parameter
> containing a roman number and then converts it to an integer.
>
> In roman numerals, letters represent values:
>
> * I = 1
> * V = 5
> * X = 10
> * L = 50
> * C = 100
> * D = 500
> * M = 1000
>
> Letters can be combined to form numbers.
> For example we make 6 by using $$5 + 1 = 6$$ and have the roman number `VI`
> Example: 7 is `VII`, 11 is `XI` and 101 is `CI`.
> Some numbers need to make use of a subtractive notation to be represented.
> For example we make 40 not by `XXXX`, but instead we use $50 - 10 = 40$ and have the roman number `XL`.
> Other examples: 9 is `IX`, 40 is `XL`, 14 is `XIV`.
> 
> The letters should be ordered from the highest to the lowest value.
> The values of each individual letter is added together.
> Unless the subtractive notation is used in which a letter with a lower value is placed in front of a letter with a higher value.
>
> Combining both these principles we could give our method `MDCCCXLII` and it should return 1842.


{% set video_id = "srJ91NRpT_w" %}
{% include "/includes/youtube.md" %}


A possible implementation for the _Roman Numerals_ requirement is as follows:

```java
public class RomanNumeral {
  private static Map<Character, Integer> map;

  static {
    map = new HashMap<>();
    map.put('I', 1);
    map.put('V', 5);
    map.put('X', 10);
    map.put('L', 50);
    map.put('C', 100);
    map.put('D', 500);
    map.put('M', 1000);
  }

  public int convert(String s) {
    int convertedNumber = 0;

    for (int i = 0; i < s.length(); i++) {
      int currentNumber = map.get(s.charAt(i));
      int next = i + 1 < s.length() ? map.get(s.charAt(i + 1)) : 0;

      if (currentNumber >= next) {
        convertedNumber += currentNumber;
      } else {
        convertedNumber -= currentNumber;
      }
    }

    return convertedNumber;
  }
}
```

With the implementation in hands, the next step is to devise
test cases for the program.
Use your experience as a developer to devise as many test cases as you can.
To get you started, a few examples: 

* T1 = Just one letter, e.g., C should equal 100
* T2 = Different letters combined, e.g., CLV = 155
* T3 = Subtractive notation, e.g., CM = 900

In future chapters, we will explore how to devise those test cases. The output
of that stage will often be similar to the one above: a test case number,
an explanation of what the test is about (we will later call it _class_ or _partition_),
and a concrete instance of input that exercises the program in that way, together
with the expected output.

Once you are done with the "manual task of devising test cases", you are ready to move on to the next section, which shows how to turn them into automated test cases using JUnit.

## The JUnit Framework

Testing frameworks enable us to write test cases in a way that they can be easily executed by the machine. In Java, the standard framework to write automated tests is JUnit, and its most recent version is 5.x.

The steps to create a JUnit class/test is often the following:

* Create a Java class under the directory `/src/test/java/roman/` (or whatever test directory your project structure uses). As a convention, the name of the test class is similar to the name of the class under test. For example, a class that tests the `RomanNumeral` class is often called `RomanNumeralTest`. In terms of package structure, the test class also inherits the same package as the class under test.

* For each test case we devise for the program/class, we write a test method. A JUnit test method returns `void` and is annotated with `@Test` (an annotation that comes from JUnit 5's `org.junit.jupiter.api.Test`). The name of the test method does not matter to JUnit, but it does matter to us. A best practice is to name the test after the case it tests. 

* The test method instantiates the class under test and invokes the method under test. The test method passes the previously defined input in the test case definition to the method/class. The test method then stores the result of the method call (e.g., in a variable).

* The test method asserts that the actual output matches the expected output. The expected output was defined during the test case definition phase. To check the outcome with the expected value, we use assertions. An assertion checks whether a certain expectation is met; if not, it throws an `AssertionError` and thereby causes the test to fail. A couple of useful assertions are:

  * `Assertions.assertEquals(expected, actual)`: Compares whether the expected and actual values are equal. The test fails otherwise. Be sure to pass the expected value as the first argument, and the actual value (the value that comes from the program under test) as the second argument.
    Otherwise the fail message of the test will not make sense.
  * `Assertions.assertTrue(condition)`: Passes if the condition evaluates to true, fails otherwise.
  * `Assertions.assertFalse(condition)`: Passes if the condition evaluates to false, fails otherwise.
  * More assertions and additional arguments can be found in [JUnit's documentation](https://junit.org/junit5/docs/current/api/org.junit.jupiter.api/org/junit/jupiter/api/Assertions.html). To make easy use of the assertions and to import them all in one go, you can use `import static org.junit.jupiter.api.Assertions.*;`.


The three test cases we have devised can be automated as follows:

```java
import static org.junit.jupiter.api.Assertions.*;
import org.junit.jupiter.api.Test;

public class RomanNumeralTest {

  @Test
  void convertSingleDigit() {
    RomanNumeral roman = new RomanNumeral();
    int result = roman.convert("C");

    assertEquals(100, result);
  }

  @Test
  void convertNumberWithDifferentDigits() {
    RomanNumeral roman = new RomanNumeral();
    int result = roman.convert("CCXVI");

    assertEquals(216, result);
  }

  @Test
  void convertNumberWithSubtractiveNotation() {
    RomanNumeral roman = new RomanNumeral();
    int result = roman.convert("XL");

    assertEquals(40, result);
  }
}
```

At this point, if you see other possible test cases (there are!), go ahead and implement them.


{% set video_id = "XS4-93Q4Zy8" %}
{% include "/includes/youtube.md" %}

## Test code engineering matters

In practice, developers write (and maintain!) thousands of test code lines. 
Taking care of the quality of test code is therefore of utmost importance.
Whenever possible, we will introduce you to some best practices in test
code engineering.

In the test code above, we create the `roman` object four times.
Having a fresh clean instance of an object for each test method is a good idea, as 
we do not want "objects that might be already dirty" (and thus, being the cause for the test to fail, and not because there was a bug in the code) in our test. 
However, having duplicated code is not desirable. The problem with duplicated test code
is the same as in production code: if there is a change to be made, the change has to be made
in all the points where the duplicated code exists.

In this example, in order to reduce some duplication, 
we could try to isolate the line of code responsible for creating
the class under test.
To that aim, we can use the `@BeforeEach` feature that JUnit provides.
JUnit runs methods that are annotated with `@BeforeEach` before every test method.
We therefore can instantiate the `roman` object inside a method annotated with `BeforeEach`.

Although you might be asking yourself: "But it is just a single line of code... Does it really matter?", remember that as test code becomes more complicated, the more important test code quality becomes.

The new test code would look as follows:

```java
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.*;

class RomanNumeralTest {
  
  private RomanNumeral roman;
  
  @BeforeEach
  void setup() {
    roman = new RomanNumeral();
  }

  @Test
  void convertSingleDigit() {
    int result = roman.convert("C");
    assertEquals(100, result);
  }

  @Test
  void convertNumberWithDifferentDigits() {
    int result = roman.convert("CCXVI");
    assertEquals(216, result);
  }

  @Test
  void convertNumberWithSubtractiveNotation() {
    int result = roman.convert("XL");
    assertEquals(40, result);
  }
}
```

{% hint style='tip' %}
Note that moving the instantiation of the class to a `@BeforeEach` method would work if all tests make use of the same constructor. Classes that offer more than a single constructor might need a different approach to avoid duplication. Can you think of any? We discuss test code quality in a more systematic way in a future chapter.
{% endhint %}

You can also see a video of us refactoring the `MinMax` test cases. Although the test suite was still small, it had many opportunities for better test code.

{% set video_id = "q5mq_Bkc8-s" %}
{% include "/includes/youtube.md" %}

## Tests and refactoring

A more experienced Java developer might be looking at our implementation of the
Roman Numeral problem and thinking that there are more elegant ways of implementing it.
That is indeed true. _Software refactoring_ is a constant activity in software
development. 

However, how can one refactor the code and still make sure it presents the same behaviour?
Without automated tests, that might be a costly activity. Developers would have to perform
manual tests after every single refactoring operation. Software refactoring activities benefit
from extensive automated test suites, as developers can refactor their code and, in a matter
of seconds or minutes, get a clear feedback from the tests.

See this new version of the `RomanNumeral` class, where we deeply refactored the code:

* We gave a better name to the method: we call it `asArabic()` now.
* We made a method for single char conversion using method overloading with `asArabic()`
* We inlined the declaration of the Map, and used the `Map.of` utility method.
* We create an array of characters from the string
* We make a stream of indices of the character array
* We map each character to its subtractive value
* We extracted a private method that decides whether it is a subtractive operation `isSubtractive()`.
* We extracted `getSubtractiveValue()` to return a negative number if it's subtractive
* We made use of the `var` keyword, as introduced in Java 10.

```java
public class RomanNumeral {
  private final static Map<Character, Integer> CHAR_TO_DIGIT = 
          Map.of('I', 1, 'V', 5, 'X', 10, 'L', 50, 'C', 100, 'D', 500, 'M', 1000);

  public static int asArabic(String roman) {
    var chars = roman.toCharArray();
    return IntStream.range(0, chars.length)
            .map(i -> getSubtractiveValue(chars, i, asArabic(chars[i])))
            .sum();
  }

  public static int asArabic(char c) {
    return CHAR_TO_DIGIT.get(c);
  }

  private static int getSubtractiveValue(char[] chars, int i, int currentNumber) {
    return isSubtractive(chars, i, currentNumber) ? -currentNumber : currentNumber;
  }

  private static boolean isSubtractive(char[] chars, int i, int currentNumber) {
    return i + 1 < chars.length && currentNumber < asArabic(chars[i + 1]);
  }
}
```

The number of refactoring operations is not small. And experience shows us that a lot
of things can go wrong. Luckily, we now have an automated test suite that we can run and
get some feedback.

Let us also take the opportunity and improve our test code:

* Given that our goal was to isolate the single line of code that instantiated the class under test, instead of using the `@BeforeEach`, we now instantiate it directly in the class. JUnit creates a new instance of the test class before each test (again, as a way to help developers in avoiding test cases that fail due to previous test executions). This allows us to mark the field as `final`.
* We inlined the method call and the assertion. Now tests are written in a single line.
* We give test methods better names. It is common to rename test methods; the more we understand the problem, the more we can give good names to the test cases.
* We devised one more test case and added it to the test suite.

```java
public class RomanNumeralTest {
  /*
  JUnit creates a new instance of the class before each test,
  so test setup can be assigned as instance fields.
  This has the advantage that references can be made final
  */
  final private RomanNumeral roman = new RomanNumeral();

  @Test
  public void singleNumber() {
      Assertions.assertEquals(1, roman.asArabic("I"));
  }

  @Test
  public void numberWithManyDigits() {
      Assertions.assertEquals(8, roman.asArabic("VIII"));
  }

  @Test
  public void numberWithSubtractiveNotation() {
      Assertions.assertEquals(4, roman.asArabic("IV"));
  }

  @Test
  public void numberWithAndWithoutSubtractiveNotation() {
      Assertions.assertEquals(44, roman.asArabic("XLIV"));
  }
}

```

Lessons to be learned: 

* Get to know your testing framework. 
* Never stop refactoring your production code.
* Never stop refactoring your test code.

{% hint style='tip' %}
While building this book, we have noticed that different people had different suggestions on how to implement this roman numeral converter! See our [code-examples](https://github.com/sttp-book/code-examples/tree/master/src/main/java/tudelft/dbc/roman) for the different implementations we have received as suggestions. 

Interestingly, we can test them all together, as they should have the same behaviour. See our [RomanConverterTest](https://github.com/sttp-book/code-examples/blob/master/src/test/java/tudelft/dbc/roman/RomanConverterTest.java) as an example of how to reuse the same test suite to all the different implementations! Note that, for that to happen, we defined a common interface among all the implementations: the _RomanConverterTest_. This testing strategy is quite common in object-oriented systems, and we will discuss more about it in the design by contracts chapter.
{% endhint %}

## The structure of an automated test case

Automated tests are very similar in structure.
They almost always follow the **AAA** ("triple A") structure.
The acronym stands for **Arrange**, **Act**, and **Assert**.

* In the **Arrange** phase, the test defines all the input values that will
then be passed to the class/method under test.
In terms of code, it can vary from a single value to
complex business entities.

* The **Act** phase is where the test "acts" or, calls the behavior under test, passing
the input values that were set up in the Arrange phase.
This phase is usually done by means of one or many method calls.

* The result is then used in the **Assert** phase, where the test asserts that 
the system behaved as expected. In terms of code, it is where the `assert` instructions are.

Using one of the test methods above to illustrate the different parts of an automated test code 
(note that the Arrange/Act/Assert comments here are just to help you visualize the three parts, 
 we would not usually add them in real test code):

```java
@Test
void convertSingleDigit() {
  // Arrange: we define the input values
  String romanToBeConverted = "C";

  // Act: we invoke the method under test
  int result = roman.convert(romanToBeConverted);

  // Assert: we check whether the output matches the expected result
  assertEquals(100, result);
}
```


Understanding the structure of a test method enables us to explore best practices
(and possible test code smells) in each one of them. From now on, we will use the
terms _arrange_, _act_, and _assert_ to talk about the different parts of an
automated test case.

{% hint style='tip' %}
AAA is a good structure for most tests, and a good way to think about what you're trying to show about the code.
As you learn more techniques, you will find that there other valuable ways to think about the structure of a test.
We'll discuss these in a later chapter.
{% endhint %}


## Advantages of test automation

Having an automated test suite brings several advantages to software
development teams. Automated test suites:

* **Are less prone to obvious mistakes.** Developers who perform manual testing several times a day might make mistakes, e.g., by forgetting to execute a test case, by mistakenly marking a test as passed when the software actually exhibited faulty behaviour, etc.

* **Execute tests faster than developers.** The machine can run test cases way faster than developers can. Just imagine more complicated scenarios where the developers would have to type long sequences of inputs, verify the output at several different parts of the system. An automated test runs and gives feedback orders of magnitude faster than developers.

* **Brings confidence during refactoring.** As we just saw in the example, automated test suites enables developers to refactor their code more constantly. After all, developers know that they have a safety net; if something goes wrong, the test will fail.

Clearly, at first, one might argue that writing test code might feel like a loss in productivity. 
After all, developers now have to not only write production code, but also test code.
Developers now have to not only maintain production code, but also maintain test code.
 _This could not be further from the truth_.
 Once you master the tools and techniques, formalising test cases as JUnit methods
will actually save you time; imagine how many times you have executed the same 
manual test over and over. How much time have you lost by doing 
the same task repeatedly? 

Studies have shown that developers who write tests spend less time debugging their systems when compared to developers who do not (Janzen), that the
impact in productivity is not as significant as one would think (Maximilien and Williams), 
and that bugs
are fixed faster (Lui and Chen). Truth be told: 
these experiments compared teams using Test-Driven Development (TDD) against teams not
using TDD, and not the existence of a test suite per se. Still, the presence of test code
is the remarking characteristic that emerges from TDD. Nevertheless, as a society,
we might not need more evidence on the benefits of test automation. If we look around,
from small and big companies to big open source projects, they all rely on extensive
test suites to ensure quality. **Testing (and test automation) pays off.**

## Exercises

**Exercise 1.**
Implement the `RomanNumeral` class. Then, write as many tests as you
can for it, using JUnit.

For now, do not worry about how to derive test cases. Just follow
your intuition. 

**Exercise 2.**
Choose a problem from [CodingBat](https://codingbat.com/java/Logic-2). Solve it.
Then, write as many tests as you
can for it, using JUnit.

For now, do not worry about how to derive test cases. Just follow
your intuition. 


## References

* Pragmatic Unit Testing in Java 8 with Junit. Langr, Hunt, and Thomas. Pragmatic Programmers, 2015.

* JUnit's manual: https://junit.org/junit5/docs/current/user-guide/.

* JUnit's manual, Annotations: https://junit.org/junit5/docs/current/user-guide/#writing-tests-annotations.


* Janzen, D. S. (2005, October). Software architecture improvement through test-driven development. In Companion to the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications (pp. 240-241).

* Maximilien, E. M., & Williams, L. (2003, May). Assessing test-driven development at IBM. In 25th International Conference on Software Engineering, 2003. Proceedings. (pp. 564-569). IEEE.

* Lui, K. M., & Chan, K. C. (2004, June). Test driven development and software process improvement in china. In International Conference on Extreme Programming and Agile Processes in Software Engineering (pp. 219-222). Springer, Berlin, Heidelberg.
# Testing vs Writing Tests

**TL;DR: Testing is different from writing tests. Developers write tests as a a way to give them space to think and confidence for refactoring. Testing focuses on finding bugs. Both should be done.**

We are right now at a point where developers do know that they need to write automated tests. After all, ideas such as Kent Beck's [Extreme Programming](https://en.wikipedia.org/wiki/Extreme_programming) and [Test-Driven Development](https://sttp.site/chapters/pragmatic-testing/tdd.html), Michael Feather's on the [synergy between testing and design](https://medium.com/r/?url=https%3A%2F%2Fvimeo.com%2F15007792), Steve Freeman's and Nat Pryce's on [how to grow an Object-Oriented software guided by tests](https://medium.com/r/?url=https%3A%2F%2Fwww.amazon.de%2FGrowing-Object-Oriented-Software-Addison-Wesley-Signature%2Fdp%2F0321503627), DHH and Ruby on Rails on [making sure that a web framework comes with a testing framework](https://medium.com/r/?url=http%3A%2F%2Fguides.rubyonrails.org%2Ftesting.html), etc, really stuck.

These ideas stuck with me as well. I have been trying to write as many automated tests as I can for the software systems I work since 2007 (I can later tell the history of how traumatic my 2006 project was). In 2012, I thought I had enough experience with the topic, so I decided to [write a book about how I was practicing TDD](https://medium.com/r/?url=https%3A%2F%2Fwww.casadocodigo.com.br%2Fproducts%2Flivro-tdd) (unfortunately, only available in Brazilian Portuguese).
After many years of development and consultancy, I see developers using automated test code as a sort of _support net_. The tests enable them to clearly think about what they want to implement, and support them throughout the innumerous refactorings they apply.

In fact, research has shown quite a few times that doing TDD can improve your class design (see [Janzen](http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1039&context=csse_fac), [Janzen and Saiedian](http://digitalcommons.calpoly.edu/cgi/viewcontent.cgi?article=1030&context=csse_fac), [George and Williams](https://www.semanticscholar.org/paper/An-Initial-Investigation-of-Test-Driven-Development-George-Williams/66869075d20858a2e9af144b2749a055c6b03177), [Langr](http://eisc.univalle.edu.co/materias/TPS/archivos/articulosPruebas/test_first_design.pdf), [Dogsa and Batic](https://link.springer.com/article/10.1007/s11219-011-9130-2), [Munir](https://pdfs.semanticscholar.org/16d4/2a1eaefb1f404f6da91b12d6c0e710dacb9d.pdf), [Moyaaed and Petersen](https://pdfs.semanticscholar.org/16d4/2a1eaefb1f404f6da91b12d6c0e710dacb9d.pdf), [myself](https://journal-bcs.springeropen.com/articles/10.1186/s13173-015-0034-z), among others). Recently, [Fucci et al.](https://arxiv.org/pdf/1611.05994.pdf) even argued that the important part is to write tests (and it doesn't matter whether it's before or after). This is also the perception of practitioners, where I quote a recent blog post from Feathers: _"That's the magic, and it's why unit testing works also. When you write unit tests, TDD-style or after your development, you scrutinize, you think, and often you prevent problems without even encountering a test failure."_


---

Writing automated tests is therefore something to be recommended.

Pragmatically speaking, I feel that, when using test code as a way to strengthen confidence, good weather tests are often enough. And indeed, there is no need for a lot of knowledge or theories to test good weather, but knowing how to make testing possible (developers talk a lot about designing for testability) and to use the best tools available (and developers indeed master tools like JUnit, Mockito, and Selenium).

What I argue is that, after implementing the production code (with all the support from the good weather test code), the next step should then be about testing it properly. Software testing is about finding many bugs; it is about exploring how your system behaves not only in good weather, but also in exceptional and corner cases.

I recently asked my 1st year CS students on what kinds of mistakes they have done in their programs so far, which have taught them that the testing is important. I got a huge list:

* Programs that don't work when a loop doesn't iterate (the loop condition was never evaluated to true, and the program crashed).
* Programs that don't work because they missed some last iteration (the famous off-by-one error).
* Programs that don't work when inputs are invalid, such as null, or do not conform to what's expected (e.g., a string that represents the name of a file to open containing an extra space at the beginning).
* Programs that don't work because the value is either out of boundaries or in the boundary. For example, if you have an if that expects a number to be greater than 10, what happens if the number is smaller than 10? Or precisely 10?
* Dependencies that are not there, e.g., your program reads from a file, but the file may not be there.
* Among others.

Regardless of your experience, I am sure you also have faced some of these bugs before. _But how often do developers actually explore and write such test cases?_
I feel not that much. (If you are an empirical software engineering researcher, this is a good question to answer).

[Robert Binder in his 2000 book](https://www.amazon.de/Testing-Object-Oriented-Systems-Addison-wesley-Technology/dp/0201809389) (p.45) makes a distinction between "fault-directed" versus" "conformance directed" testing:

* Fault directed: Intent to reveal faults through failures.
* Conformance directed: Intent to demonstrate conformance to required capabilities (e.g. Meets a user story, focus on good weather behavior).

I argue that a strong developer should test from both perspectives. They are complementary.

---

It is clear that, when it comes to software testing, [researchers and practitioners talk about different topics](https://ieeexplore.ieee.org/abstract/document/8048625/). Very good pragmatic books, like [Pragmatic Unit Testing in Java 8 with JUnit](https://pragprog.com/book/utj2/pragmatic-unit-testing-in-java-8-with-junit), have a strong focus on how to do test automation. While they provide good tips on what to test (the chapters on [CORRECT and RIGHT-BICEP](https://media.pragprog.com/titles/utj2/bicep.pdf) in the PragProg book is definitely interesting), they usually don't go beyond it.

On the other hand, software testing books from academia, my favorite being [_Software Testing Analysis_](https://www.amazon.com/Software-Testing-Analysis-Principles-Techniques/dp/0471455938) from Young and Pezzè, have a strong focus on theories and techniques on how to design test cases that explore as much as one can from the program under test. However, with not so many timely examples on how to apply these ideas, which is definitely a requirement for practitioners.

As a developer, if you want to take your testing to the next level, you should definitely get familiar with both perspectives. Some examples (but definitely not a complete list):

* Understand _domain testing_. Exploring the domain of the problem and its boundaries is still the best way to design good test cases.
* Making use of [structural testing](http://laser.cs.umass.edu/courses/cs521-621/papers/ZhuHallMay.pdf) as it was meant to be (and not simply trying to achieve a certain level of code coverage).
* Detecting whether some test suite is able to detect possible bugs. The idea of [mutation testing has being applied at Google](https://research.google.com/pubs/archive/46584.pdf).
* Deriving [models from your production system logs and build models](https://pure.tudelft.nl/portal/en/publications/an-experience-report-on-applying-passive-learning-in-a-largescale-payment-company%28b463c54a-d69f-4db4-9fcc-cbeb6e2ddf09%29.html) that can be analysed by domain experts.
* Automatically generating tests (the famous example is [EvoSuite](https://pdfs.semanticscholar.org/216b/98bb3d9221d5f5d261864975612e4d0faaa6.pdf), which is able to generate tests for Java classes) or to automatically reproduce crashes. I myself worked on [automatically generating tests for SQL queries](https://pure.tudelft.nl/portal/en/publications/searchbased-test-data-generation-for-sql-queries%2890a6431f-f78f-4ac3-bf87-c052cd9cd5d4%29.html).
* Several tools for testing web applications, like automatically exploring the web app looking for crashes as well as [finding crashes in REST APIs](https://github.com/EMResearch/EvoMaster/blob/master/docs/publications/2017_qrs.pdf). More can be found in this [web testing literature review](https://www.sciencedirect.com/science/article/pii/S0164121214000223).
* Property-based testing is also getting to practitioners by means of tools like ScalaCheck and [JQWik](https://jqwik.net).

---

Writing tests is different from testing. They both provide developers with different outcomes. The former gives confidence and support throughout development. The latter makes sure the software will really work in all cases. While they are both important, it's about time for both communities to get aligned:

* Developers to get more familiar with more advanced software testing techniques (which researchers have been providing). They are definitely a good addition to their seatbelts and will help them to better test their softwares.
* Educators to teach both perspectives. My perception is that universities teach testing techniques in an abstract way (developers need to learn how to use the tools!), while practitioners only teach how to use the automated tools (developers need to learn how to properly test!).
* Researchers to better understand how practitioners have been evolving the software testing field from their perspective as well as to better share their results, as developers do not read papers. [Research lingo is for researchers](https://twitter.com/mauricioaniche/status/997498070395949057).

_(An initial version of this text was published in [Medium](https://medium.com/@mauricioaniche/testing-vs-writing-tests-d817bffea6bc))._


# The developer testing workflow

Modern software testing is about **testing and designing, designing and testing**. It is only when we combine both perspectives and their respective techniques that we achieve high-quality systems. 

See the workflow below:

![The developer testing workflow](img/dtw.svg)

Let us walk through the main flow (indicated by the solid dashes in the diagram):

* Any software development activity starts with **requirements**. These requirements are often in form of text (e.g., UML use cases, agile user stories).
	* Exploring whether an **abstract model** can be derived might facilitate the design and the testing of the functionality later on. Common models are _state machines_, _decision tables_, _activity diagrams_, etc.
* Given the requirement (and maybe some abstract model), the developer then performs some initial **domain analysis**. The developer explores the input and output domains, and judiciously partitions the domain into a set of _equivalent classes_.
* With a better understanding of the domain, the developer then dives into (**TDD**) development cycles, with the goal of building the feature itself. In these cycles, developers:
	* Use **good weather/conformance tests** to support the development of the feature.
	* Design their classes with **testability in mind**. Small units are favoured in comparison to larger units.
	* Think of the **contracts** (i.e., pre-conditions, post-conditions, and invariants) that classes/components will offer, and even automate some of them as property-based testings.
	* Build enough **test infrastructure** to support the fast development and execution of the test cases.
	* Continuously **refactor production and test code**.
	* Perform **state testing** or **interaction testing**, according to the problem in hands. _Mock objects_ and clear _contracts_ play an important role in case of interaction testing.
	* Go back to domain analysis whenever a deeper understanding of the requirements is necessary.
* With the new set of (apparently working) classes, the developer changes its focus to **rigorous testing**. The goal is clear: to find as many bugs and to devise strong test suites that would alarm them in case of regressions.
	* The set of new classes (each of them with a specific architectural role and responsibility within the software system) goes through **test level analysis**. In other words, developers decide whether to apply unit, integration, or system testing for each of the emerged classes.
	* Developers exercise the **boundaries** of the system (using the _equivalence partitioning_ devised during domain analysis) as well as the **structure of the produced source code** (which might as well contain implementation details that were not captured by the requirements). 
	* The **context** and the **test level analysis** are taken into consideration, e.g., a database-related class will be exercised by means of an integration testing, or a web system will have tests for both its client side, server side, and their integration.
* With the best set of test cases, developers then rely on **intelligent testing tools** to complement their test cases. **Mutation testing**, **test case generation**, and **static analysis** tools are among the recommended intelligent testing tools. Depending on the domain, (automated) **fuzz and security testing** might also be leveraged.

Note that, although the diagram makes the workflow to look linear, in practice, it is _extremely incremental_. Based on what they learn in their current step, developers often take a step back and re-think of the decisions taken in the previous step (indicated by the dashed arrows in the diagram):

* The domain analysis might indicate problems in the requirements. 
* During the design cycle, developers might observe partitions they did not see during domain analysis.
* Structural, boundary, and intelligent testing might raise questions that can only be answered through more domain analysis.

Clearly, not all steps (design or testing techniques) have to be done for each and every feature of the software system. However, the developer _should_ explicitly analyse the trade-offs: 

* Do the benefits of the technique outperform its costs? After all, some bugs are more expensive than others.
* What is the chance of the technique to reveal unforeseen bugs? As we saw, variation is important, but context is king.

In the remaining of this book, we will explore how to perform each of these techniques in a systematic and productive manner.
# Principles of software testing

In this chapter, we first define some terminology. Using the right
terms helps us understand each other better. We then discuss the differences
between *verification* and *validation*. Finally, we discuss some testing principles
that will guide us (or, more specifically, force us to perform trade-offs whenever
we choose a testing technique) throughout the book.

## Failure, Fault and Error

It is common to hear different terms to indicate that a software system
is not behaving as expected.
Just to name a few: _error_, _mistake_, _defect_, _bug_, _fault_, and _failure_.
To describe the events that led to a software crash more precisely, 
we need to agree on a certain vocabulary.
For now, this comes down to three terms: **failure**, **fault**, and **error**.

A **failure** is a component of the (software) system that is not behaving as expected.
Failures are often visible to the end user.
An example of a failure is a mobile app that stops working, or a 
news website that starts to show yesterday's news on its front page. 
The software system did something it was not supposed to do.

Failures are generally caused by _faults_.
**Faults** are also called _defects_ or _bugs_.
A fault is the flaw in the component of the system that caused the 
system to behave incorrectly. A fault is technical and, in our world, usually refers to 
source code, such as a comparison in an `if` statement that uses a "less than" operator (`<`) instead of a "greater than" operator (`>`). 
A broken connection is an example of a hardware fault.

> Note that the existence of a fault in the source code does not necessarily lead to a failure.
> If the code containing the fault is never executed, it will never cause a failure.
> Failures only occur when the system is being used, and someone notices it not behaving as expected.

Finally, we have an **error**, also called *mistake*.
An error is the human action that caused the system to run not as expected.
For example, a developer didn't cover an obscure condition because they misunderstood
the requirement. Plugging a cable into the wrong socket is an example of a hardware mistake. 

In other words: a *mistake* by a developer can lead to a *fault* in the source code that will
eventually result in a *failure*.

In the _Min-Max_ code example of the previous chapter: the *failure* was the program returning
a large number, the *fault* was a bad `if/else if` condition, and the *mistake* was me not dealing 
properly with that case.

{% set video_id = "zAty8Rpg92I" %}
{% include "/includes/youtube.md" %}



## Verification and Validation

**Verification** and **validation** are both about 
assessing the quality of a system. However, they do have a subtle difference,
which can be quickly described by a single question:

* **Validation: Are we building the right software?**
Validation concerns the features that our software system offers, and the customer (i.e., for whom the system is made): 
  * is the system under development what the users really want and/or need?
  * is the system actually useful? 

  Validation techniques, thus, focus on understanding
whether the software system is delivering the business value it should deliver. In this book, we only briefly cover validation techniques (see chapters on Continuous Experimentation and on Behaviour-Driven Development).


* **Verification: Are we building the system right?** 
Verification, on the other hand, is about the system behaving as it 
is supposed to, according to the specification. 

  In simple words, this mostly means that the system behaves without any bugs.
  Note that this does not guarantee that the system is useful: a system might work 
  beautifully, bug-free, but  not deliver the features that customers really need.

Note how both _validation_ and _verification_ are fundamental for ensuring
the delivery of high-quality software systems.

{% set video_id = "LZ3Fb2Jq7yw" %}
{% include "/includes/youtube.md" %}


## Principles of software testing (or why software testing is so hard)

A simplistic view on software testing is that,
if we want our systems to be well-tested, all we need to do is to keep adding more tests until it is enough. We wish it were that simple.

Indeed, a very important part of any software testing process is 
to know _when to stop testing_.
After all, resources (e.g., money, developers, infrastructure) are limited. Therefore,
the goal should always be to maximise the number of bugs found while minimising the 
amount of resources we had to spend on finding those bugs.
Creating too few tests might leave us with a software system that does not behave as intended (i.e., _full of bugs_).
On the other hand, creating tests after tests, without proper consideration, might lead to ineffective tests (besides costing too much time and money).


Given resource constraints, we highlight an important principle in 
software testing: **exhaustive testing is impossible**. It might be impossible
even if we had unlimited resources. Imagine a software system that has "just" 300 different
flags (or configuration settings). Those flags can be set to either true or false (Booleans) and
they can be set independently from the others. The software system behaves differently
according to the configured combination of flags. This implies that we need to test all the possible combinations. A simple calculation shows us that 2 possible values for each of the 300 different flags gives $$2^{300}$$ combinations that need to be tested. As a matter
of comparison, this number is higher than the estimated number of atoms in the universe.
In other words, this software system has more possible combinations to be tested than the universe has atoms.

Given that exhaustive testing is impossible,
software testers have to then prioritise the tests they will perform.

When prioritising the test cases, we note that **bugs are not uniformly distributed**.
Empirically, we observe that some components in some software systems present more
bugs than other components.

Another crucial consequence of the fact that exhaustive testing is impossible is that, 
as Dijkstra used to say, **program testing can be used to show the presence of bugs, but never to show their absence**.
In other words, while we might find more bugs by simply testing more, our test suites,
however large they might be,
will never ensure that the software system is 100% bug-free. They will only ensure
that the cases we test for behave as expected.

To test our software, we need a lot of variation in our tests.
For example, we want variety in the inputs when testing a method, 
like we saw in the examples above.
To test the software well, however, we also need variation in 
the testing strategies that we apply.

Indeed, an interesting empirical finding is that if testers apply the same testing techniques over and over, they will at some point lose their efficacy. 
This is described by what is known as the **pesticide paradox** (which nicely refers to "bugs" as an equivalent term for software faults):
_"Every method you use to prevent or find bugs leaves a residue of 
subtler bugs against which those methods are ineffectual."_
In practice, this means that no single testing strategy 
can guarantee that the software under test is bug-free.
A concrete example might be a team that solely relies on unit testing techniques.
At some point, maybe all the bugs that can be captured at unit test level will be found
by the team; however, the team might miss bugs that only occur at integration level.
From the pesticide paradox, we thus conclude that testers have to use 
different testing strategies to minimise the number of bugs left in the software.
When studying the various testing strategies that we present in this book, 
keep in mind that combining them all might be a wise decision.

The context also plays an important role in how one devises test cases.
For example, devising test cases for a mobile app is very different 
from devising test cases for a web application, or for software used in a rocket.
In other words: **testing is context-dependent**.

Again, while this book mostly focuses on verification techniques, 
let us not forget that having a low number of bugs is not enough for good software.
As we have said before, a program that works flawlessly, but is of no use for its users, 
is still not a good program.
That is a common fallacy (also known as the **absence-of-errors fallacy**) that software testers face when they decide to focus solely
on verification and not so much on validation.

{% set video_id = "dkbvb_wTN-M" %}
{% include "/includes/youtube.md" %}


## Exercises

**Exercise 1.**
Having certain terminology helps testers to explain the problems they have with a program or in their software.

Below is a small conversation.
Fill in the blanks with one of the following terms: failure, fault, or error.

* **Mark**: Hey, Jane, I just observed a (1) _ _ _ _ _ _ in our software: if the user has multiple surnames, our software doesn't allow them to sign in. 
* **Jane**: Oh, that's awful. Let me debug the code so that I can find the (2) _ _ _ _ _ _.
* **Jane** *(a few minutes later):* Mark, I found it! It was my (3) _ _ _ _ _ _. I programmed that part, but never thought of this case.
* **Mark**: No worries, Jane! Thanks for fixing it!


**Exercise 2.**
Kelly, a very experienced software tester, visits *Books!*, a social network focused on matching people based on books they read.
Users do not report bugs so often, as *Books!* developers have strong testing practices in place.
However, users do say that the software is not really delivering what it promises.

What testing principle applies to this problem?

**Exercise 3.**
Suzanne, a junior software tester, has just joined a very large online payment company in the Netherlands. As a first task, Suzanne analysed their past two years of bug reports.
Suzanne observes that more than 50% of bugs have been happening in the *International payments* module. 

Suzanne then promises her manager that she will design test cases that will completely cover the *International payments* module, and thus, find 
all the bugs.

Which of the following testing principles might explain why this is **not** possible?

1. Pesticide paradox. 
2. Exhaustive testing.
3. Test early.
4. Defect clustering.

**Exercise 4.**
John strongly believes in unit testing. In fact, this is the only type of testing he actually
does at any project he's in. All the testing principles below but one might help convince John that he should also focus on different types of testing. 

Which of the following **is the least related** when we want to convince John to move away from his 'only unit testing' approach?

1. Pesticide paradox. 
2. Tests are context-dependent.
3. Absence-of-errors fallacy.
4. Test early.



## References

* Graham, D., Van Veenendaal, E., & Evans, I. (2008). Foundations of software testing: ISTQB certification. Cengage Learning EMEA. Chapters 1, 2, 3.
# Getting started with software testing

In this section, we introduce the reader to the idea of software testing and its importance. More specifically, we discuss the following topics:

* **Why software testing?**: By means of a simple example, we will convince the reader that proper testing is a fundamental activity that needs to be done to achieve high-quality software systems. We will also convince the reader that, while the process of deriving test cases is a manual activity, its execution should be automated.

* **Principles of software testing**: We explain to the reader some of the "truths" about testing software. Among others, we explain why exhaustive testing is infeasible, and why we need to vary the testing techniques we apply in order to keep finding bugs.

* **Software testing automation**: We introduce to the reader the JUnit tool. JUnit is the de-facto industry standard for writing automated tests in Java. We show that, once tests are automated, software development teams enjoy benefits, such as confident refactoring. This is an introductory chapter; readers that are familiar with JUnit can skip it.

* **Testing vs writing tests** and **developer testing workflow**: Modern software development is all about quality. To achieve it, developers should perform both testing and design techniques. In this chapter, we explain how we envision testing as part of the developer's work cycle.
# Why software testing?


Why should we actually care about software testing?
**Because bugs are everywhere!** 

As a person who is probably highly dependent on software technology, you should have encountered a few software bugs in your life. Some of them probably did not affect you that much. For example, maybe your Alarm app on your mobile phone crashed, and you did not wake up in time for a meeting. However, some other bugs might have (negatively) affected your life. Societies all over the world have faced critical issues due to software bugs, from medical devices that do not work properly and harm patients, to electric power plants that completely shut down.

And, while these software systems we gave as examples might seem far out from most developers' daily jobs, it is impressively easy to make mistakes even in less critical/complex software systems.

To illustrate how hard it is to spot bugs, let's start with a requirement:

> **Requirement: Min-max**
>
> Implement a program that, given a list of numbers (integers), returns 
> the smallest and the largest numbers in this list.

This looks like a very simple program to implement; maybe even an exercise for an Introduction to Programming course. 

A first implementation in Java could be as follows:

```java
public class NumFinder {
  private int smallest = Integer.MAX_VALUE;
  private int largest = Integer.MIN_VALUE;

  public void find(int[] nums) {
    for(int n : nums) {
      if(n < smallest) smallest = n;
      else if(n > largest) largest = n;
    }
  }

  // getters for smallest and largest
}
```


The idea behind the code is as follows: we go through all the elements of the `nums` list and store the smallest and the largest numbers in two different variables.
If `n` is smaller than the smallest number we have seen so far, we replace the smallest number by `n`. The same idea applies to the largest number: 
if `n` is bigger than the largest number, we just replace it by `n`. 

A common technique developers use (which we will try as much as possible to convince you  _not to do_) is that they implement the program based on the requirements, and then perform "small checks" to make sure the program works as expected. (Note that these "small checks" are what we will fight against; developers should perform rigorous and systematic testing to make sure their program works!)

For the sake of the argument, let us do a "small check" on the program we just wrote. A simple way of doing it would be to come up with a "main" method that exercises the program a few times. 
Suppose that the developer then tried their implementation with 4, 25, 7, and 9 as inputs. 


```java
public class NumFinderMain {

  public static void main (String[] args) {
    NumFinder nf = new NumFinder();
    nf.find(new int[] {4, 25, 7, 9});

    System.out.println(nf.getLargest());
    System.out.println(nf.getSmallest());
  }
}
```

The output of this program is: `25, 4`. This means the implementation works as expected. In a larger context, this would mean that the developer can ship this new implementation to the final user, and let them use this new feature. **Can we really...?**

No, we can not. The current implementation does not work for all the possible cases. **There's a bug in there!** (Can you find the bug? Look at the implementation above and try to find it!)

The program does not work properly for the following input: an array with values 4, 3, 2, and 1. For this input, the program returns the following output: `-2147483648, 1`.
In a more generalised way, the implementation does not handle "numbers in decreasing order" well enough. 

We have just found a bug. This is maybe the right time for a reflection: if bugs can occur
even in simple programs like the ones above, imagine what happens in large complex
software systems, on which our society relies upon.

Before anything else, let us just fix the bug.
Back to the source code, we see that the bug is 
actually caused by the `else if` instruction.  
The `else if` should actually just be an `if`. 
Take a moment to understand why.

```java
public class NumFinder {
  private int smallest = Integer.MAX_VALUE;
  private int largest = Integer.MIN_VALUE;

  public void find(int[] nums) {
    for(int n : nums) {
      if(n < smallest) smallest = n;

      // BUG was here!!
      if(n > largest) largest = n;
    }
  }

  // getters for smallest and largest
}
```

Again, this is indeed a trivial bug. Once you have found it, it might indeed
look like an unfortunate mistake. But this kind of mistakes 
can and do happen all the time. Why? The answer is simple: developers deal 
with highly complex software systems. Software systems that are composed of millions (if not billions) of lines of code. Software that generates tons of data per second. Software that communicates with hundreds (if not thousands) of external systems in an asynchronous and distributed manner. Software that has millions of user requests per hour. 
It is simply impossible to predict, during development time, everything that can happen. 

**This is why we need to test software. Because the world is complex, bugs do happen.**
And they can really have a huge impact in our lives.

What is the solution? To rigorously and systematically test the software systems we develop.

{% set video_id = "xtLgp8LXWp8" %}
{% include "/includes/youtube.md" %}

## Exercises

**Exercise 1.**
Google for "famous software bugs". Learn what caused them as well as the impact they
had.

## References

* Wikipedia. List of software bugs. https://en.wikipedia.org/wiki/List_of_software_bugs
# Non-functional testing
# Security Testing

In May of 2018, a [Code Injection Vulnerability was discovered in the Desktop Signal app](https://thehackernews.com/2018/05/signal-desktop-hacking.html). An attacker could execute code on a victim's machine by sending a specially crafted message. The victim's app would hand over the `/etc/passwd` file, and even send all the chats in plain text, _without any human intervention_! This was ironic since Signal is known for its end-to-end encryption feature.

*Why did this vulnerability exist, and how could we have avoided it? In this chapter, we answer these questions and introduce the concept of security testing.*

After reading this chapter, you should be able to:
- Explain the key difference between traditional software testing and security testing,
- Understand the cause of security vulnerabilities in Java applications,
- Implement the Secure Software Development Life Cycle,
- Explain the various facets of Security Testing,
- Evaluate the key differences between SAST and DAST.

## Software vs. security testing

We start this chapter with what you already know: *software testing*. The key difference between *software testing* and *security testing* is as follows:

> The goal of software testing is to check the correctness of the implemented functionality, while the goal of security testing is to find bugs (i.e. vulnerabilities) that can potentially allow an *intruder* to make the software behave insecurely.

Security testing is all about finding those edge cases in which a software *can be made to* malfunction. What makes a security vulnerability different from a typical software bug is the assumption that *an intruder may exploit it to cause harm*. Often, a software bug is exploited to be used as a security vulnerability, e.g. entering a specially crafted input that triggers a buffer overflow may be used to extract sensitive information from a system's memory. This was done in the [Heartbleed vulnerability](https://heartbleed.com/) that made $$2/3^{rd}$$ of all web servers in the world leak passwords. However, security vulnerabilities are not necessarily software bugs, and are also not always functional, e.g. authentication cookies never being invalidated allows a [Cross Site Request Forgery](https://owasp.org/www-community/attacks/csrf) attack, in which an attacker exploits a valid cookie to forge a victim's action.  

 Similar to traditional testing, thoroughly testing software *does not* guarantee the absence of security vulnerabilities. In fact, new *variants of exploits* can pop up at any time and can hit even a time-tested software. This is why security testing is not a one-off event, but has to be incorporated in the whole Software Development Life Cycle.

{% hint style='tip' %}
We discuss the **Secure Software Development Life Cycle** later in this chapter.
{% endhint %}

Security testers are always at an arms-race with the attackers. Their aim is to find and fix the vulnerabilities before the adversary gets the chance to exploit them.
You can think of the attack surface as the surface of a rubber balloon, as shown in the figure: there are endless points on this surface that, when pricked by a needle (exploit analogy) will pop the balloon. The goal of security testing is to limit the exposed attack surface and to increase the efforts required by the attackers to exploit it.


![Representation of attack surface and exploits](img/security-testing/attack-surface.png)


## Understanding Java vulnerabilities

In this chapter, we investigate the threat landscape of Java applications because of its popularity: 3 billion devices run Java globally [according to Oracle](https://www.oracle.com/java/). Also, Java is often considered to be a more mature language: Java handles memory management and garbage collection itself, unlike C that requires developers to handle these tasks manually. However, the added abstraction layers might make Java code slower than native C code; this is why some Java components are built upon native code for optimization purposes.

> The Java Virtual Machine, the sandbox that enables Java programs to execute platform-independently, is itself written in C.

In order to understand the threat landscape for Java applications, we must analyse what kind of security vulnerabilities have been discovered in them over the years. There exist online repositories that consolidate such vulnerability information. The [NIST National Vulnerability Database](https://www.cvedetails.com/) is one such example.

The [National Vulnerability Database](https://www.cvedetails.com/) is the largest repository of security vulnerabilities that are discovered in open source software. Each vulnerability is assigned a unique ***CVE (Common Vulnerabilities and Exposures)*** identifier, a ***CWE (Common Weakness Enumeration)*** that determines the _type_ of vulnerability, and a ***CVSS (Common Vulnerability Scoring System)*** score that determines the _severity_ of the vulnerability. Additionally, you can also view the products and their versions that are affected by the vulnerability.


### JRE vulnerabilities

![Vulnerabilities reported in JRE](img/security-testing/jre-vuln.png)


The plots show the number of vulnerabilities (left) and type of vulnerabilities (right) in the Java Runtime Environment (JRE) from 2007 to 2019. The spike in 2013 and 2014 is due to the exploitation of the *Type Confusion Vulnerability* (explained later), that allows a user to bypass the Java Security Manager and perform highly privileged actions.

### Android vulnerabilities


![Vulnerabilities reported in android](img/security-testing/android-vuln.png)


The second set of plots show vulnerabilities discovered between 2009 and 2019 in Android OS, which is mostly written in Java.

What is interesting to see in these plots is that the top 3 vulnerability types are related to *bypassing controls*, *executing code in unauthorized places*, and causing *denial of service*. Hence, we see that although memory corruption is not a major threat for Java applications, the effects caused by classical buffer overflows in C applications can still be achieved in Java by other means.

## Vulnerability use cases

Let's take the following commonly exploited vulnerabilities in Java applications, and analyse how they work:
1. Code injection vulnerability
  * Update attack
2. Type confusion vulnerability
  * Bypassing Java Security Manager
3. Buffer overflow vulnerability
4. Arbitrary Code Execution (ACE)
5. Remote Code execution (RCE)

### Code injection vulnerability

The code snippet below has a *Code Injection* vulnerability.  

``` java

Socket socket = null;
BufferedReader readerBuffered = null;
InputStreamReader readerInputStream = null;

/*Read data using an outbound tcp connection */
socket = new Socket("host.example.org", 39544);

/* Read input from socket */
readerInputStream = new InputStreamReader(socket.getInputStream(), "UTF-8");
readerBuffered = new BufferedReader(readerInputStream);

/* Read data using an outbound tcp connection */
String data = readerBuffered.readLine();

Class<?> tempClass = Class.forName(data);
Object tempClassObject = tempClass.newInstance();

IO.writeLine(tempClassObject.toString());

// Use tempClass in some way

```

The `Class.forName(data)` is the root cause of the vulnerability. If you look closely, the object's value is loaded dynamically from `host.example.org:39544`. If the host is controlled by an attacker, they can introduce new functions or overload existing ones with their malicious code in the class that is returned. This new code becomes part of the application's logic at run-time. A famous version of this attack is an **Update attack** in Android applications, where a plugin seems benign, but it downloads malicious code at run-time.


Static analysis tools often fail to detect this attack, since the malicious code is not part of the application logic at compile time.
>Due to the variations that Code Injection can present itself in, it is the top entry in the [OWASP Top 10 list of vulnerabilities](https://owasp.org/www-project-top-ten/). To limit its effect, developers can disallow 'untrusted' plugins, and can limit the privileges that a certain plugin has, e.g. by disallowing plugins to access sensitive folders.


### Type confusion vulnerability

This vulnerability was present in the implementation of the `tryfinally()` method in the *Reflection API* of the [Hibernate ORM library](https://access.redhat.com/security/cve/cve-2014-3558). Due to insufficient type checks in this method, an attacker could cast objects into arbitrary types with varying privileges.

The Type confusion vulnerability is explained in [this blog by Vincent Lee](https://www.thezdi.com/blog/2018/4/25/when-java-throws-you-a-lemon-make-limenade-sandbox-escape-by-type-confusion), from where we take the example below.

``` java
class Cast1 extends Throwable {
  Object lemon;
}

class Cast2 extends Throwable {
  Lime lime;
}

public static void throwEx() throws Throwable {
  throw new Cast1();
}

public static void handleEx(Cast2 e) {
  e.lime.makeLimenade();
}
```

Suppose that an attacker wants to execute the `makeLimenade()` method of the `Lime` object, but only has access to a `lemon` of type `Object`. The attacker exploits the fact that `throwEx()` throws a `Cast1` (lemon) object, while `handleEx()` accepts a `Cast2` (lime) object.

For the sake of brevity, consider that the output of `throwEx()` is an input to `handleEx()`. In the old and vulnerable version of Java, these type mismatches did not raise any alerts, so an attacker could send a `lemon` object that was then cast into a `Lime` type object, hence allowing them to call the `makeLimenade()` function from *(what was originally)* a `lemon`.

In a real setting, an attacker can use this _type confusion_ vulnerability to escalate their privileges by **bypassing the Java Security Manager (JSM)**. The attacker's goal is to access `System.security` object and set it to `null`, which will disable the JSM. However, the `security` field is private and cannot be accessed by an object that the attacker has (let's call it `Obj`). So, they will exploit the _type confusion_ vulnerability to cast `Obj` into something that does have higher privileges and access to the `System.security` field. Once the JSM is bypassed, the attacker can execute whatever code they want to.


### Arbitrary Code Execution (ACE)

A common misconception is that Java, unlike C, is not vulnerable to **Buffer overflows**. In fact, any component implemented in native code is as much vulnerable to exploits as the original C code would be. An interesting example here is of graphics libraries that often use native code for fast rendering.

An earlier version of a GIF library in the Sun JVM contained a memory corruption vulnerability: A valid GIF component with the block's width set to 0 caused a _buffer overflow_ when the parser copied data to the under-allocated memory chunk. This overflow caused multiple pointers to be corrupted, and resulted in **Arbitrary Code Execution** (see [CVE-2007-0243](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2007-0243) for more details).

A similar effect was caused by an [XML deserialization bug in the XStream library](https://access.redhat.com/security/cve/cve-2013-7285): while deserializing XML into a Java Object, a malicious XML input caused the memory pointer to start executing code from _arbitrary memory locations_ (which could potentially be controlled by an attacker).

When an ACE is triggered remotely, it is called a **Remote Code Execution** (RCE) vulnerability. The underlying principle is the same: it is also caused by *Improper handling of 'special code elements'*. We have seen it in the [Spring Data Commons](https://pivotal.io/security/cve-2018-1273) library, a part of the Spring framework that provides cloud resources for database connections.

An Oracle report in 2018 stated that [most of the Java vulnerabilities can be remotely exploited](https://www.waratek.com/alert-oracle-guidance-cpu-april-2018/). With 3 billion devices running Java, this creates a large attack surface.


## The Secure Software Development Life Cycle (Secure-SDLC)

Security testing is a type of non-functional testing, but if it fails to fix security vulnerabilities, (i) there is a high impact on the functionality of the application, e.g. a denial of service attack that makes the entire application unreachable, and (ii) it also causes reputation and/or monetary loss, e.g. loss of customers.

There is an interesting debate about *who gets the responsibility for security testing*.
The pragmatic approach is to **include security testing in each phase of the SDLC**.

The figure below shows the Secure-SDLC variant of the traditional SDLC taken from [this article by Rohit Singh](https://www.dignitasdigital.com/blog/easy-way-to-understand-sdlc/).


![The secure SDLC](img/security-testing/ssdlc.png)


At the *planning phase*, risk assessment should be done and potential abuse cases should be designed that the application will be protected against. In the *analysis phase*, the threat landscape should be explored, and attacker modelling should be done.

>For example, an attacker model is that the vendor that supplies the plugins has been infected, so all plugins received from the vendor might be malicious.

The *design* and *implementation* plans of the application should include insights from the attacker model and abuse cases.

>For example, the choice of certain libraries, and the permissions assigned to certain modules should be guided by the threat landscape under consideration.

Security testing should be a part of the *testing and integration* phases. Code reviews should also be done from the perspective of the attacker (using abuse cases). Finally, during the *maintenance phase*, in addition to bug fixes, developers should keep an eye on the CVE database and update *(if possible)* the vulnerable components in the application.

*Just like the traditional SDLC is not a one-time process, the Secure-SDLC is also a continuous process*. Therefore, security testing should also be integrated into the *Continuous Integration* framework as well.

Currently, most companies solely do *penetration testing* which tests the entire application at the very end of the SDLC. The problem with penetration testing is that it tests the application as a whole, and does not stress-test each individual component. When security is not an integral part of the design phase, the vulnerabilities discovered in the penetration testing phase are patched in an ad-hoc manner that increase the risk of them falling apart after deployment.



## Facets of security testing

As such, the term *security testing* is very broad and it covers a number of overlapping concepts. We classify them as follows:

|         |    White-box    |    Black-box    |
|------------|----------------------------------------------------|-------------------------------------------------------------------------|
|    **Static Application Security Testing**    | Code checking, Pattern matching, ASTs, CFGs,  DFDs  |  Reverse engineering|
|    **Dynamic Application Security Testing**    | Tainting, Dynamic validation, Symbolic  execution | Penetration testing,  Reverse engineering, Behavioural analysis,  Fuzzing |

You should already be familiar with _white/black-box_ testing, static testing and some of the mentioned techniques.


{% hint style='tip' %} In this chapter, we specifically focus on how to use these techniques to find security vulnerabilities. {% endhint %}


In the context of automated security testing, _static_ and _dynamic_ analysis are called _Static Application Security Testing (SAST)_ and _Dynamic Application Security Testing (DAST)_, respectively.
Before we dive into further explanation of SAST and DAST techniques, let's look at the assessment criteria for evaluating the quality of security testing techniques.

### Quality assessment criteria
The quality of testing tools is evaluated in a number of ways. You have already learnt about _code coverage_ in a previous chapter. Here, we discuss four new metrics that are often used in the context of security testing:

Designing an ideal testing tool requires striking a balance between two measures: (a) Soundness, and (b) Completeness. 
**Soundness** dictates that there should be no False Negatives (FN) — no vulnerability should be skipped. This implies that no alarm is raised *IF* there is no existing vulnerability in the *System Under Test (SUT)*. **Completeness** dictates that there should be no False Positives (FP) — no false alarm should be raised. This implies that an alarm is raised *IF* a valid vulnerability is found.

>Here, a 'positive' instance indicates a _bug_ and a 'negative' instance indicates _benign code_. So, True Positives (TP) are _actual bugs_, and True Negatives (TN) are _actual benign code snippets_. Similarly, False Positives (FP) are _false bugs_ (or _false alarms_), and False Negatives (FN) are _bugs that weren't found_ (or _missed bugs_).

A perfect testing tool is both sound and complete. However, this is an undecidable problem — given finite time, the tool will always be wrong for some input. In reality, tools often compromise of FPs or FNs.

Low FNs are ideal for security critical applications where a missed vulnerability can cause significant loss, e.g. banking apps. Low FPs are ideal for applications that don't have a lot of manpower to evaluate the correctness of each result.

Additionally, an ideal testing tool is (c) **interpretable**: an analyst can trace the results to a solid cause, and are (d) **scalable**: the tool can be used for large applications without compromising heavily on performance.



## Static Application Security Testing (SAST)


SAST techniques aim to find security bugs without running the application. They can find bugs that can be observed in the source code and for which _signatures_ can be made, e.g. SQL Injection and basic Cross-Site Scripting. _SpotBugs_, _FindSecBugs_, and _Coverity_ are static analysis tools specially meant to test security problems in applications.

SAST is not only limited to code checking — it includes any approach that does not require running the SUT. For example, **Risk-based testing**  is a business-level process where we model the worst-case scenarios (or abuse cases) using threat modelling. An application is tested against the generated abuse cases to check its resilience against them. Risk-based testing can be done both statically (if the abuse-case targets problems found in source code) and dynamically (for run-time threats).

Below, we discuss SAST techniques and provide examples of the security bugs that they can find.

1. Code checking for security
  * Pattern matching via _RegEx_
  * Syntax analysis via _Abstract Syntax Trees_
3. Structural testing for security
  * Control Flow Graphs (CFGs)
  * Data Flow Diagrams (DFDs)

### Code checking for security

#### Pattern matching via RegEx
Pattern matching can find simplistic security issues, such as:
1. *Misconfigurations*, like `port 22` being open for every user,
2. *Potentially bad imports*, like importing the whole `System.IO` namespace,
3. *Calls to dangerous functions*, like `strcpy` and `memcpy`.

#### Syntax analysis via AST

Abstract Syntax Trees can also find security misconfigurations in the codebase, and sometimes are more appropriate than using regular expressions. Code that violates security specifications can be detected by walking the AST. For example, for a rule specifying how the print function should be used, i.e. `printf(format_string, args_to_print)`, and the following code snippet, an error will be raised because of a missing parameter that can be detected by counting the child-nodes.

![AST rule enforcement](img/security-testing/ast-usecase2.png)


This is an example of the famous *[Format string attack](https://owasp.org/www-community/attacks/Format_string_attack)*, which exploits a vulnerability in the `printf()` function family: in the absence of a format string parameter like `%s`, an attacker can supply their own format string parameter in the input, which will be evaluated as a pointer resulting in either arbitrary code execution or a denial of service.


### Structural testing for security

#### Control Flow Graphs (CFGs)
Recall that **Control Flow Graphs** show how the control is transferred among different pieces of code in an application. A CFG is an *over-estimation* of what any potential execution might look like — it is the union of all possible combinations of execution paths.

For security testers, a CFG is an overall picture of an application's behaviour, in a graphical format. It can help testers pin-point strange control transfers, e.g.
* *an unintended transition going from a low- to a high- privileged code block*, or
* *certain unreachable pieces of code* that can result in application hanging and eventually, a denial of service.

Existing literature has used CFGs to detect the maliciousness of an application based on how its CFG looks like. For example, [this work by Bruschi _et al._](https://link.springer.com/chapter/10.1007/11790754_8) detects self-mutating malware by comparing its CFG with CFGs of known malwares, and [this work by Sun _et al._](https://link.springer.com/chapter/10.1007/978-3-642-55415-5_12) uses CFGs to measure code reuse as a means to detect malware variants.

#### Data Flow Diagram (DFD)

A DFD is built on top of a CFG and shows how data traverses through a program. Since Data Flow Analysis (DFA) is also a static approach, a DFD tracks all possible values a variable might have during any execution of the program. This can be used to detect _sanitization problems_, such as the deserialization vulnerability that caused an ACE, and some simplistic _code injection vulnerabilities_.

**How DFA works:** A user-controlled variable whose value we intend to track is called a ***Source***, and the other variables are called ***Sinks***. We say that the *Source variables are tainted* and *all Sinks are untainted*. For a Source to be connected to a Sink, it must first be untainted by proper input validation, for example.

In DFA, we prove that (a) _No tainted data is used_, and (b) _No untainted data is expected_. An alert is raised if either of the two conditions are violated. Consider the following scenario for a single Source and Sink. There exists a direct path between a Source and a Sink, which violates the first rule. This indicates that a malicious user input can cause an SQL injection attack. The solution to fix this violation is to include an input clean-up step between the Source and Sink variables.



![Source/sink example](img/security-testing/source-sink-example.png)

The code snippet below shows a real case that DFA can detect. The variable `data` is tainted, as it is received from a user. Without any input cleaning, it is directly used in `println()` method that expects untainted data, thus raising an alert.


``` Java
/* Uses bad source and bad sink */
public void bad(HttpServletRequest request, HttpServletResponse response)
  throws Throwable {

  String data;

  /* Potential flaw: Read data from a queryString using getParameter */
  data  = request.getParameter("name");

  if (data != null) {
    /* Potential flaw: Display of data in web pages after using
    * replaceAll() to remove script tags,
    * will still allow XSS with string like <scr<script>ipt>. */
    response.getWriter().println("<br>bad(): data = " +
    	data.replaceAll("(<script>)", ""));
  }
}

```



{% hint style='tip' %} A dynamic version of Data Flow Analysis is called Taint analysis, where the tainted variables' values are tracked in memory. We cover it in the DAST section of this chapter. {% endhint %}



##### Reaching Definitions Analysis
 One application of DFA is called the **Reaching Definitions**. It is a top-down approach that identifies all the possible values of a variable. For security purposes, it can detect the _Type Confusion vulnerability_ and _Use-after-free vulnerability_ (which uses a variable after its memory has been freed).

 Consider the following code snippet and its CFG given below:


``` java
int b = 0;
int c = 1;

for (int a = 0; a < 3; a++) {
  if (a > 1)
    b = 10;
  else
    c = b;
}
return b, c;
```

![Making a DFD](img/security-testing/dfd-code2.png)


The solid transitions show *control transfers*, while the dotted transitions show *data transfers*. Suppose that we want to perform reaching definitions analysis of the three variables: `a`, `b`, and `c`. First, we label each basic block, and draw a table that lists the variable values in each block.

![Performing reaching definitions analysis](img/security-testing/dfd-code3.png)


If a variable is not used in the block, or the value remains the same, nothing is listed. At the end, each column of the table is merged to list the full set of potential values for each variable.


| code blocks | **a** | **b** | **c** |
|:----:|:--------:|:----:|:---:|
| **b1** | - | 0 | 1 |
| **b2** | 0, **a**++ | - | - |
| **b3** | - | - | - |
| **b4** | - | 10 | - |
| **b5** | - | - | **b** |
| **b6** | - | - | - |

Remember, if the value of a variable is controlled by a user-controlled parameter, it cannot be resolved until run-time, so it is written as it is.
If a variable `X` copies its value to another variable `Y`, then the reaching definitions analysis dictates that the variable `Y` will receive all potential values of `X`, once they become known.
Also, whether a loop terminates is an undecidable problem (also called the *halting problem*), so finding the actual values that a looping variable takes on is not possible using static analysis.

The analysis results in the following values of the three variables. If you look closely, some values are impossible during actual run-time, but since we trace the data flow statically, we perform an over-estimation of the allowed values. This is why, static analysis, in particular DFA, is `Sound` but `Incomplete`.

``` java
a = {0, 1, 2, 3, ...}
b = {0, 10}                // 0 is impossible       
c = {1, b} -> {0, 1, 10}  // 1, 10 are impossible
```

## Dynamic Application Security Testing (DAST)

Application crashes and hangs leading to Denial of Service attacks are common security problems. They cannot be detected by static analysis since they are only triggered when the application is executed.
DAST techniques execute an application and observe its behaviour. Since DAST tools typically do not have access to the source code, they can only test for functional code paths, and the analysis is only as good as the behaviour triggering mechanism. This is why search-based algorithms have been proposed to maximize the code coverage, e.g. see [this work by Gross _et al._](https://dl.acm.org/doi/abs/10.1145/2338965.2336762) and [this work by Chen _et al._](https://ieeexplore.ieee.org/abstract/document/8418633).

DAST tools are typically difficult to set up, because they need to be hooked-up with the SUT, sometimes even requiring to modify the SUT's codebase, e.g. for instrumentation. They are also slower because of the added abstraction layer that monitors the application's behaviour. Nevertheless, they typically produce less false positives and more advanced results compared to SAST tools. Even when attackers obfuscate the codebase to the extent that it is not statically analysable anymore, dynamic testing can still monitor the behaviour and report strange activities. _BurpSuite_, _SonarQube_, and _OWASP's ZAP_ are some dynamic security testing tools.

In this section, we explain the following techniques for dynamic analysis:

1. Taint analysis
2. Dynamic validation
3. Penetration testing
4. Behavioural analysis
5. Reverse Engineering
6. Fuzzing

### Taint analysis

Taint analysis is the dynamic version of Data Flow Analysis. In taint analysis, we track the values of variables that we want to *taint*, by maintaining a so-called *taint table*. For each tainted variable, we analyse how the value propagates throughout the codebase and affects other statements and variables. To enable tainting, ***code instrumentation*** is done by adding hooks to variables that are of interest. _Pin_ is an instrumentation tool from Intel, which allows taint analysis of binaries.

An example here is of the tool [Panorama](https://dl.acm.org/doi/abs/10.1145/1315245.1315261) that detects malicious software like _keyloggers_ (that log keystrokes in order to steal credentials) and _spyware_ (that stealthily collects and sends user data to $$3^{rd}$$ parties) using dynamic taint analysis. Panorama works on the intuition that benign software does not interfere with OS-specific information transfer, while information-stealing malware attempts to access the sensitive information being transferred. Similarly, malicious plugins collect and share user information with $$3^{rd}$$ parties while benign plugins don't send information out. These behaviours can be detected using the source/sink principles of taint analysis.  

### Dynamic validation

Dynamic validation does a functional testing of the SUT based on the system specifications. It basically checks for any deviations from the pre-defined specifications. **Model Checking** is a similar idea in which specifications are cross-checked with a model that is learnt from the SUT. Model checking is a broad field in the Software Verification domain.

[This work by Chen _et al._](http://seclab.cs.ucdavis.edu/papers/Hao-Chen-papers/ndss04.pdf) codifies security vulnerabilities as _safety properties_ that can be analysed using model checking. For example, they analyse processes that may contain _race conditions_ that an attacker may exploit to gain control over a system. In this regard, consider the following code snippet. Suppose that the process first checks the owner of `foo.txt`, and then reads `foo.txt`. An attacker may be able to introduce a race condition in between the two statements and alter the symbolic link of `foo.txt` such that it starts referring to `/etc/passwd` file. Hence, what the user reads as `foo.txt` is actually the `/etc/passwd` file that the attacker now has access to.


```java
Files.getOwner("foo.txt");
Files.readAllLines("foo.txt");
```

To check the existence of such scenarios, they codify it in a property that _stops a program from passing the same filename to two system calls on the same path_. Once codified in a model checker, they run it on various applications and report on deviations from this property.   

### Penetration testing

Penetration (or Pen) testing is the most commonly used type of security testing in organizations. It is sometimes also referred to as ***Ethical hacking***. What makes pen testing different from others is that it is done from the perspective of an attacker — [pen-testers attempt to breach the security of the SUT just as an adversary might](https://www.ncsc.gov.uk/guidance/penetration-testing). Since it is done from the perspective of the attacker, it is generally black-box, but depending on the assumed knowledge of the attacker, it may also be white-box.

Penetration testing checks the SUT in an end-to-end fashion, which means that it is done once the application is fully implemented, so it can only be done at the end of the SDLC. *MetaSploit* is an example of a powerful penetration testing framework. Most pen testing tools contain a ***Vulnerability Scanner*** module that either *runs existing exploits*, or allow the tester to *create an exploit*. They also contain ***Password Crackers*** that either *brute-force* passwords (i.e. tries all possible combinations given some valid character set), or perform a *dictionary attack* (i.e. chooses inputs from pre-existing password lists).

### Behavioural analysis

Given a software that may contain modules from unknown sources, behavioural analysis aims to gain insights about the software by generating behavioural logs and analysing them. This can be particularly helpful for finding abnormal behaviours (security problems, in particular) when neither the source code, nor the binary are accessible. The logs can be compared with known-normal behaviour in order to debug the SUT.

An example here is of JPacman that currently has support for two point calculator modules (`Scorer 1` and `Scorer 2`) that calculate the score in different ways. The goal is to find what the malicious module (`Scorer 2`) does. We have implemented a ***Naïve Fuzzer*** that automatically runs various instances of JPacman to generate behavioural logs. At each iteration, it randomly picks a move (from the list of acceptable moves) until Pacman dies, and logs the values of different interesting variables. The fuzzing code is given below.

```java
  /**
   * Basic fuzzer implementation.
   *
   * @param repetitionInfo repeated test information
   * @throws IOException when the log write created.
   */
  @RepeatedTest(RUNS)
  void fuzzerTest(RepetitionInfo repetitionInfo) throws IOException {
      Game game = launcher.getGame();
      Direction chosen = Direction.EAST;

      String logFileName = "log_" + repetitionInfo.getCurrentRepetition() + ".txt";
      File logFile = new File(logDirectory, logFileName);

      try (BufferedWriter logWriter = new BufferedWriter(new OutputStreamWriter(
          new FileOutputStream(logFile, true), StandardCharsets.UTF_8))) {

          logWriter.write(LOG_HEADER);

          try {
              game.start();

              while (game.isInProgress()) {
                  chosen = getRandomDirection();

                  log(logWriter, chosen);
                  game.getLevel().move(game.getPlayers().get(0), chosen);
              }
          } catch (RuntimeException e) {
              // Runtime exceptions should not stop the execution of the fuzzer
          } finally {
              log(logWriter, chosen);
              game.stop();
          }
      }
  }

```

Below you see an example of a log file resulting from one run of the fuzzer.

![behavioural log screenshot](img/security-testing/behav-log-screenshot.png)


In the figure below, the plots on the right show how the value of `score` variable changes over time. It is apparent that something goes wrong with `Scorer 2`, since the score is typically programmed to increase monotonically.

![JPacman and scorers](img/security-testing/jpacman-screenshot.png)


Behavioural logs are a good data source for forensic analysis of the SUT.

### Reverse Engineering (RE)

Reverse Engineering is a related concept where the goal is to reveal the internal structure of an application. We can consider RE as a system-wide process that converts a black-box application into a white-box. RE is, strictly speaking, not a testing technique, but it is useful when (i) converting legacy software into a modern one, or (ii) understanding a competitor's product.

A use case for the behavioural logs from the previous technique is to use them for automated reverse engineering that learns a model of the SUT. This model can then be used for, e.g. *Dynamic validation*, and/or to *guide path exploration* for better code coverage.

For example, [TABOR](https://dl.acm.org/doi/abs/10.1145/3196494.3196546) learns a model of a water treatment plant in order to detect attacks. They learn an automata representing the _normal behaviour_ of the various sensors present in the plant. Anomalous incoming events that deviate from the normal models raise an alert.

### Fuzzing

Fuzzing has been used to uncover previously-unknown security bugs in several applications. _American Fuzzy Lop (AFL)_ is an efficient security fuzzer that has been used to find security vulnerabilities in command-line-oriented tools, like PuTTY, openSSH, and SQLite. [This video](https://www.youtube.com/watch?v=ibjkz7GTT3I) shows how to fuzz _ImageMagick_ using AFL to find security bugs.

Additionally, in 2015 a severe security vulnerability, by the name of [Stagefright](https://blog.zimperium.com/experts-found-a-unicorn-in-the-heart-of-android/), was discovered in Android smartphones that impacted 1 billion devices. It was present in the library for unpacking MMS messages, called `libstagefright`. A specially crafted MMS message could silently cause an overflow leading to remote code execution and privilege escalation. It was discovered by [fuzzing the `libstagefright` library using AFL](https://www.blackhat.com/docs/us-15/materials/us-15-Drake-Stagefright-Scary-Code-In-The-Heart-Of-Android.pdf) that ran \~3200 tests per second for about 3 weeks!

Finally, [Sage](https://dl.acm.org/doi/pdf/10.1145/2090147.2094081) is a white-box fuzzing tool that combines symbolic execution with fuzzing to find deeper security bugs. They report a use case of a critical security bug that black-box fuzzing was unable to find, while Sage discovered it in under 10 hours, despite having no prior knowledge of the file format.

## Performance evaluation of SAST and DAST

* Static analysis tools create a lot of FPs because they cannot see the run-time behaviour of the code. They also generate FNs if they don't have access to some code, e.g. code that is added at run-time.
* Dynamic analysis reduces both FPs and FNs — if an action is suspicious, an alarm will be raised. However, even in this case, we cannot ensure perfect testing.  
* Static analysis is generally more white-box than dynamic analysis, although there are interpretable dynamic testing methods, like symbolic execution.
* Static testing is more scalable in the sense that it is faster, while black-box dynamic testing is more generalizable.

## Chapter Summary

- Software testing checks correctness of the software, while security testing finds potential defects that may be exploited by an intruder.
- Even though Java handles memory management itself, Java applications still have a large attack surface.
- Security testing is much more than penetration testing, and must be integrated at each step of the SDLC.
- Threat modelling can derive effective test cases.
- Perfect (security) testing is impossible.
- SAST checks the code for problems without running it, while DAST runs the code and monitors its behaviour to find problems.
- SAST is fast, but generates many false positives. DAST is operationally expensive, but generates insightful and high-quality results.
- Pattern matching finds limited but easy to find security problems.
- ASTs make the code structure analysis easy. CFGs and DFDs are better at finding security vulnerabilities.
- Behavioural logs are very useful for forensic analysis of the SUT.
- Combining fuzzing with Symbolic execution leads to finding optimal test cases that can maximize code coverage and find maximum security problems.

## Exercises

**Exercise 1.** Why is security testing hard?
1. To find a vulnerability, you must fully understand assembly code.
2. Pointers are hard to follow since they can point to any memory location.
3. Hacking is difficult and requires many years of specialization.
4. There exist a lot of strange inputs, and only few trigger a vulnerability.

**Exercise 2.** Give an example of a software bug that also qualifies as a security vulnerability, and also explain how.

**Exercise 3.** Static analysis can detect some classes of injection vulnerabilities more reliably than others. Which vulnerability is static analysis most likely to _prevent_?
1. Cross-Site Scripting
2. Cross-Site Request Forgery
3. Update attack
4. Format string injection

**Exercise 4.** What is the underlying defect that enables arbitrary code execution?
1. Buffer overflows
2. Deserialising bugs
3. Type confusion
4. All of the above

**Exercise 5.** In the following table, several testing objectives and techniques are given. Associate each _Objective_ with the most appropriate _Testing technique_. Note that no repetitions are allowed.

| Objective | Testing Technique | (Answer option) Testing Technique |
|----------------------------------------|------------------------|-----------------------------------|
| 3. Detect pre-defined patterns in code | A. Fuzzing | __ |
| 2. Branch reachability analysis | B. Regular expressions | __ |
| 1. Testing like an attacker  | C. Symbolic execution | __ |
| 4. Generate complex test cases | D. Penetration testing | __ |

**Exercise 6.** How can you use Tainting to detect spyware?

**Exercise 7.** Perform Reaching Definitions Analysis on the following piece of code. Which values of the variables does the analysis produce?
``` java
        int x = 0;
        int y = 1;
        while(y < 5) {
    	    y++;
        	if(y > 7)
        		x = 12;
        	else
        		x = 21;
    	}
        return x;
```
1. $$x = \{0\}; y = \{0,1,2,3,...\}$$
2. $$x = \{0,21\}; y = \{1,2,3,...\}$$
3. $$x = \{0,12,21\}; y = \{1,2,3,...\}$$
4. $$x = \{21\}; y = \{1,2,3,4\}$$

## References
* Bruschi, Danilo, Lorenzo Martignoni, and Mattia Monga. "Detecting self-mutating malware using control-flow graph matching." In International conference on detection of intrusions and malware, and vulnerability assessment, pp. 129-143. Springer, Berlin, Heidelberg, 2006.
* Chen, Hao, Drew Dean, and David A. Wagner. "Model Checking One Million Lines of C Code." In NDSS, vol. 4, pp. 171-185. 2004.
* Chen, Peng, and Hao Chen. "Angora: Efficient fuzzing by principled search." In 2018 IEEE Symposium on Security and Privacy (SP), pp. 711-725. IEEE, 2018.
* Godefroid, Patrice, Michael Y. Levin, and David Molnar. "SAGE: whitebox fuzzing for security testing." Queue 10, no. 1 (2012): 20-27.
* Gross, Florian, Gordon Fraser, and Andreas Zeller. "Search-based system testing: high coverage, no false alarms." In Proceedings of the 2012 International Symposium on Software Testing and Analysis, pp. 67-77. 2012.
* Lin, Qin, Sridha Adepu, Sicco Verwer, and Aditya Mathur. "TABOR: A graphical model-based approach for anomaly detection in industrial control systems." In Proceedings of the 2018 on Asia Conference on Computer and Communications Security, pp. 525-536. 2018.
* Sun, Xin, Yibing Zhongyang, Zhi Xin, Bing Mao, and Li Xie. "Detecting code reuse in android applications using component-based control flow graph." In IFIP international information security conference, pp. 142-155. Springer, Berlin, Heidelberg, 2014.
* Yin, Heng, Dawn Song, Manuel Egele, Christopher Kruegel, and Engin Kirda. "Panorama: capturing system-wide information flow for malware detection and analysis." In Proceedings of the 14th ACM conference on Computer and communications security, pp. 116-127. 2007.
# Design for Testability
 
 
We just learned how the use of mocks and stubs can help developers in being highly productive and efficient in writing test code.
In our previous chapter, it was easy to pass an `IssuedInvoices` stub to the `InvoiceFilter` class. The refactoring operation we performed (where we made the class receive its dependencies via constructors) facilitated the testing of the `InvoiceFilter` class.
Note, however, that this was not the case at the beginning of that chapter. We had to refactor the code for that to happen.
 
Software systems are often not ready/prepared to be tested, as seen in the classes in our previous chapter.
And so this chapter will show **how to design and build a software system in a way that increases its testability.**
 
Testability is the term used to describe how easy it is to write automated tests for the system, class, or method to be tested.
We already know that automated tests are crucial for high-quality software; it is, therefore, essential that our code is testable.
 
In this chapter, we'll discuss some design practices that increase the testability of software systems. This idea is referred to as **design for testability**. Specifically it involves:
 
* Dependency injection;
* The separation between domain and infrastructure code;
* Implementation-level tips.
 
{% set video_id = "iVJNaG3iqrQ" %}
{% include "/includes/youtube.md" %}
 
 
## Dependency injection
 
Dependency injection is a design choice we can use to make our code more testable.
We will illustrate what dependency injection is by means of this analogy:
 
> We need a hammer to perform a certain task.
> When we are asked to do this task, we find the hammer by ourselves and once we have the hammer, we use it to perform the task.
> However, another approach is to say that, while we still need a hammer when someone asks us to perform the task, instead of getting the hammer ourselves, we get it from the person that wants us to do the task.
 
We can do the same when managing the dependencies in our systems.
Simply put, instead of the class instantiating the dependency itself,
the class asks for the dependency (via constructor or a setter, for example).
 
But let's revisit how we applied this idea in the previous chapter.
Let's assume that the `InvoiceFilter` was implemented as follows:
 
```java
public class InvoiceFilter {
 
  public void filter() {
    IssuedInvoices dao = new IssuedInvoices();
 
    // ...
  }
 
}
```
 
In our analogy, the `InvoiceFilter` (the worker) itself instantiates (searches for) the `IssuedInvoices` (hammer) class. 
With an implementation like this, there is no easy way to pass any mocks to the `InvoiceFilter`.
Any test code we devise will necessarily use a concrete instance of `IssuedInvoices`. As we know, `IssuedInvoices` goes to a database, which is something we have been trying to avoid.
Thus, we cannot control the way the `IssuedInvoice` operates, at least for testing purposes.
This makes it harder for developers to write automated tests.
 
Instead, we can design our class in a way that it allows dependencies to be injected. Note how we receive the dependency via constructor now:
 
```java
public class InvoiceFilter {
 
  private final IssuedInvoices issuedInvoices;
 
  public InvoiceFilter(IssuedInvoices issuedInvoices) {
    this.issuedInvoices = issuedInvoices;
  }
 
  public void lowValuedInvoices() {
    // ...
  }
}
```
 
In this new implementation, we can now instantiate
the `InvoiceFilter` class and pass it a mocked/stubbed version of `IssuedInvoices` in the test code.
This simple change in the design of the class makes the creation of automated tests easier and, therefore, increases the testability of the code.
 
Note that with such a design decision, the `InvoiceFilter` class also enables the production code to pass a concrete instance of
`IssueInvoices`. After all, when the program is running "for real", we want the real implementation of `IssuedInvoices` to work.
 
More formally, _Dependency injection_ is a technique where one object supplies the required dependencies of the another object. As in our example, whenever a client decides to make use of `InvoiceFilter`, it will have to also supply an `IssuedInvoice`. The term "injection" is about "injecting" a dependency, in this case the `IssuedInvoice`, to another class, in this case `InvoiceFilter`.
 
The use of dependency injection improves our code in many ways:
 
* It enables us to mock/stub the dependencies in the test code, increasing the productivity of the developer during the testing phase.
* It makes all the dependencies more explicit; after all, they all need to be injected (via constructor, for example).
* It affords better separation of concerns: classes now do not need to worry about how to build their dependencies, as they are injected to them.
* The class becomes more extensible. As a client of the class, you can pass any dependency via the constructor. Suppose a class depends on a type `A` (and receives it via constructor). As a client, you can pass `A` or any implementation of `A`, e.g., if `A` is `List`, you can pass `ArrayList` or `LinkedList`. Your class can now work with many different implementations of `A`.
 
{% hint style='tip'%}
If you want to understand more advanced OOP concepts, we suggest reading more about:
 
* The Open-Closed Principle;
* Inversion of Control;
* Separation of concerns.
{% endhint %}
 
 
 
{% set video_id = "mGdsdBEWB5E" %}
{% include "/includes/youtube.md" %}
 
 
 
## Domain vs infrastructure
 
A general recommendation to design for testability comes down to separating _domain_ from _infrastructure_.
The _domain_ is where the core of the system lies, i.e. where all the business rules, logics, entities, services, etc, reside.
Throughout this book we have been using business systems as examples. Entities like `Invoice`, `ChristmasDiscountCalculator`
are examples of domain classes.
 
_Infrastructure_ relates to all code that handles some infrastructure. For example, pieces of code that handle database queries, or webservice calls, or file reads and writes.
In our examples, all our Data Access Objects are part of what we call infrastructure code.
 
We observe that, when domain code and infrastructure code are mixed up together, the system becomes harder to test.
Let us go back to our `InvoiceFilter` example with it now containing the SQL logic, instead of it depending on a Data Access Object:
 
```java
public class InvoiceFilter {
 
  // accessing the database
  private List<IssuedInvoice> all() {
    Connection connection = DriverManager.getConnection("db", "root", "");
 
    return withSql( () -> {
        try (var ps = connection.prepareStatement("select * from invoice")) {
            final var rs = ps.executeQuery();
 
            List<Invoice> allInvoices = new ArrayList<>();
            while (rs.next()) {
                allInvoices.add(new Invoice(rs.getString("name"), rs.getInt("value")));
            }
            return allInvoices;
        }
    });
 
    connection.close();
  }
 
  public List<Invoice> lowValueInvoices() {
 
    var issuedInvoices = all();
 
    return issuedInvoices.all().stream()
        .filter(invoice -> invoice.value < 100)
        .collect(toList());
  }
 
}
```
 
We can make the following observations about the code above:
 
* The code is less cohesive. It knows how to extract data from the database and it also knows the "low value invoices" business rule. This class now requires test cases that cover both responsibilities.
* Domain code and infrastructure code are mixed up. This means a tester will not be able to avoid database access when
testing the "low value invoices" rule. As we have seen many times already, this will incur higher costs.
* This new version of the `InvoiceFilter` class is definitely more complex than our previous version and complex code are more prone to defects.
 
Our previous version was indeed better. It was more cohesive and simpler. More importantly it also had a clear separation between domain code and infrastructure code. This is what software developers should always do when they design systems in order to ensure these two responsibilities are separated from each other.
 
This idea of separating infrastructure and domain is explored in the following literature:
 
* In the **Ports and Adapters** (also called the **Hexagonal Architecture**) idea, as proposed by Alistair Cockburn, the domain (business logic) depends on "Ports", rather than directly on the infrastructure.
These ports are interfaces that define what the infrastructure is able to do.
These ports are completely separated from the implementation of the infrastructure.
The "adapters", on the other hand, are very close to the infrastructure.
These are the implementations of the ports that talk to the database, webservice, etc.
They know how the infrastructure works and how to communicate with it.
 
In the schema below, you can see that the ports are part of the domain.
 
![Hexagonal Architecture](img/design-for-testability/hexagonal_architecture.svg)

Ports and Adapters help us a lot with the testability of our code.
If our core domain depends only on ports, we can easily stub/mock them.
 
* In his **Domain-Driven Design** work, Eric Evans proposes that the domain (the core of the system) will be isolated from the infrastructure layer. Besides all the design benefits that Eric cites in his book, testers benefit from this separation, as it enables them to exercise parts of code without having to depend on heavy infrastructure.
 
In practice, we observe that separating the infrastructure from domain is often challenging. The database example, where we move all the code to another class, is rather a simplistic one. When building software, we frequently rely on different libraries and frameworks that are often opinionated and require you to follow certain design decisions that might not be ideal, from a testability perspective. It is the duty of a developer to be able to abstract these problems, making sure that the domain concerns are always separated from the infrastructure concerns.
 
{% hint style='tip' %}
You see developers vouching for domain objects not to depend on concrete implementations of the infrastructure code, but rather, to depend solely on abstractions. In our example, the `InvoiceFilter` domain object, instead of depending on a concrete implementation of `IssuedInvoices` (one that right now contains SQL code and knows how to communicate with the database), it would depend on an abstraction/interface.
 
By devising interfaces that represent the abstract interaction that domains and infrastructure classes will have with each other, the developer ends up separating the concerns in a better way, reducing the coupling between both layers, and devising simpler flows of interactions between both layers.
 
The dependency inversion principle (note the _inversion_ and not _injection_) helps us to formalise these concepts:
 
* High-level modules should not depend on low-level modules. Both should depend on abstractions (e.g. interfaces).
* Abstractions should not depend on details. Details (concrete implementations) should depend on abstractions.
{% endhint %}
 
 
{% set video_id = "hv1XV87lJgA" %}
{% include "/includes/youtube.md" %}
 
 
 
## Implementation-level tips on designing for testability
 
We end this chapter with a couple of practical tips that will help you to devise testable systems/classes:
 
- **Cohesion and testability**: cohesive classes are classes that do only one thing.
Cohesive classes tend to be easier to test. This is because fewer responsibilities imply fewer test cases and fewer responsibilities often imply fewer dependencies (as you need fewer to compose the required functionality) which in turn incurs lower testing costs.<br><br>
On the other hand, a non-cohesive class tends to consume a large amount of testing effort from developers. You might notice that a non-cohesive class requires so many test cases, that you often feel like "the testing is never-ending".<br><br>
Refactoring non-cohesive classes is therefore an important task when it comes to testability. A common way to do this is by splitting the non-cohesive class into several smaller-but-cohesive classes. Each small class can then be tested separately, and the class that combines them  might rely either on mock objects to assert the correctness of the interactions among the dependencies or on an integration test (or both).
 
- **Coupling and testability**: Coupling refers to the number of classes that a class depends on. A highly coupled class requires several other
classes to do its work. Coupling decreases testability. A tester trying to test a highly dependent class ends up having to test all its dependencies together. If the tester then decides to use stubs/mocks, the costs of setting them up will also be higher than it needed to be (just imagine yourself setting up 10 or 15 stubs/mocks to test a single class). Moreover, the number of test cases that would be required to achieve a minimum amount of coverage is too high, as each dependency probably brings together a whole set of requirements and conditions.<br><br>
Reducing coupling, however, is often tricky, and maybe one of the biggest challenges in software design.
A common coupling-related refactoring is to group dependencies together into a higher and meaningful abstraction.
Imagine that class A depends on B, C, D and E. After inspection, you notice that B interacts with C, and D interacts with E.
Devising a new class that handles the communication between B and C (let us call it BC), and other one that handles the communication between D and E (let us call it DE), already reduces A's coupling. After all, it now depends only on BC, and DE. In general, pushing responsibilities and dependencies to smaller classes and later connecting them via larger abstractions is the way to go.
 
- **Complex conditions and testability**: We have seen in previous chapters that conditions that are very complex (e.g., an `if` statement composed of multiple Boolean operations) require great effort from testers. For example, the number of tests one might devise after applying some boundary testing or condition+branch coverage criteria might be
too high. <br><br>
Reducing the complexity of such conditions, for example by breaking it into multiple smaller conditions, will not reduce the overall complexity of the problem, but will "spread" it.
 
- **Private methods and testability**: A common question among developers is whether to test private methods or not.
In principle, testers should test private methods only through their public methods. However, testers often feel the urge to test a particular private method in isolation. One common cause for this feeling is the lack of cohesion or the complexity of this private method. In other words, this method does something so different to the public method, and/or its task is so complex, that it has to be tested separately. This is a good example of when "the test speaks to the developer" (a common saying among Test-Driven Developers).<br><br>
In terms of the design this might mean that this private method does not belong in its current place. A common refactoring is to extract this method, maybe to a brand new class. There, the former private method, now a public method, can be tested normally by the developer. The original class, where the private method used to be, should now depend on this new class.
 
- **Static methods and testability**: As we have seen before, static methods adversely affect testability, as they can not be stubbed easily. Therefore, a good rule of thumb is to avoid the creation of static methods whenever possible.
Exceptions to this rule are utility methods. As we saw before, utility methods are often not mocked.<br><br> 
If your system has to depend on a specific static method, e.g., because it comes with the framework your software depends on, adding an abstraction on top of it, similar to what we did with the `Calendar` class in the previous chapter, might be a good decision to facilitate testability.<br><br>
The same recommendation applies when your system needs code from others or external dependencies.
Again, creating layers/classes that abstract away the dependency might help you in increasing testability.
We emphasise that developers should not be afraid to create these extra layers. While it might seem that these layers will increase the overall complexity of the design, the increased testability pays off.
 
Finally, note how there is a [deep synergy between well designed production code and testability](https://www.youtube.com/watch?v=4cVZvoFGJTU).
We repeat that focusing only on testing techniques (like the ones we discussed in the _Testing Techniques_ section of this book), or only on design techniques (like the ones we have been focusing on in this section of the book), is not enough. High-quality software is only achieved when software systems are designed with testability in mind, and rigorous testing techniques are applied.
 
{% set video_id = "VaScxLhsDBQ" %}
{% include "/includes/youtube.md" %}
 
 
 
## Exercises
 
 
**Exercise 1.**
How can we improve the testability of the `OrderDeliveryBatch` class?
 
```java
public class OrderDeliveryBatch {
 
  public void runBatch() {
 
    OrderDao dao = new OrderDao();
    DeliveryStartProcess delivery = new DeliveryStartProcess();
 
    List<Order> orders = dao.paidButNotDelivered();
 
    for (Order order : orders) {
      delivery.start(order);
 
      if (order.isInternational()) {
        order.setDeliveryDate("5 days from now");
      } else {
        order.setDeliveryDate("2 days from now");
      }
    }
  }
}
 
class OrderDao {
  // accesses a database
}
 
class DeliveryStartProcess {
  // communicates with a third-party webservice
}
```
 
Which techniques can we apply? What would the new implementation look like? Think about what you would need to include in order to test the `OrderDeliveryBatch` class.
 
 
**Exercise 2.**
Consider the following requirement and implementation.
 
```text
A webshop gives a discount of 15% on King's Day.
```
 
```java
public class KingsDayDiscount {
 
  public double discount(double value) {
 
    Calendar today = Calendar.getInstance();
 
    boolean isKingsDay = today.get(MONTH) == Calendar.APRIL
        && today.get(DAY_OF_MONTH) == 27;
 
    return isKingsDay ? value * 0.15 : 0;
 
  }
}
```
 
We want to create a unit test for this class.
 
Why does this class have bad testability?
What can we do to improve the testability?
I.e. why is it difficult to test the method?
 
 
 
 
**Exercise 3.**
Sarah has joined a mobile app team that has been trying to write automated tests for a while.
The team wants to write unit tests for part of their code, but "that's really hard", according to the developers.
 
After some code review, the developers themselves listed the following
problems in their codebase:
 
1. Many classes mix infrastructure and business rules
2. The database has large tables and no indexes
3. Use of static methods
4. Some classes have too many attributes/fields
 
To increase the testability, the team has a budget to work on two out of the four issues above.
Which items should Sarah recommend them to tackle first?
 
Note: All of the four issues should obviously be fixed.
However, try to prioritise the two most important ones: which influence the testability the most?
 
 
**Exercise 4.**
Observability and controllability are two important concepts when it comes to software testing.
Three developers could benefit from improving either the observability or the controllability of the system/class which they are testing but each developer encounters a problem:
 
1. "I can't really assert that the method under test worked well."
2. "I need to make sure this class starts with that Boolean set to false, but I simply can't do it."
3. "I just instantiated the mock object, but there's no way to inject it in the class."
 
State for each of the problems above whether it relates to observability or controllability.
 
 
 
## References
 
* Cockburn, Alistair. The Hexagonal Architecture. https://wiki.c2.com/?HexagonalArchitecture
 
* Hevery, Misko. The Testability Guide. http://misko.hevery.com/attachments/Guide-Writing%20Testable%20Code.pdf
 
* Michael Feathers. The deep synergy between well design production code and testability. https://www.youtube.com/watch?v=4cVZvoFGJTU
 
* Martin, Robert C. The Dependency Inversion Principle. C++ Report. Archived from the original (PDF) on 2011-07-14: https://web.archive.org/web/20110714224327/http://www.objectmentor.com/resources/articles/dip.pdf
# Test code quality and engineering

You probably noticed that, once _test infected_, 
the amount of JUnit code that a software development team writes and maintains
is quite significant. In practice,
test code bases tend to grow quickly. Empirically, we have been observing
that Lehman's laws of evolution also apply to test code: code tends to rot, unless
one actively works against it. Therefore,
as with production code, **developers have to put extra effort 
into making high-quality test code bases, so that these can be maintained and developed in a
sustainable way**.

In this chapter, we address the following best practices in test code engineering:

* A set of principles that should guide developers when writing test code.
For these, we discuss both the FIRST principles (from the Pragmatic Unit Testing book),
as well as the recent Test Desiderata (proposed by Kent Beck).
* A set of well-known test smells that might emerge in test code.
* Some tips on how to make tests more readable.
* What flaky tests are and their possible causes.


## The FIRST properties

In the Pragmatic Unit Testing book, the authors discuss the "FIRST Properties of Good Tests".
FIRST is an acronym for fast, isolated, repeatable, self-validating, and timely:

- **Fast**: 
Tests are the safety net of a developer. Whenever developers perform any maintenance
or evolution in the source code, they use the feedback of the test suite to understand
whether the system is still working as expected. 
The faster the developer gets feedback from their test code, the better. 
Slower test suites force developers to simply run the tests less often,
making them less effective. Therefore, good tests are fast.
There is no hard line that separates slow from fast tests. It is fundamental to apply common sense.
Once you are facing a slow test, you may consider to:
    - Make use of mocks/stubs to replace slower components that are part of the test
    - Re-design the production code so that slower pieces of code can be tested separately from fast pieces of code
    - Move slower tests to a different test suite, one that developers may run less often. 
    It is not uncommon to see developers having sets of unit tests that run fast and all day long,
	and sets of slower integration and system tests that run once or twice a day on the Continuous Integration server. 


- **Isolated**: Tests should be as cohesive, as independent, and as isolated as possible. 
Ideally, a single test method should test just a single functionality or behaviour of the system.
Fat tests (or, as the test smells community calls them, eager tests) which test
multiple functionalities are often complex in terms of implementation. Complex test code reduces
the ability of developers to understand at a glance what is being tested, and makes future maintenance
harder. If you are facing such a test, break it into multiple smaller tests. Simpler and shorter
code is always better.

	Moreover, tests should not depend on other tests to run. The result of a test should be the
	same, whether the test is executed in isolation or together with the rest of the test suite.
	It is not uncommon to see cases where for example test B only works if test A is executed first.
	This is often the case when test B relies on the work of test A to set up the environment
	for it. Such tests become highly unreliable.
	In such cases, refactor the test code so that the tests
	are responsible for setting up the whole environment they need. If tests A and B depend on
	similar resources, make sure they can share the same code, so that you avoid duplicating
	code. JUnit's `@BeforeEach` or `@BeforeAll` methods can become handy. Make sure
	that your tests "clean up their messes", e.g., by deleting any possible files they created
	on the disk, or cleaning up values they inserted into a database.


- **Repeatable**: A repeatable test is a test that gives the same result, no matter how many times it is executed.
Developers tend to lose their trust in tests that present flaky behaviour (i.e., it sometimes passes, and sometimes fails without any changes in the system and/or in the test code).
Flaky tests can happen for different reasons, and some of the causes can be tricky
to identify. (Companies have reported extreme examples where a test presented flaky behaviour
only once in a month.) Common causes are dependencies on external resources, not waiting long
enough for an external resource to finish its task, and concurrency.


- **Self-validating**: 
The tests should validate/assert the result themselves. This might seem an unnecessary
principle to mention. However, it is not uncommon for developers to make mistakes and not write
any assertions in a test, causing the test to always pass. In other more complex cases,
writing the assertions or, in other words, verifying the expected behaviour, might not be possible.
In cases where observing the outcome of behaviour is not easily achievable, we suggest
the developer to refactor the class or method under test to increase its observability (revisit
our chapter on design for testability).


- **Timely**: 
Developers should be _test infected_. They should write and run tests as often
as possible. While less technical than the other principles in this list, changing
the behaviour of development teams towards writing automated test code can still be challenging.

	Leaving the test phase to the very end of the development process, as commonly done
	in the past, can result in
	unnecessary costs. After all, at that point, the system might be simply too hard to test.
	Moreover, as we have seen, tests serve as a safety net for developers. Developing large
	complex systems without such a net is highly unproductive and likely to fail.

{% set video_id = "5wLrj-cr9Cs" %}
{% include "/includes/youtube.md" %}


## Test Desiderata

Kent Beck, the "creator" of Test-Driven Development (and author of the 
["Test-Driven Development: By Example"](https://www.amazon.com/Test-Driven-Development-Kent-Beck/dp/0321146530) book), recently wrote a list of twelve
properties that good tests have (the [test desiderata](https://medium.com/@kentbeck_7670/test-desiderata-94150638a4b3)). 

The following list comes directly from his blog post. Note how some of these properties
are also part of the FIRST properties.

* [Isolated](https://www.youtube.com/watch?v=HApI2cspQus): tests should return the same results regardless of the order in which they are run.
* [Composable](https://www.youtube.com/watch?v=Wf3WXYaMt8E): if tests are isolated, then I can run 1 or 10 or 100 or 1,000,000 and get the same results.
* [Fast](https://www.youtube.com/watch?v=L0dZ7MmW6xc): tests should run quickly.
* [Inspiring](https://www.youtube.com/watch?v=2Q1O8XBVbZQ): passing the tests should inspire confidence.
* [Writable](https://www.youtube.com/watch?v=CAttTEUE9HM): tests should be cheap to write relative to the cost of the code being tested.
* [Readable](https://www.youtube.com/watch?v=bDaFPACTjj8): tests should be comprehensible for their readers, and it should be clear why they were written.
* [Behavioural](https://www.youtube.com/watch?v=5LOdKDqdWYU): tests should be sensitive to changes in the behaviour of the code under test. If the behaviour changes, the test result should change.
* [Structure-insensitive](https://www.youtube.com/watch?v=bvRRbWbQwDU): tests should not change their result if the structure of the code changes.
* [Automated](https://www.youtube.com/watch?v=YQlmP08dj6g): tests should run without human intervention.
* [Specific](https://www.youtube.com/watch?v=8lTfrCtPPNE): if a test fails, the cause of the failure should be obvious.
* [Deterministic](https://www.youtube.com/watch?v=PwWyp-wpFiw): if nothing changes, the test result should not change.
* [Predictive](https://www.youtube.com/watch?v=7o5qxxx7SmI): if the tests all pass, then the code under test should be suitable for production.

For more interested readers, watch [Kent Beck talking about it in an open talk](https://www.youtube.com/watch?v=lXTwxMxNx-Y).


## Test code smells

Now that we have covered some best practices, let us look at the other
side of the coin: **test code smells**.

The term _code smell_ is a well-known term that indicates possible symptoms that might
indicate deeper problems in the source code of the system. 
Some well-known examples are *Long Method*, *Long Class*, or *God Class*.
A number of research papers show us that code smells hinder the comprehensibility and the maintainability of software systems.

While the term has long been applied to production code, given
the rise of test code, our community has been developing catalogues of smells that
are now specific to test code.
Research has also shown that test smells are prevalent in real life and, unsurprisingly, 
often have a negative impact on the maintenance and comprehensibility of the test suite.

We discuss below several of the well-known test smells. A more comprehensive
list can be found in the xUnit Test Patterns book, by Meszaros.


**Code Duplication**: 
It is not surprising that code duplication can also happen in test code, 
as it is very common in production code.
Tests are often similar in structure. You may have noticed it in several of the code
examples throughout this book. We even made use of parameterised tests
to reduce some of the duplication.
A less attentive developer might end up writing duplicated code 
(copying and pasting often happens in real life) instead of putting
some effort into implementing a better solution. 

Duplicated code can reduce the productivity of software testers.
After all, if there is a need for a change in a duplicated piece of code, a developer
will have to apply the same change in all the places where the code was duplicated. 
In practice, it is easy to forget one of these places, which causes one to end up with 
problematic test code.
Note that the effects are similar to the effects of code duplication in production code.

We advise developers to refactor their test code ruthlessly. The extraction of a duplicated piece of
code to private methods or external classes is often a good solution for the problem.

**Assertion Roulette**:
Assertions are the first thing a developer looks at when a test is failing.
Assertions, have to communicate clearly what is going wrong with the component
under test. 
The test smell emerges when it is hard to understand the 
assertions themselves, or why they are failing.

There are several reasons for this smell to happen. Some features or business rules
are so complex that they require a complex set of assertions to ensure their behaviour.
In these situations, developers end up writing complex assert instructions that are not easy to
understand. To help with such cases, we recommend developers to:

1. Write customised assert instructions that abstract away part of the complexity of the assertion code itself.
2. Write code comments that explain quickly and in natural language what those assertions are about. (This mainly applies when the assertions are not self-explanatory.)

Interestingly, a common best practice that is often found in the test best practice literature is the "one assertion per method" strategy. While forcing developers to have a single assertion per test method is too extremist, the idea of minimising the number of assertions in a test method is valid.

Note that a high number of simple assertions in a single test can be as harmful as a complex
set of assertions. In such cases, we provide a similar recommendation: write a customised
assertion instruction to abstract away the need for long sequences of assertions.

Empirically, we also observe that the number of assertions in a test is often large, because
developers tend to write more than one test case in a single test method. We have done
this in this book too (see the boundary testing chapter, where we test both sides of the boundary
in a single test method). However, parsimony is fundamental. Splitting up a large test method
that contains multiple test cases can reduce the cognitive load required by the developer
to understand it.


**Resource Optimism**:
Resource optimism happens when a test assumes that a necessary resource (e.g., a database) is readily available at the start of its execution. 
This is related to the _isolated_ principle of the FIRST principles and of Beck's test desiderata.

To avoid resource optimism, a test should not assume that the resource is already in the correct state. The test should be the one responsible for setting up the state itself. 
This can mean that the test is the one responsible for populating a database, for writing the required files in the disk, or for starting up a Tomcat server. (This set up might require complex code, and developers
should also do their best effort in abstracting way such complexity by, e.g., moving such 
code to other classes, like `DatabaseInitialization` or `TomcatLoader`, allowing the
test code to focus on the test cases themselves).

Similarly, another incarnation of the resource optimism smell happens
when the test assumes that the resource is available all the time.
Imagine a test method that interacts with a webservice,
which might be down for reasons we do not control.

To avoid this test smell, developers have two options:

1. Avoid using external resources, by using stubs and mocks.
2. If the test cannot avoid using the external dependency, make it robust enough.
Make your test suite skip that test when the resource is unavailable, and provide a message explaining why that was the case. This seems counterintuitive, but again, remember
that developers trust their test suites. Having a single test failing for the wrong reasons
makes developers lose their confidence in the entire test suite.

In addition to changing the tests, developers must make sure 
that the environments where the tests are executed have the required resources available.
Continuous integration tools like Jenkins, CircleCI, and Travis can help developers in 
making sure that tests are being run in the correct environment.



**Test Run War**:
The war is an analogy for when two tests are "fighting" over the same resources.
One can observe a test run war when tests start to fail as soon as more than one developer
run their test suites.
Imagine a test suite that uses a centralised database. When developer A runs the test, the test changes the state of the database. At the same time, 
developer B runs the same test, which also uses the same database. 
Both tests are now using the same database at the same time. 
This unexpected situation may cause the test to fail.

_Isolation_ is key to avoid this test smell. In the example of a centralised database,
one solution would be to make sure that each developer has their own instance of a database. That would
avoid the fight over the same resource. (Related to this example, also see the chapter on database testing.)


**General Fixture**:
A fixture is the set of input values that will be used to exercise the component under test.
Fixtures are set up in the _arrange_ part of the test, which was discussed in a previous chapter.
As you may have noticed, fixtures are the "stars" of the test method, as they derive
naturally from the test cases we devised using any of the techniques we have discussed.

When testing more complex components, developers may need to make use of several
different fixtures: one for each partition they want to exercise. These fixtures can
then become complex. 
And to make the situation worse, while tests are different from each other, their fixtures
might have some intersection. 

Given this possible intersection amongst the different fixtures, as well as the difficulty
with building these complex entities and fixtures, a less attentive developer
could decide to declare a "large" fixture that works for many different tests. 
Each test would then use a small part of this large fixture. 

While this approach might work and the tests might correctly implement the test cases,
they are hard to maintain. Once a test fails, developers who try
to understand the cause of the failure, will face a large fixture that is not totally
relevant for them. In practice, the developer would have to manually
"filter out" parts of the fixture that are not really exercised by the failing test.
That is an unnecessary cost.
Making sure that the fixture of a test is as specific and cohesive as possible helps
developers to comprehend the essence of a test (which is often highly relevant when
the test starts to fail).

Build patterns, with the focus of building test data, 
can help developers in avoiding such a smell. More specifically, 
the **[Test Data Builder](http://www.natpryce.com/articles/000714.html)** pattern is
often used in test code of enterprise applications (we give an example
of a Test Data Builder later in this chapter). Such applications
often have to deal with the creation of complex sets of interrelated 
business entities, which can easily
lead developers to write general fixtures.

**Indirect tests** and **eager tests**:
Tests should be as cohesive and as focused as possible. A testing class `ATest` that aims at testing
class `A` should solely focus on testing class `A`. Even if it depends on class
`B`, requiring `ATest` to instantiate `B`, `ATest` should focus on exercising `A` and `A` only.
The smell emerges when a test class focuses its efforts on testing many classes
at once. 

Less cohesive tests lead to less productivity. How do developers know where tests for a given class `B` 
are? If test classes focus on more than a single class, tests for `B` could be anywhere.
Developers would have to look for them. 
It is also expected that, without proper care, tests for a single class would live in
many other test classes, e.g., tests for `B` might exist in `ATest`, `BTest`, `CTest`, etc.

Tests, and more specifically, unit test classes and methods, should have a clear focus.
They should test a single unit. If they have to depend on other classes, the use of
mocks and stubs can help the developer in isolating that test and avoid _indirect
testing_. If the use of mocks and stubs is not possible, make sure that assertions
focus on the real class under test, and that failures caused by dependencies (and not 
by the class under test) are clearly indicated in the outcome of the test method.

Similar to what we discussed when talking about the excessive number of assertions
in a single test, avoiding _eager tests_, or tests that exercise more than a unique
behaviour of the component is also best practice. Test methods that exercise multiple
behaviours at once tend to be overly long and complex, making it harder for developers
to comprehend them quickly.

**Sensitive Equality**:
Good assertions are fundamental in test cases. A bad assertion may cause a test
to not fail when it should. However, a bad assertion may also cause a test _to fail
when it should not_.
Engineering a good assertion statement is challenging. Even more so when components
produce fragile outputs, i.e., outputs that tend to change often. 
Test code should be as resilient as possible to the implementation details
of the component under test. Assertions should also not be oversensitive to internal changes. 

Imagine a class `Item` that represents an item in a shopping cart. 
An item is composed of a name, a quantity, and an individual price. The final price
of the item is the multiplication of its quantity per its individual price.
The class has the following implementation:

```java
import java.math.BigDecimal;

public class Item {

	private final String name;
	private final int qty;
	private final BigDecimal individualPrice;

	public Item(String name, int qty, BigDecimal individualPrice) {
		this.name = name;
		this.qty = qty;
		this.individualPrice = individualPrice;
	}

	// getters ...

	public BigDecimal finalAmount() {
		return individualPrice.multiply(new BigDecimal(qty));
	}

	@Override
	public String toString() {
		return "Product " + name + " times " + qty + " = " + finalAmount();
	}
}
```

Suppose now that a less attentive developer writes the following test to exercise
the `finalAmount` behaviour:

```java
public class ItemTest {

	@Test
	void qtyTimesIndividualPrice() {
		var item = new Item("Playstation IV with 64 GB and super wi-fi",
				3,
				new BigDecimal("599.99"));

		// this is too sensitive!
		Assertions.assertEquals("Product Playstation IV with 64 GB " +
				"and super wi-fi times " + 3 + " = 1799.97", item.toString());
	}
}
```

The test above does exercise the calculation of the final amount. However,
one can see that the developer took a shortcut. She decided to assert the overall
behaviour by making use of the `toString` method of the class. Maybe because
the developer felt that this assertion was stricter, as it asserts not only
the final price, but also the name of the product and its quantity. 

While this seems to work at first,
this assertion is sensitive to changes in the implementation of the `toString`. 

Clearly,
the tester only wants the test to fail if the `finalAmount` method changes, and not if the
`toString` method changes. That is not what happens. Suppose that another developer
decided to shorten the length of the outcome of the `toString`:

```java
@Override
public String toString() {
	return "Product " + name.substring(0, Math.min(11, name.length())) + 
	  " times " + qty + " = " + finalAmount();
}
```

Suddenly, our `qtyTimesIndividualPrice` test fails:

```
org.opentest4j.AssertionFailedError: 
Expected :Product Playstation IV with 64 GB and super wi-fi times 3 = 1799.97
Actual   :Product Playstation times 3 = 1799.97
```

A better assertion for this would be to assert precisely what is wanted from that behaviour.
In this case, assert that the final amount of the item is correctly calculated. Then the 
implementation for the test would be better like this:

```java
@Test
void qtyTimesIndividualPrice_lessSensitiveAssertion() {
	var item = new Item("Playstation IV with 64 GB and super wi-fi",
			3,
			new BigDecimal("599.99"));

	Assertions.assertEquals(new BigDecimal("1799.97"), item.finalAmount());
}

```

Remember our discussion regarding design for testability. It may be better to 
create a method with the sole purpose of facilitating the test (or, in this case, the assertion)
rather than having to rely on sensitive assertions that will possibly break the test
for the wrong reason in the future.

**Inappropriate assertions**: Having the proper assertions makes a huge
difference between a good and a bad test case. While we have discussed how to derive
good test cases (and therefore also good assertions), choosing the right implementation strategy
for writing the assertion can affect the maintenance of the test in the long run.
The wrong choice of an assertion instruction may give developers less information
about the failure, making the debugging process more difficult.

Imagine an implementation of a `Cart` that receives products
to be inserted into it. 
Products cannot be repeated. 
A simple implementation could be:

```java
public class Cart {
	private final Set<String> items = new HashSet<>();

	public void add(String product) {
		items.add(product);
	}

	public int numberOfItems() {
		return items.size();
	}
}
```

Now, a developer decided to test the `numberOfItems` behaviour. He then wrote the following
test cases:

```java
public class CartTest {
	private final Cart cart = new Cart();

	@Test
	void numberOfItems() {
		cart.add("Playstation");
		cart.add("Big TV");

		assertTrue(cart.numberOfItems() == 2);
	}

	@Test
	void ignoreDuplicatedEntries() {
		cart.add("Playstation");
		cart.add("Big TV");
		cart.add("Playstation");

		assertTrue(cart.numberOfItems() == 2);
	}
}
```

Note that the less attentive developer opted for an `assertTrue`. While the test
works as expected, if it ever fails (which we can easily force by replacing the Set for a List
in the `Cart` implementation), the assertion error message will be something like this:

```
org.opentest4j.AssertionFailedError: 
Expected :<true> 
Actual   :<false>
```

The error message does not explicitly show the difference in the values. In this
simple example, it may not seem very important, but with more 
complicated test cases, a developer would have to add some debugging
code (`System.out.println`s) to print the actual value that was produced by the method.

The test could help the developer by giving as much information as possible. With that aim,
choosing the right assertions is important, as they tend to give more information. 
In this case, the use of an `assertEquals` is a better fit:

```java
@Test
void numberOfItems() {
	cart.add("Playstation");
	cart.add("Big TV");

	// assertTrue(cart.numberOfItems() == 2);
	assertEquals(2, cart.numberOfItems());
}
```

Libraries such as AssertJ, besides making the assertions more legible, also
help us in providing better error messages. Suppose our `Cart` class now has an 
`allItems()` method that returns all items that were previously stored in this cart:

```java
public Set<String> allItems() {
	return Collections.unmodifiableSet(items);
}
```

Asserting the outcome of this method using plain old JUnit assertions would look
like the following:

```java
@Test
void allItems() {
	cart.add("Playstation");
	cart.add("Big TV");

	var items = cart.allItems();

	assertTrue(items.contains("Playstation"));
	assertTrue(items.contains("Big TV"));
}
```

AssertJ enables us to not only write assertions about the items, 
but also about the structure of the set itself.
In the following example, we use a single assertion to make sure 
the set _only_ contains two specific items.
The method `containsExactlyInAnyOrder()` does exactly what its name says:

```java
@Test
void allItems() {
	cart.add("Playstation");
	cart.add("Big TV");

	var items = cart.allItems();

	assertThat(items).containsExactlyInAnyOrder("Playstation", "Big TV");
}
```

We therefore recommend that developers choose wisely how to write the assertion statements.
A good assertion clearly reveals its reason for failing, is legible, and is as specific
and insensitive as possible.


**Mystery Guest**: Integration tests often rely on external 
dependencies. These dependencies, or "guests", can be things like databases, 
files on the disk, or webservices. 
While such dependencies are unavoidable in these types of tests, making them explicit
in the test code may help developers in cases where these tests suddenly start to fail.
A test that makes use of a guest, but hides it from the developer (making it 
a "mystery guest") is simply harder to comprehend.

Make sure your test gives proper error messages, differentiating between a fail in the
expected behaviour and a fail due to a problem in the guest. Having assertions dedicated
to ensuring that the guest is in the right state before running the tests is often
the remedy that is applied to this smell.

{% set video_id = "QE-L818PDjA" %}
{% include "/includes/youtube.md" %}

{% set video_id = "DLfeGM84bzg" %}
{% include "/includes/youtube.md" %}


## Test code readability

We have discussed several test code best practices and smells. In many situations,
we have argued for the need of comprehensible, i.e., easy to read, test code.

We reinforce the fact that it 
is crucial that the developers can understand the test code easily.
**We need readable and understandable test code.** 
Note that "readability" is one of the test desiderata we mentioned above.

In the following subsections we provide advice on how to write readable test code.

### Test structure
As you have seen earlier, tests all follow the same structure: the Arrange, Act and Assert
structure.
When **these three parts are clearly separated, it is easier for a developer to see what is happening in the test**.
Your tests should make sure that a developer can identify these parts quickly and get the answers to the following questions: Where is the fixture? Where is the behaviour/method under test? 
Where are the assertions?

### Comprehensibility of the information
Test code is full of information, i.e., the input values that will be provided
to the class under test, how the information flows up to the method under test, 
how the output comes back from the exercise behaviour, and what the expected outcomes are.

Often we have to deal with complex data structures and information, which makes 
the test code also complex.
In such cases, **we should make sure that the (meaning of the) 
important information present in a test is easy to understand.**

Giving descriptive names to the information in the test code is helpful with this.
Let us illustrate this in the example below.
Suppose we have written a test for an `Invoice` class, which calculates the tax for
that invoice.

```java
public class Invoice {

	private final BigDecimal value;
	private final String country;
	private final CustomerType customerType;

	public Invoice(BigDecimal value, String country, CustomerType customerType) {
		this.value = value;
		this.country = country;
		this.customerType = customerType;
	}

	public BigDecimal calculate() {
		double ratio = 0.1;

		// some business rule here to calculate the ratio
		// depending on the value, company/person, country ...

		return value.multiply(new BigDecimal(ratio));
	}
}
```

Not-so-clear test code for the `calculate()` method could look like this:

```java
@Test
void test1() {
	var invoice = new Invoice(new BigDecimal("2500"), "NL", CustomerType.COMPANY);
	var v = invoice.calculate();
	assertEquals(250, v.doubleValue(), 0.0001);
}
```

Note how, at first glance, it may be hard to understand what all the information 
in the code means. It may require some extra effort to understand what
this invoice looks like. Imagine a real entity from a real enterprise system: an `Invoice`
class might have dozens of attributes. The name of the test as well as the name of
the cryptic variable `v` do not clearly explain what they mean. For developers less fluent
in JUnit, it may also be hard to understand what the 0.0001 means (it mitigates floating-point errors by making sure that both numbers are equal within the given delta).

A better version of this test method could be:

```java
@Test
void taxesForCompanies() {
	var invoice = new InvoiceBuilder()
			.asCompany()
			.withCountry("NL")
			.withAValueOf("2500")
			.build();

	var calculatedValue = invoice.calculate();

	assertThat(calculatedValue).isCloseTo(new BigDecimal("250"), within(new BigDecimal("0.001")));
} 
```

The usage of the `InvoiceBuilder` (the implementation of which we will show shortly) clearly expresses what
this invoice is about: it is an invoice for a company (as clearly stated by the `asCompany()` 
method), "NL" is the country of that invoice, and the invoice has a value of 2500. The
result of the behaviour now goes to a variable whose name says it all (`calculatedValue`). 
The assertion then explicitly mentions that, given this is a float number, the best we can do
is to compare whether they are close enough.

The `InvoiceBuilder` is an example of an implementation of a **Test Data Builder**, the design
pattern we mentioned before. The builder helps developers in creating fixtures, by providing
them with a clear and expressive API. The use of fluent interfaces 
(e.g., `asCompany().withAValueOf()...`) is also a common implementation choice.
In terms of its implementation, the `InvoiceBuilder` is simply
a Java class. The trick that allows methods to be chained is to return the class itself
in the methods (note that methods return `this`).

```java
public class InvoiceBuilder {

	private String country = "NL";
	private CustomerType customerType = CustomerType.PERSON;
	private BigDecimal value = new BigDecimal("500.0");

	public InvoiceBuilder withCountry(String country) {
		this.country = country;
		return this;
	}

	public InvoiceBuilder asCompany() {
		this.customerType = CustomerType.COMPANY;
		return this;
	}

	public InvoiceBuilder withAValueOf(String value) {
		this.value = new BigDecimal(value);
		return this;
	}

	public Invoice build() {
		return new Invoice(value, country, customerType);
	}
}
``` 

Developers should feel free to customise their builders as much as they want. A common
trick is to make the builder build a "common" version of the class, without requiring
the call to all the setup methods. A developer can then write:

```java
var invoice = new InvoiceBuilder().build();
```

In such a case, the `build` method, without any setup, will always build an invoice
for a person, with a value of 500.0, and having NL as country (see the initialised
values in the `InvoiceBuilder`). 

Other developers may write several shortcut methods that build other
"common" fixtures for the class. In the following example,
the `anyCompany()` method returns an Invoice that belongs to a company (and the
default value for the other fields). 
The `anyUS()` method builds an Invoice for someone in the US:

```java
public Invoice anyCompany() {
	return new Invoice(value, country, CustomerType.COMPANY);
}

public Invoice anyUS() {
	return new Invoice(value, "US", customerType);
}
```

Note how test data builders can help developers to avoid general fixtures.
Given that the builder makes it easier to build complex objects, developers may not
feel the need to rely so much on a general fixture.

Introducing test data builders, making good use of variable names to explain the meaning
of the information, having clear assertions, and (although not exemplified here) adding
comments in cases where code is not expressive enough will help developers in better
comprehending test code.


{% set video_id = "RlqLCUl2b0g" %}
{% include "/includes/youtube.md" %}

## Flaky tests

Flaky tests (or _erratic tests_, as Meszaros calls them in his book)
are tests that present "flaky" behaviour: 
they sometimes pass and sometimes fail, even though
developers have not performed any changes in their software systems.

Such tests have a negative impact on the productivity of software development teams.
It is hard to know whether a flaky test is failing because the behaviour
is buggy, or because it is simply flaky. Little by little, the excessive
presence of flaky tests can make developers lose confidence in their test
suites. Such lack of confidence might lead them to deploy their
systems even though the tests are failing (after all, they might be broken just because
of flakiness, and not because the system is misbehaving).

The prevalence and consequential impact of flaky tests in the software development
world has been increasing over time. Companies like Google and Facebook as well
as software engineering researchers have been
working extensively on automated ways to detect and fix flaky tests.


Flaky tests can have many causes, of which we can name these:

* A test can be flaky because it **depends on external and/or shared resources**.
For example, when we need a database to run our tests, 
any of the following things can happen:
    - Sometimes the test passes, because the database is available.
    - Sometimes it fails, because the database is not available.
    - Sometimes the test passes because the database is clean and ready for that test.
    - Sometimes the test fails because the same test was being run by the next developer and the database was not in a clean state.

* The tests can be flaky due to improper time-outs.
This is a common cause in web testing.
Suppose a test has to wait for something to happen in the system, e.g., a request
coming back from a webservice, which is then displayed in some HTML element.
If the web application is a bit slower than normal, the test might fail, just
because "it did not wait long enough".

* Tests can be flaky due to a possible hidden interaction between different
test methods.
Suppose that test A somehow influences the result of test B, 
possibly causing it to fail.

As you may have noticed, some of these causes 
correspond to scenarios we described when discussing the test smells 
and emphasise again how important the quality of test code is.

{% hint style='tip' %}
If you want to find the exact cause of a flaky test, 
the author of the XUnit Test Patterns book has made a whole decision table.
You can find it in the book or on Gerard Meszaros' website [here](http://xunitpatterns.com/Erratic%20Test.html).
Using the decision table you can find a probable cause for the flakiness of your test.

We list several interesting research papers on flaky tests, their impact on software testing, and current state-of-the-art detection tools in our references section.
{% endhint %}


{% set video_id = "-OQgBMSBL5c" %}
{% include "/includes/youtube.md" %}






## Exercises

**Exercise 1.**
Jeanette just heard that two tests are behaving strangely as when executed in isolation, both of them pass, but when executed together, they fail. Which one of the following **is not** the cause of this?

1. Both tests are very slow.
2. They depend upon the same external resources.
3. The execution order of the tests matters.
4. They do not perform a clean-up operation after execution.


**Exercise 2.**
RepoDriller is a project that extracts information from Git repositories. Its integration tests use lots of real Git repositories (that are created solely for the purpose of the test), each one with a different characteristic, e.g., one repository contains a merge commit, another repository contains a revert operation, etc.

Its tests look like this:

```java
@Test
public void test01() {

  // arrange: specific repo
  String path = "test-repos/git-4";

  // act
  TestVisitor visitor = new TestVisitor();
  new RepositoryMining()
  .in(GitRepository.singleProject(path))
  .through(Commits.all())
  .process(visitor)
  .mine();
  
  // assert
  Assert.assertEquals(3, visitor.getVisitedHashes().size());
  Assert.assertTrue(visitor.getVisitedHashes().get(2).equals("b8c2"));
  Assert.assertTrue(visitor.getVisitedHashes().get(1).equals("375d"));
  Assert.assertTrue(visitor.getVisitedHashes().get(0).equals("a1b6"));
}
```


Which test smell does this piece of code _might_ suffer from?

1. Mystery guest.
2. Condition logic in test.
3. General fixture.
4. Flaky test.


**Exercise 3.**
In the code below, we present the source code of an automated test.


```java
@Test
public void flightMileage() {
  // setup fixture
  // exercise constructor
  Flight newFlight = new Flight(validFlightNumber);
  // verify constructed object
  assertEquals(validFlightNumber, newFlight.number);
  assertEquals("", newFlight.airlineCode);
  assertNull(newFlight.airline);
  // setup mileage
  newFlight.setMileage(1122);
  // exercise mileage translator
  int actualKilometres = newFlight.getMileageAsKm();    
  // verify results
  int expectedKilometres = 1810;
  assertEquals(expectedKilometres, actualKilometres);
  // now try it with a canceled flight
  newFlight.cancel();
  boolean flightCanceledStatus = newFlight.isCancelled();
  assertTrue(flightCanceledStatus);
}
```

However, Joe, our new test specialist, believes this test is smelly and that it can be improved.
Which of the following should be Joe's main concern?

  
1. The test contains code that may or may not be executed, making the test less readable.
2. It is hard to tell which of several assertions within the same test method will cause a test failure.
3. The test depends on external resources and has nondeterministic results depending on when/where it is run.
4. The test reader is not able to see the cause and effect between fixture and verification logic because part of it is done outside the test method.



**Exercise 4.**
Look at the test code below. What is the most likely test code smell that this piece of code presents?

```java
@Test
void test1() {
  // webservice that communicates with the bank
  BankWebService bank = new BankWebService();

  User user = new User("d.bergkamp", "nl123");
  bank.authenticate(user);
  Thread.sleep(5000); // sleep for 5 seconds

  double balance = bank.getBalance();
  Thread.sleep(2000);

  Payment bill = new Payment();
  bill.setOrigin(user);
  bill.setValue(150.0);
  bill.setDescription("Energy bill");
  bill.setCode("YHG45LT");

  bank.pay(bill);
  Thread.sleep(5000);

  double newBalance = bank.getBalance();
  Thread.sleep(2000);
  
  // new balance should be previous balance - 150
  Assertions.assertEquals(newBalance, balance - 150);
}
```

1. Flaky test.
2. Test code duplication.
3. Obscure test.
4. Long method.



**Exercise 5.**
In the code below, we show an actual test from Apache Commons Lang, a very popular open source Java library. This test focuses on the static `random()` method, which is responsible for generating random characters. An interesting detail in this test is the comment: *Will fail randomly about 1 in 1000 times.*

```java
/**
 * Test homogeneity of random strings generated --
 * i.e., test that characters show up with expected frequencies
 * in generated strings.  Will fail randomly about 1 in 1000 times.
 * Repeated failures indicate a problem.
 */
@Test
public void testRandomStringUtilsHomog() {
    final String set = "abc";
    final char[] chars = set.toCharArray();
    String gen = "";
    final int[] counts = {0,0,0};
    final int[] expected = {200,200,200};
    for (int i = 0; i< 100; i++) {
       gen = RandomStringUtils.random(6,chars);
       for (int j = 0; j < 6; j++) {
           switch (gen.charAt(j)) {
               case 'a': {counts[0]++; break;}
               case 'b': {counts[1]++; break;}
               case 'c': {counts[2]++; break;}
               default: {fail("generated character not in set");}
           }
       }
    }
    // Perform chi-square test with df = 3-1 = 2, testing at .001 level
    assertTrue("test homogeneity -- will fail about 1 in 1000 times",
        chiSquare(expected,counts) < 13.82);
}
```

Which one of the following **is incorrect** about the test?

1. The test is flaky because of the randomness that exists in generating characters.
2. The test checks for invalidly generated characters, and that characters are picked in the same proportion.
3. The method being static has nothing to do with its flakiness.
4. To avoid the flakiness, a developer should have mocked the random function. 



## References

Test code best practices:

- Chapter 5 of Pragmatic Unit Testing in Java 8 with Junit. Langr, Hunt, and Thomas. Pragmatic Programmers, 2015.
- Meszaros, G. (2007). xUnit test patterns: Refactoring test code. Pearson Education.

Empirical studies:

- Pryce, N. Test Data Builders: an alternative to the Object Mother pattern. http://natpryce.com/articles/000714.html. Last accessed in March, 2020.
- Bavota, G., Qusef, A., Oliveto, R., De Lucia, A., & Binkley, D. (2012, September). An empirical analysis of the distribution of unit test smells and their impact on software maintenance. In 2012 28th IEEE International Conference on Software Maintenance (ICSM) (pp. 56-65). IEEE.

Flaky tests:

- Luo, Q., Hariri, F., Eloussi, L., & Marinov, D. (2014, November). An empirical analysis of flaky tests. In Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (pp. 643-653). ACM. 
Authors' version: [http://mir.cs.illinois.edu/~eloussi2/publications/fse14.pdf](http://mir.cs.illinois.edu/~eloussi2/publications/fse14.pdf)
- Bell, J., Legunsen, O., Hilton, M., Eloussi, L., Yung, T., & Marinov, D. (2018, May). DeFlaker: automatically detecting flaky tests. In Proceedings of the 40th International Conference on Software Engineering (pp. 433-444). ACM. 
Authors' version: [http://mir.cs.illinois.edu/legunsen/pubs/BellETAL18DeFlaker.pdf](http://mir.cs.illinois.edu/legunsen/pubs/BellETAL18DeFlaker.pdf)
- Lam, W., Oei, R., Shi, A., Marinov, D., & Xie, T. (2019, April). iDFlakies: A Framework for Detecting and Partially Classifying Flaky Tests. In 2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST) (pp. 312-322). IEEE. 
Authors' version: [http://taoxie.cs.illinois.edu/publications/icst19-idflakies.pdf](http://taoxie.cs.illinois.edu/publications/icst19-idflakies.pdf)
- Listfield, J. Where do our flaky tests come from?  
Link: [https://testing.googleblog.com/2017/04/where-do-our-flaky-tests-come-from.html](https://testing.googleblog.com/2017/04/where-do-our-flaky-tests-come-from.html), 2017.
- Micco, J. Flaky tests at Google and How We Mitigate Them.  
Link: [https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html](https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html), 2017.
- Fowler, M. Eradicating Non-Determinism in Tests. Link: [https://martinfowler.com/articles/nonDeterminism.html](https://martinfowler.com/articles/nonDeterminism.html), 2011.
# Pragmatic testing

While software testing is an essential skill for every developer and often leads to a much higher quality end product, some systems are written in such ways that simple testing becomes virtually impossible. More often than not, the difficulties in testing a system often leads to testing being neglected.

The idea behind this section on pragmatic testing is realizing that software is highly variable, and that different components need different techniques and/or resources to be tested. 

In this section, we present the various ways you can test your system and, more importantly, try to guide you in determining which parts require the most attention, what alternative ways there are to write and test them, and how to divide your testing resources among them. We also talk about the maintenance of your test suite as time passes and the software grows to larger and larger proportions.

We will discuss:

- **The Testing pyramid**: The various levels of testing and how much effort should be distributed amongst them.
- **Mock Objects**: How to use mock (mimic) objects to ease the testability of some components.
- **Design for Testability**: How to design and build a software system in a way that increases its testability.
- **Test-Driven Development**: The idea of writing test code before the production code in order to make use of the design feedback they can provide.
- **Test Code Quality and Engineering**: Best practices in test code engineering, guiding principles for developers, well-known test smells, tips on making test more readable, and flaky tests.
# Test-Driven Development

So far in this book, our approach to testing has been the following: we wrote the production code first, and after that we moved on to writing the tests.
One disadvantage of this approach is that this creates a delay before we have tests, causing us to miss the "design feedback" that our tests can give us.

Test-Driven Development (TDD) proposes the opposite: to write the tests before the production code.

In this chapter, we will:
1. Introduce the reader to the TDD cycle.
1. Discuss the advantages of TDD.
1. Discuss whether developers should adhere 100% to TDD.

## The TDD cycle

The TDD cycle is illustrated in the diagram below:

![Test Driven Development Cycle](img/tdd/tdd_cycle.svg)

With a given requirement, we start by thinking of test cases. 
Often the simplest test case we can think of will take us a step further in implementing this requirement.
We try to answer the question: what is the next input we want our code to be able to handle, and what output should it give?
We then write the corresponding test.
The test will probably fail, as we have not written the production code yet.
With this failing test, we write the production code that makes the test pass.
In doing this, the aim is to write the simplest production code that makes the test pass.
Once this is achieved, it is time to refactor the code we have written. This is because, when focusing on making the test
pass, we might have ignored the quality of our production code.
We then repeat the process from the beginning. We stop once we are satisfied with our implementation and
the requirement is met.

## Advantages of TDD

TDD has several advantages:

* **By creating the test first, we also look at the requirements first.**
This makes us write the code for the specific problem that it is supposed to solve.
In turn, this prevents us from writing unnecessary code.

* **We can control our pace of writing production code.**
Once we have a failing test, our goal is clear: to make the test pass.
With the test that we create, we can control the pace we follow when writing the production code.
If we are confident about how to solve the problem, we can make a big step by creating a complicated test.
However, if we are not sure how to tackle the problem, we can break it into smaller parts and start by creating tests for these and then proceed with the other parts.

* **The tests are derived from the requirements.**
Therefore, we know that, when the tests pass, the code does what it is supposed to do.
If we write tests using code that has already been written, the tests might pass but the code might not be doing the right thing.
The tests also show us how easy it is to use the class we just made.
We are using the class directly in the tests so we know immediately when we can improve something.

* **Testable code from the beginning.**
Creating the tests first makes us think about the way to test the classes before implementing them. After all, we need testable classes from the beginning, if we start from the test code. Because TDD forces developers to change how they design their code, it improves the testability, and more specifically also the controllability of our code. We already talked about the importance of this aspect when creating automated tests. Controllable code will be easier to test, resulting in improved code quality. 

* **Quick feedback on the code that we are writing.**
Instead of writing a lot of code and then a lot of tests, i.e. getting a lot of feedback at once after a long period of time, we create a test and then write a small piece of code for that test.
It becomes easier to identify new problems as they arise, because they relate to the small amount of code that was added last.
In addition, if we want to change something, it will be easier to do on a relatively small amount of code.

* **Baby steps**: TDD encourages developers to work in small (baby) steps: first define the smallest possible functionality, then write the simplest code that makes the test green, and carry out one refactoring at a time.

* **Feedback on design**. The properties of the tests we write can indicate certain types of problems in the code.
This is why Test-Driven Development is sometimes called *Test-Driven Design*. We discussed design for testability in
a previous chapter. You might get information about the quality of your design by looking at the tests during the writing process.
For example, too many tests for just one class can indicate that the class has too many functionalities and that it should be split up into more classes.
If we need too many mocks inside of the tests, the class might be too coupled, i.e. it needs too many other classes to function.
If it is very complex to set everything up for the test, we may have to think about the pre-conditions that the class uses.
Maybe there are too many pre-conditions or maybe some are not necessary.
All of this can be observed in the tests, while doing TDD.
It is good to think about design early on as it is easier to change the design of a class at the beginning of the development, rather than a few months later.


## Should I do TDD 100% of the time?

Given all these advantages, should we use TDD 100% of time?
There is, of course, no universal answer. While some research shows the advantages of TDD, others throw more doubt on it.

Our pragmatic suggestion is:

* You should use TDD when you do not know how to design and/or architect a part of the system. TDD might help you to explore
different design decisions.

* You should use TDD when you are dealing with a complex problem, a problem in which you lack experience. If you are facing a
new challenging implementation, TDD might help you in taking a step back and learn on the way. The use of baby steps might
help you to start slowly, to learn more about the requirement, and to get up to speed once you are more familiar with the problem.

* You should not use TDD when you are familiar with the problem, or the design decisions are clear in your mind.
If there is "nothing to be learned or explored", TDD might not really afford any significant benefit. However, we note that, even if you
are not doing TDD, you should write tests in a timely manner. Do not leave it for the end of the day or the end of a sprint.
Write them together with the production code, so that the growing automated test suite will give you more and more confidence 
about the code.

## TDD in practice

James Shore created a series of 200 impressive videos where he uses TDD to build an entire "real-world" project from scratch. You can see it in his Youtube playlist, _Let's Play: Test-Driven Development_: https://www.youtube.com/playlist?list=PL0CCC6BD6AFF097B1. While he created these videos in 2012, they are still relevant and highly recommended.

{% hint style='tip' %}
We recommend readers to watch at least the first 3-5 episodes to get a sense of what TDD is about. 
{% endhint %}

## Exercises


**Exercise 1.**
We have the following skeleton for a diagram illustrating the Test Driven Development cycle.
What words/sentences should be at the numbers?

![Test Driven Development exercise skeleton](img/tdd/exercises/tdd_skeleton.svg)

(Try to answer the question without scrolling up!)





**Exercise 2.**
Remember the `RomanNumeral` problem?


> **The Roman Numeral problem**
>
> It is our goal to implement a program that receives a string 
> as a parameter containing a roman number and then 
> converts it to an integer.
>
> In roman numeral, letters represent values:
>
> * I = 1
> * V = 5
> * X = 10
> * L = 50
> * C = 100
> * D = 500
> * M = 1000
> 
> Letters can be combined to form numbers.
> For example we make 6 by using $$5 + 1 = 6$$ and 
> have the roman number "VI".
> Example: 7 is "VII", 11 is "XI" and 101 is "CI".
> Some numbers need to make use of a subtractive notation 
> to be represented.
> For example we make 40 not by "XXXX", but 
> instead we use $$50 - 10 = 40$$ and have the roman number "XL".
> Other examples: 9 is "XI", 40 is "XL", 14 is "XIV".
> 
> The letters should be ordered from the highest to the lowest value.
> The values of each individual letter is added together.
> Unless the subtractive notation is used in which a letter 
> with a lower value is placed in front of a letter with a higher value.
>
> Combining both these principles we could give our 
> method "MDCCCXLII" and it should return 1842.

Implement this program. Practise TDD!


**Exercise 3.**
Which of the following **is the least important** reason to do Test-Driven Development?

1. As a consequence of the practice of TDD, software systems get tested completely.
2. TDD practitioners use the feedback from the test code as a design hint.
3. The practice of TDD enables developers to have steady, incremental progress throughout the development of a feature.
4. The use of mock objects helps developers to understand the relationships between objects.



**Exercise 4.**
TDD has become a really popular practice among developers. According to them, TDD has several benefits. Which of the following statements 
**is not** considered a benefit which results from the practice of TDD?

*Note:* We are looking from the perspective of developers, which may not always match the results of empirical research.

1. Better team integration. Writing tests is a social activity and makes the team more aware of their code quality. 

2. Baby steps. Developers can take smaller steps whenever they feel this is necessary.

3. Refactoring. The cycle prompts developers to improve their code constantly.

4. Design for testability. Developers are "forced" to write testable code from the beginning.




## References

* Beck, K. (2003). Test-driven development: by example. Addison-Wesley Professional.

* Martin R (2006) Agile principles, patterns, and practices in C#. 1st edition. Prentice Hall, Upper Saddle River.

* Steve Freeman, Nat Pryce (2009) Growing object-oriented software, Guided by Tests. 1st edition. Addison-Wesley Professional, Boston, USA.

* Astels D (2003) Test-driven development: a practical guide. 2nd edition. Prentice Hall.

* Janzen D, Saiedian H (2005) Test-driven development concepts, taxonomy, and future direction. Computer 38(9): 43–50. doi:10.1109/MC.2005.314.

* Beck K (2001) Aim, fire. IEEE Softw 18: 87–89. doi:10.1109/52.951502.

* Feathers M (2007) The deep synergy between testability and good design. https://web.archive.org/web/20071012000838/http://michaelfeathers.typepad.com/michael_feathers_blog/2007/09/the-deep-synerg.html.
﻿
# Test doubles


While testing single units is simpler than testing entire systems, we still face challenges when unit testing classes that depend on other classes or on external infrastructure.

As an example, consider an application in which all SQL-related code has been encapsulated in an `IssuedInvoices` class. Other parts of the system that depend on this `IssuedInvoices` class, then also depend (directly or indirectly) on a database. This implicitly creates a dependency on the database when we write tests for those other parts, even though we're not immediately concerned with its behaviour. As we saw before, this can make testing more difficult. We'd really like to test this other code, *assuming that the `IssuedInvoices` works*.

In other words, we want to unit test a component A that depends on another component B, but setting up B for testing A is expensive.

This is where **test doubles** come in handy. We will create an object to mimic the behaviour of component B ("it looks like B, but it is not B"). 
Within the test, we have full control over what this "fake component B" does so we can make it behave as B would *in the context of this test* 
and cut the dependency on the real object. In our example above, suppose that A is a plain Java class that depends on an `IssuedInvoices` to retrieve 
values from a database, we can implement a fake `IssuedInvoices` that just returns a hard-coded list of values in its `findAllInvoices()` method
rather than hitting an external database. This means that, in our test, we can control the environment around A so we can check how it behaves.
We'll be showing some examples of how this works later in the chapter. 

The use of objects that simulate the behaviour of other objects has advantages:

* Firstly, we have **more control**. We can easily tell these objects what to do, without the need for complicated setups. If we want a method to throw an exception, we just tell the mock method to throw it; there is no need for complicated setups to "force" the dependency to throw the exception. Similarly, if we want a method that returns a Calendar object representing a date in 2009, we just tell the mock method to do it; there is no need to complicate your OS in order to make it go back in time. Note how hard it usually is to "force" a class to throw an exception, or to "force" a class to return a fake date. This effort is close to zero when we simulate the dependencies with mock objects. 

* Simulations are also **faster**. Imagine a dependency that communicates with a webservice or a database. A method in one of these classes might take a few seconds to process. On the other hand, if we simulate the dependency, it will no longer need to communicate with a database or with a webservice anymore and wait for a response; the simulation will simply return what it was configured to return, and it will cost nothing in terms of time.  

**Simulating dependencies is therefore a widely used technique in software testing, mainly to increase testability.**

Such objects are also known as **test doubles**. In the rest of this chapter, we will:

1. Explain the differences between dummies, fakes, stubs, spies, and mocks.
1. Learn _Mockito_, a popular mocking framework in Java. Although Mockito has "mock" in the name, Mockito can be used for stubbing and spying as well. We also show, by means of a few examples, how simulations can help developers in writing unit tests more effectively.
1. Discuss _interaction testing_ and how mocks can be used as a design technique.
1. Discuss what Google has learned about test doubles.

## Dummies, fakes, stubs, spies, and mocks

Meszaros, in his book, defines five different types: dummy objects, fake objects, stubs, spies, and mocks:

* **Dummy objects**: Objects that are passed to the class under test, but are never really used. This is common in business applications, where in many cases you need to fill a long list of parameters, but the test only exercises a few of them. Think of a unit test for a `Customer` class. Maybe this class depends on several other classes (`Address`, `Last Order`, etc). But a specific test case A wants to exercise the "last order business rule", and does not care much about which Address this Customer has. In this case, a tester would then set up a dummy Address object and pass it to the Customer class.

* **Fake objects**: Fake objects have real working implementations of the class they simulate. However, they usually do the same task in a much simpler way. Imagine a fake database class that uses an array list instead of a real database.

* **Stubs**: Stubs provide hard-coded answers to the calls that are performed during the test. 
Stubs do not actually have a working implementation, as fake objects do. In the examples above, where we used the word "simulation", we were actually talking about stub objects. Stubs do not know what to do if the test calls a method for which it was not programmed and/or set up. 

* **Spies**: As the name suggests, spies "spy" a dependency. It wraps itself around the object and observes its behaviour. Strictly speaking it does not actually simulate the object, but rather just delegates all interactions to the underlying object while recording information about these interactions. Imagine you just need to know how many times a method X is called in a dependency: that is when a spy would come in handy.

* **Mocks**: Mock objects act like stubs that are pre-configured ahead of time to know what kind of interactions should occur with them. For example, imagine a mock object that is configured as follows: method A should be called twice and, for the first call it should return "1", and for the second call it should throw an exception, while method B should never be called.

As a tester, you should decide which test double to use. We will discuss some guidelines later.

## Mockito

One of the most popular Java mocking/stubbing frameworks is Mockito ([mockito.org](https://site.mockito.org)). Mockito has a very simple API. Developers can set up stubs and/or define expectations in mock objects with just a few lines of code.

The most important methods one needs to know from Mockito are:

- `mock(<class>)`: creates a mock object/stub of a given class. The class can be retrieved from any class by `<ClassName>.class`.
- `when(<mock>.<method>).thenReturn(<value>)`: defines the behaviour when the given method is called on the mock. In this case `<value>` will be returned.
- `verify(<mock>).<method>`: asserts that the mock object was exercised in the expected way for the given method.

Mockito is an extensive framework and we will only cover part of it in this chapter. To learn more, take a look at its [documentation](https://javadoc.io/page/org.mockito/mockito-core/latest/org/mockito/Mockito.html).

## Stubbing

Let us learn how to use Mockito and setup stubs with a practical example.

> **Requirement: Low value invoices**
>
> The program must return all the issued invoices with values smaller than 100. The collection of invoices can be found in our database.

The following is a possible implementation of this requirement. Note that
`IssuedInvoices` is a type responsible for retrieving all the invoices from the database. 
It connects to the database using some global properties.

```java
import java.util.List;
import static java.util.stream.Collectors.toList;

public class InvoiceFilter {

  public List<Invoice> lowValueInvoices() {
    final var issuedInvoices = new IssuedInvoices();
    try {
      return issuedInvoices.all().stream()
              .filter(invoice -> invoice.value < 100)
              .collect(toList());
    } finally {
      issuedInvoices.close();
    }
  }

}
```
Without stubbing the `IssuedInvoices` class, the `InvoiceFilter` test would need to also handle the database (making the unit testing more expensive). Note how the tests need to open and close the connection (see the `@BeforeEach` and `@AfterEach` methods) and save invoices to the database (see the many calls to `invoices.save()` in the `filterInvoices` test).

```java
public class InvoiceFilterTest {
  private IssuedInvoices invoices;
  private DatabaseConnection dbConnection;

  @BeforeEach public void open() {
    dbConnection = new DatabaseConnection();
    invoices = new InvoiceDao(dbConnection.getConnection());

    // we need to clean up all the tables,
    // to make sure old data doesn't interfere with the test.
    dbConnection.resetDatabase();
  }

  @AfterEach public void close() {
    if (dbConnection != null) dbConnection.close();
  }

  @Test
  void filterInvoices() {
    final var mauricio = new Invoice("Mauricio", 20);
    final var steve = new Invoice("Steve", 99);
    final var arie = new Invoice("Arie", 300);

    invoices.save(mauricio);
    invoices.save(steve);
    invoices.save(arie);

    final InvoiceFilter filter = new InvoiceFilter();

    assertThat(filter.lowValueInvoices()).containsExactlyInAnyOrder(mauricio, steve);
  }

}
```

Note also the `dbConnection.resetDatabase();` that resets the database at the beginning of every test. This is a common thing to do in tests that involve databases, in order to avoid "old data" (i.e., data that was inserted by previous test cases) interfering with the current test. So far, we have never had to "clean up our mess" in test code, as all the objects we created were in-memory only and recreated between each test.

{% hint style='tip'%}
Did you notice the `assertThat...containsExactlyInAnyOrder` assertion we used? This ensures that the list contains exactly the objects we pass, and in any order.

Such assertions do not come with JUnit 5. These assertions are part of the [AssertJ](https://joel-costigliola.github.io/assertj/) project. AssertJ is a fluent assertions API for Java, giving us several interesting assertions that are especially useful when dealing with lists or complex objects. We recommend you get familiar with it!
{% endhint %}

Let us now re-write the test. This time we will stub the `IssuedInvoices` class which means that we need to find a way for the test to make the substitution. The current implementation of `InvoiceFilters` creates its instance of `IssuedInvoices` internally which means the test has no way of doing so. The most direct way to do this is to have the `IssuedInvoices` passed in as an explicit dependency, in this case through the constructor.

{% hint style='tip'%}
There are JVM frameworks that use runtime reflection to allow different implementations to be substituted when a class is instantiated, but that's missing the point here. 
That would continue to hide the dependency that already exists (on the underlying database) leaving the code diverging from the structure it represents. We can use the writing of the test to show us where we need to correct that.     
{% endhint %}


```java
import java.util.List;
import static java.util.stream.Collectors.toList;

public class InvoiceFilter {

  final IssuedInvoices issuedInvoices;

  public InvoiceFilter(IssuedInvoices issuedInvoices) {
    this.issuedInvoices = issuedInvoices;
  }
  public List<Invoice> lowValueInvoices() {
      return issuedInvoices.all().stream()
              .filter(invoice -> invoice.value < 100)
              .collect(toList());
  }

}
```

Let us now stub `IssuedInvoices`. Note that now our test does not need to do anything that is related to databases. The full control of the stub enables us to try different cases (even exceptional ones) very quickly:

```java
import static java.util.Arrays.asList;
import static org.assertj.core.api.Assertions.assertThat;
import static org.mockito.Mockito.when;

public class InvoiceFilterTest {
    private final IssuedInvoices issuedInvoices = Mockito.mock(IssuedInvoices.class);
    private final InvoiceFilter filter = new InvoiceFilter(issuedInvoices);

    @Test
    void filterInvoices() {
      final var mauricio = new Invoice("Mauricio", 20);
      final var steve = new Invoice("Steve", 99);
      final var arie = new Invoice("Arie", 300);

      when(issuedInvoices.all()).thenReturn(asList(mauricio, arie, steve));

      assertThat(filter.lowValueInvoices()).containsExactlyInAnyOrder(mauricio, steve);
    }

}
```

Note how we setup the stub, using Mockito's `when` method. In this example, we tell the stub to return a list containing `mauricio`, `arie`, and `steve` (the three invoices we instantiate as part of the test case). The test then invokes the method under test, `filter.lowValueInvoices()`. As a consequence, the method under test invokes `issuedInvoices.all()`. However, at this point, `issuedInvoices` is actually a stub that returns the list with the three invoices. The method under test continues its execution, returns a new list with only the two invoices that are below 100, causing the assertion to pass.

Note that, besides making the test easier to write, the use of stubs also made the test class more cohesive. The `InvoiceFilterTest` only tests the `InvoiceFilter` class. It does not test the usage of the `IssuedInvoices` class. Clearly, `IssuedInvoices` deserves to be tested, but in another place, and by means of an integration test.

Note that a cohesive test has less chances of failing because of something else. In the old version, the `filterInvoices` test could fail because of a bug in the `InvoiceFilter` class or because of a bug in the `IssuedInvoices` class. The new tests can now only fail because of a bug in the `InvoiceFilter` class, and never because of `IssuedInvoices`. This is handy, as a developer will spend less time debugging in case this test starts to fail.

Our new approach for testing `InvoiceFilter` is faster, easier to write, and more cohesive.


{% hint style='tip' %}
In a real software system, the business rule implemented by the `InvoiceFilter` would probably be best done directly in the database. After all, a simple SQL query would do the job in a much more performatic manner. 

Nevertheless, abstract away the example. Imagine that for a class to do its job, it needs some data from (or to interact with) another dependency.
{% endhint %}

## Mocks and expectations

Suppose our system has a new requirement:

> **Requirement: Send low valued invoices to SAP**
>
> All low valued invoices should be sent to our SAP system.
> SAP offers a /sendInvoice webservice that receives invoices.

Let us follow the idea of using test doubles to facilitate the development of our production and test code. To that aim, let us create an `SAP` interface that represents the communication with SAP:

```java
public interface SAP {
  void send(Invoice invoice);
}
```

We now need a class that will coordinate the process: retrieve the low valued invoices from the `InvoiceFilter`and pass them to the `SAP` service. Let us create a new `SAPInvoiceSender` class:

```java
public class SAPInvoiceSender {

  private final InvoiceFilter filter;
  private final SAP sap;

  public SAPInvoiceSender(InvoiceFilter filter, SAP sap) {
    this.filter = filter;
    this.sap = sap;
  }

  public void sendLowValuedInvoices() {
    filter
      .lowValueInvoices()
      .forEach(invoice -> sap.send(invoice));
  }
}
```

Let us now test the `SAPInvoiceSender` class.

Note that, for this test, we now mock the `InvoiceFilter` class. After all, for the `SAPInvoiceSender`, `InvoiceFilter` class is "just a class that returns a list of invoices". As it is not the goal of the current test to test the filter itself, we should mock this class, in order to facilitate the testing of the method under test.

After the execution of the method under test (`sendLowValuedInvoices()`), we 
should expect that the `SAP` mock received `mauricio`'s and `steve`'s invoices. For that, we use Mockito's `verify()` method:

```java
public class SAPInvoiceSenderTest {

  private static final InvoiceFilter filter = mock(InvoiceFilter.class);
  private static final SAP sap = mock(SAP.class);
  private static final SAPInvoiceSender sender = new SAPInvoiceSender(filter, sap);

  @Test
  void sendToSap() {
    final var mauricio = new Invoice("Mauricio", 20);
    final var steve = new Invoice("Steve", 99);

    when(filter.lowValueInvoices()).thenReturn(asList(mauricio, steve));

    sender.sendLowValuedInvoices();

    verify(sap).send(mauricio);
    verify(sap).send(steve);
  }
}
```

Note how we defined the expectations of the mock object. We "knew" exactly how the `InvoiceFilter` class had to interact with the mock. When the test is executed, Mockito will check whether these expectations were met, and fail the test if they were not. (If you want to test it, simply comment out the `forEach...` line in the `sendLowValuedInvoices` and see the test failing).

This example illustrates the main difference between stubbing and mocking. Stubbing means simply returning hard-coded values for a given method call. Mocking means not only defining what methods do, but also explicitly defining how the interactions with the mock should be.

Mockito actually enables us to define even more specific expectations. For example, see the expectations below:

```java
verify(sap, times(2)).send(any(Invoice.class));
verify(sap, times(1)).send(mauricio);
verify(sap, times(1)).send(steve);
```

These expectations are more restrictive than the ones we had before.
We now expect the SAP mock to have its `send` method invoked precisely two times (for any given `Invoice`). We then expect the `send` method to called once for the `mauricio` invoice and once for the `steve` invoice. We point the reader to Mockito's manual for more details on how to configure expectations.

> You might be asking yourself now: _Why did you not put this new SAP sending functionality inside of the existing `InvoiceFilter` class_?
> 
> If we were to do it, the `lowValueInvoices` method would then be both a "command" and a "query". By "query", we mean that the method returns data to the caller while with "command", we mean that the method also performs an action in the system. Mixing both concepts in a single method is not a good idea, as it may confuse developers who will eventually call this method. How would they know this method had some extra side-effect, besides just returning the list of invoices?
> 
> If you want to read more about this, search for _Command-Query Separation_, or CQS, a concept devised by Bertrand Meyer.


## How to stub static methods (or with APIs you do not control)

Imagine the following requirement:

> **Requirement: Christmas Discount**
>
> The program gives a 15% discount on the total amount of an order if the current date is Christmas day. Otherwise there is no discount.

A possible implementation for this requirement is as follows:

```java
public class ChristmasDiscount {

public double applyDiscount(double amount) {
    Calendar today = Calendar.getInstance();

    double discountPercentage = 0;
    boolean isChristmas = today.get(Calendar.DAY_OF_MONTH) == 25 &&
          today.get(Calendar.MONTH) == Calendar.DECEMBER;

    if(isChristmas)
      discountPercentage = 0.15;

    return amount - (amount * discountPercentage);
  }

}
```

The implementation is quite straightforward. And given the characteristics of the class, unit testing seems to be a perfect fit for testing it. The question is: _how can we write unit tests for it?_ To test both cases (i.e., Christmas/not Christmas), we need to be able to control/stub the `Calendar` class, so that it returns the dates we want.

We can then ask a more specific question: _how can we stub the Calendar API?_

You might have noted that the call to `Calendar.getInstance()` is a static call. Mockito does not allow us to stub static methods (although some other more magical mock frameworks do). Static calls are indeed _enemies of testability_, as they do not allow for easy stubbing.

In such cases, a pragmatic solution is to create an abstraction on top of the static call. The abstraction encapsulates the "not-so-easy-to-be-stubbed" method, and offers an "easy-to-be-stubbed" method to the rest of the program. For this particular
case, a `Clock` abstraction can do the job:

```java
import java.util.Calendar;

public interface Clock {
    Calendar now();
}
```

With this interface, we can follow the same approach as before. Let us inject `Clock` to the `ChristmasDiscount` class:

```java
import java.util.Calendar;

public class ChristmasDiscount {

  private final Clock clock;

  public ChristmasDiscount(Clock clock) {
    this.clock = clock;
  }

  public double applyDiscount(double rawAmount) {
    Calendar today = clock.now();

    double discountPercentage = 0;
    boolean isChristmas = today.get(Calendar.DAY_OF_MONTH) == 25 &&
          today.get(Calendar.MONTH) == Calendar.DECEMBER;

    if(isChristmas)
      discountPercentage = 0.15;

    return rawAmount - (rawAmount * discountPercentage);
  }
}
```

With `Clock` being an interface, we can stub it the way we want and/or need:

```java
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;
import java.util.Calendar;
import java.util.GregorianCalendar;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.mockito.Mockito.when;

public class ChristmasDiscountTest {

    private final Clock clock = Mockito.mock(Clock.class);
    private final ChristmasDiscount cd = new ChristmasDiscount(clock);

    @Test
    public void christmas() {
      Calendar christmas = new GregorianCalendar(2015, Calendar.DECEMBER, 25);
      when(clock.now()).thenReturn(christmas);

      double finalValue = cd.applyDiscount(100.0);
      assertEquals(85.0, finalValue, 0.0001);
    }

    @Test
    public void notChristmas() {
      Calendar christmas = new GregorianCalendar(2015, Calendar.JANUARY, 25);
      when(clock.now()).thenReturn(christmas);

      double finalValue = cd.applyDiscount(100.0);
      assertEquals(100.0, finalValue, 0.0001);
    }
}
```

Note again that we were able to develop the main logic of the requirement without depending on the concrete implementation of `Clock`. For completeness, let us implement `Clock`:

```java
public class DefaultClock implements Clock {
    @Override
    public Calendar now() {
      return Calendar.getInstance();
    }
}
```

Creating abstractions on top of dependencies that you do not own, as a way to gain more control, is a common technique among developers.

One might ask: _Won't that increase the overall complexity of my system? After all, it requires maintaining another abstraction._ Yes. There is more complexity when we add new abstractions in our software, and that is what we are doing here. However, the point is: does the ease in testing the system that we get from adding this abstraction compensate for the cost of the increased complexity? Often, the answer is _yes, it does pay off_.


## When to mock/stub?

Mocks and stubs are a useful tool when it comes to simplifying the process of writing unit tests. However, as expected *mocking too much* might also be problem. We do not want to mock a dependency that should not be mocked. Imagine you are testing class A, which depends on a class B. Should we mock/stub B?

Pragmatically, developers often mock/stub the following types of dependencies:

* **Dependencies that are too slow**: If the dependency is too slow, for any reason, it might be a good idea to simulate that dependency.

* **Dependencies that communicate with external infrastructure**: If the dependency talks to (external) infrastructure, it might be too complex to be set up. Consider stubbing it.

* **Hard to simulate cases**: If we want to force the dependency to behave in a hard-to-simulate way, mocks/stubs can help. A common example is when we would like the dependency to throw an exception. Forcing an exception in the real dependency might be tricky, but easy to do in a stub/mock.

On the other hand, developers tend not to mock/stub:

* **Entities**. An entity is a simple class that mirrors a collection in a database, while instances of this class mirror the entries of that collection. In Java we tend to call these POJOs (Plain Old Java Objects), as they mostly consist of fields only. In business systems, it is quite common that entities depend on other entities, e.g., a `Order` depends on `OrderItem`. When testing `Order`, developers tend not to stub `OrderItem`. The reason is that `OrderItem` is probably also a well-contained class that is easy to set up. Mocking it would take more time than the actual implementation would take. Exceptions can be made for heavy entities.

* **Native libraries and utility methods**. It is not common to mock/stub libraries that come with our programming language and utility methods. For example, why would one mock `ArrayList` or a call to `String.format`? As shown with the `Calendar` example above, any library or utility methods that harm testability can be abstracted away.

Ultimately, remember that whenever you mock, you reduce the reality of the test. It is up to you to understand this trade-off.


## Mocks as a design technique

From a developer's perspective, the use of mocks enables them to develop their software, _without caring too much about external details_. Imagine a developer working on the "low value invoices" requirement. The developer knows that the invoices will come from the database. However, while developing the main logic of the requirement (i.e., the filtering logic), the developer "does not care about the database"; they only care about the list of invoices that will come from it.

In other words, the developer only cares about the existence of a method that returns all the existing invoices. In object-oriented languages, that can be represented by means of an interface:

```java
public interface IssuedInvoices {
 List<Invoice> all();
 void save(Invoice inv);
}
```

Having such an interface, the developer can then proceed to the `InvoiceFilter` and develop it completely. After all, its implementation never depended on a database, but solely on the issued invoices. Look at it again:

```java
public class InvoiceFilter {

  final IssuedInvoices issuedInvoices;

  public InvoiceFilter(IssuedInvoices issuedInvoices) {
    this.issuedInvoices = issuedInvoices;
  }
  public List<Invoice> lowValueInvoices() {
      return issuedInvoices.all().stream()
              .filter(invoice -> invoice.value < 100)
              .collect(toList());
  }
}
```

Once the `InvoiceFilter` and all its tests are done, the developer can then focus on finally implementing the `IssuedInvoices` class and its integration tests.

Hence, once you get used to this way of developing, you will notice how your code will become easier to test. We will talk more about design for testability in future chapters.

## Mock Roles, Not Objects!

In their seminal work _Mock Roles, Not Objects_, Freeman et al. show how Mock Objects can be used for thinking about _design_ as much as for testing. By treating an object (or a cluster of objects) as a closed box, the programmer can focus on how it interacts with its environment: if the object receives a triggering event (one or more method calls), what does it need to find out (stubs) and how does it change the world around it (mocks)? This, in turn, means that the programmer needs to make explicit what else the target object depends on. Because these tests focus on how an object interacts with its environment, this style of testing is sometimes called "Interaction Testing."

This thinking of objects in terms of input and outputs was inspired by the "Roles, Responsibilities, and Collaborators" technique, best described by Wirfs-Brock and McKean. It has a history going back to the programming language Smalltalk, which used a biological metaphor of communicating cells (Ingalls).

Interaction testing is most effective when combined with TDD. While writing a test, the programmer has to define what the object needs from its environment. For a new feature, this might require a new collaborating object, which introduces a new dependency. The type of this new dependency is defined in terms of what the target object needs - its caller, not its implementation; in Java, this is usually an interface. Following this process, with regular refactoring, a programmer can grow a codebase written in the terminology of the domain. The result is a set of pluggable objects, with clear dependencies, that are combined to build a system.

The authors summarized their best practices for _interaction testing_ as:

* *Only mock types you own.* Developers should not mock objects that they do not "own", e.g., a class from an external library. Rather, they should build abstractions on top of those.

* *Be explicit about things that should not happen*. Writing tests for interactions that should not happen makes intentions clearer.

* *Specify as little as possible in the test*. Overspecified tests tend to be more brittle. Focus on the interactions that really matter.

* *Don't use mocks to test boundary objects*. Objects that have no relationships to other objects do not need to be mocked. In practice, these objects tend to "only" store data or represent atomic values.

* *Don't add behaviour*. Mocks are still stubs, and you should avoid adding any extra behavior on it.

* *Only mock your immediate neighbours*. Mocking an entire network of objects adds extra complexity to the test. This might be a symptom that the component needs a better design.

* *Avoid too many mocks*. After all, mock objects add complexity to the overall test.

The authors also discuss some common misconception when using mocks:

* *Mocks are just stubs*. Mock objects indeed act as stubs. However, their main goal (or, what makes it different from "just" stubs) is to assert the interaction of the target object with its neighbours.

* *Mocks should not be used only to the boundaries of the system*. Some developers might argue that only classes that are at the boundaries of the system should be mocked (e.g., classes that talk to external systems). Mocks might be even more helpful when used _within_ the system, as they can also drive developers to better class design.

* *Gather state during the test and assert against it afterwards*. Making assertions only at the end of the test makes failing tests less easy to be understood. Favour immediate failures.

* *Testing using Mock Objects duplicates the code.* This might mean that the code under test isn't doing very much. Perhaps it should be treated as a policy object and tested in a larger cluster of objects.



## To Mock or Not To Mock?

A very common (and heated) discussion among practitioners is about whether to use mocks or not. Up to this point, we have discussed many advantages of test doubles. Let us now discuss a possible disadvantage.

Some developers strongly affirm that the use of mocks might lead to test suites that _"test the mock, not the code"_. 

That, in fact, can happen. Suppose some class A that depends on class B. Suppose class B offers a method `sum()` that always returns positive numbers (i.e., the post-condition of `sum()`). When testing class A, the developer decides to mock B. Everything seems to work. Months later, a developer decides to change the post-conditions of B's `sum()`: now, it also returns negative numbers. In a common development workflow, a developer would apply these changes in B and update B's tests to reflect such change. It is easy for the developer to forget to check whether A handles this new post-condition well. Even worse: A's test suite will still pass! After all, A mocks B. And the mock does not really know that B changed. Now, imagine this in a large-scale software. It can be really easy to lose control of your mocks in the sense that mocks might not really represent the real contract of the class.

For mock objects to work well, developers have to design careful (and hopefully stable) contracts. Note that, from a design perspective, there is always a lesson to be learned from the situation. Mocks should have some locality. If a contract change affects an unknown range of uses, there is probably a better way to design the system such that this would not happen. Remember that mocks are also a design technique and are there to also give you feedback. 

Moreover, note that, although we use the example of a contract break as a disadvantage of mocks, remember that whatever change in contract there is, it has to be part of the job of the coder to find its dependencies, and check that the new contract is covered, irrespective of mocks or not.


## Test doubles at Google

The "Software Engineering at Google" book has an entire chapter dedicated to test doubles. In the following, we provide you with a summary:

* Using test doubles requires the system to be designed for testability. Dependency injection is the common technique to enable test doubles.
* Building test doubles that are faithful to the real implementation is challenging. Test doubles have to be as faithful as possible.
* Prefer realism over isolation. When possible, opt for the real implementation, instead of fakes, stubs, or mocks.
* Some trade-offs to consider when deciding whether to use a test double: the execution time of the real implementation, how much non-determinism we would get from using the real implementation.
* When using the real implementation is not possible or too costly, prefer fakes over mocks. An in-memory database, for example, might be better (or more real) than a mock.
* Excessive mocking can be dangerous, as tests become unclear (i.e., hard to comprehend), brittle (i.e., might break too often), and less effective (i.e., reduced fault capability detection).
* When mocking, prefer _state testing_ rather than _interaction testing_. In other words, make sure you are asserting a change of state and/or the consequence of the action under test, rather than the precise interaction that the action has with the mocked object. After all, interaction testing tends to be too coupled with the implementation of the system under test.
* Use _interaction testing_ when state testing is not possible, or when a bad interaction might have an impact in the system (e.g., calling the same method twice would make the system twice as slow).
* Avoid overspecified interaction tests. Focus on the relevant arguments and functions.
* Good _interaction testing_ requires strict guidelines when designing the system under test. Google engineers tend not to do it.


## Exercises


**Exercise 1.**
See the following class:

```java
public class OrderDeliveryBatch {

  public void runBatch() {
    OrderBook orderBook = new OrderBook();
    DeliveryStartProcess delivery = new DeliveryStartProcess();

    orderBook.paidButNotDelivered()
      .forEach(delivery::start);
  }
}

class OrderBook {
  // accesses a database
}

class DeliveryStartProcess {
  // communicates with a third-party webservice
}
```

Which of the following Mockito lines would never appear in a test for the `OrderDeliveryBatch` class?

1. `OrderBook orderBook = Mockito.mock(OrderBook.class);`
2. `Mockito.verify(delivery).start(order);` (assume `order` is an instance of `Order`)
3. `Mockito.when(orderBook.paidButNotDelivered()).thenReturn(orders);` (assume `orderBook` is an instance of `OrderBook` and `orders` is an instance of `List<Order>`)
4. `OrderDeliveryBatch batch = Mockito.mock(OrderDeliveryBatch.class);`


**Exercise 2.**
You are testing a system that triggers advanced events based on complex combinations of Boolean external conditions relating to the weather (outside temperature, amount of rain, wind, ...). 
The system has been designed cleanly and consists of a set of co-operating classes that each have a single responsibility. You create a decision table for this logic, and decide to test it using mocks. Which is a valid test strategy?

1. You use mocks to support observing the external conditions.
2. You create mock objects to represent each variant you need to test.
3. You use mocks to control the external conditions and to observe the event being triggered.
4. You use mocks to control the triggered events.


**Exercise 3.**
Below, we show the `InvoiceFilter` class. This class is responsible for returning the invoices for an amount that is smaller than 100.0. It makes use of the `IssuedInvoices` type, which is responsible for communication with the database.

```java
public class InvoiceFilter {

    private IssuedInvoices invoices;

    public InvoiceFilter(IssuedInvoices invoices) {
        this.invoices = invoices;
    }

    public List<Invoice> filter() {
      return invoices.all().stream()
              .filter(invoice -> invoice.getValue() < 100.0)
              .collect(toList());
    }
}
```

Which of the following statements about this class are **false**?

1. Integration testing is the only way to achieve 100% branch coverage.
2. Its implementation allows for dependency injection, which enables mocking.
3. It is possible to write completely isolated unit tests for it by, e.g., using mocks.
4. The `IssuedInvoices` type (a direct dependency of the `InvoiceFilter`) itself should be tested by means of integration tests.


**Exercise 4.**
Class A depends on a static method in another class B. If you want to test class A, which of the following two action(s) should you apply to do this properly?

  * Approach 1: Mock class B to control the behavior of the methods in class B.
  * Approach 2: Refactor class A, so the outcome of the method of class B is now used as a parameter.

1. Only approach 1.
2. Neither.
3. Only approach 2.
4. Both.

**Exercise 5.**
Referring to the previous chapter, apply boundary testing techniques to the `InvoiceFilter` example discussed in this chapter.

## References


* Fowler, Martin. Mocks aren't stubs. https://martinfowler.com/articles/mocksArentStubs.html

* Meszaros, G. (2007). xUnit test patterns: Refactoring test code. Pearson Education.

* Mockito's website: https://site.mockito.org

* Lee Houghton: TestDouble: Don't mock types you don't own: https://github.com/testdouble/contributing-tests/wiki/Don%27t-mock-what-you-don%27t-own. Last access on March, 2020.

* Spadini, Davide, Maurício Aniche, Magiel Bruntink, and Alberto Bacchelli. "Mock objects for testing java systems." Empirical Software Engineering 24, no. 3 (2019): 1461-1498.

* Freeman, S., & Pryce, N. (2009). Growing object-oriented software, guided by tests. Pearson Education.

* Winters, T., Manshreck, T., Wright, H. Software Engineering at Google: Lessons Learned from Programming Over Time. O'Reilly, 2020. Chapter 13, "Test doubles".

* Wirfs-Brock, R. & McKean, A., "Object Design", Addison-Wesley

* Ingalls, D. "The Design Principles behind Smalltalk", http://www.cs.virginia.edu/~evans/cs655/readings/smalltalk.html

* Freeman, S., Mackinnon, T., Pryce, N., & Walnes, J. (2004, October). Mock roles, not objects. In Companion to the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications (pp. 236-246).
# The Testing Pyramid

In previous chapters, we studied different techniques to derive test cases for requirements with single responsibilities (tested via specification-based techniques) and source code that could fit into a single unit/class (tested via structural techniques).

A large software system, however, is composed of many units and responsibilities.

In this chapter, we are going to discuss 
the different **test levels** (i.e., unit, integration, and system), their advantages and disadvantages, and the trade-offs that a tester has to make in real-world testing.

## Unit testing

In some situations, the goal of the tester is to test a single feature of the software, _purposefully ignoring the other units of the systems_ (just like we have been doing so far). When we test units in isolation, we are doing what is called **unit testing**.

Defining a 'unit' is challenging and highly dependent on the context.
A unit can be just one method or can consist of multiple classes.
Here is a definition for unit testing by Roy Osherove:

 >"A unit test is an automated piece of code that invokes a unit of work in the system.
And a unit of work can span a single method, a whole class or multiple classes working together to achieve one single logical purpose that can be verified."

As with any testing strategy, unit testing has advantages and disadvantages.

#### Advantages

* Firstly, **unit tests are fast**.
A unit test usually takes just a couple of milliseconds to execute.
Fast tests give us the ability to test huge portions of the system in a small amount of time.
Fast, automated test suites give developers constant feedback; this fast safety net makes developers feel more comfortable and confident in performing evolutionary changes to the software system they are working on.

* Secondly, **unit tests are easy to control**. 
A unit test tests the software by giving certain parameters to a method and then comparing the return value of this method to the expected result.
The input values and expected result values are easy to adapt or modify in the test.

* Finally, **unit tests are easy to write**.
Unit tests do not require complicated setup or additional work. A single unit is also
often cohesive and small, making the job of the tester easier.

#### Disadvantages

* Unit tests **lack "reality"**.
A software system is rarely composed of a single class.
The large number of classes in a system and the interaction between these classes can cause the system to behave differently in its real application than in the unit tests.
Therefore, unit tests do not perfectly represent the real execution of a software system.

* **Some types of bugs are not caught**. 
Some types of bugs cannot be caught at unit test level. They only happen in the integration of the different components (which are not exercised in a pure unit test).

## System testing

Unit tests do not exercise the system in its entirety (but this is not their goal).
To get a more realistic view of the software, and thus perform more realistic tests, 
we should run the entire software system, with all its 
databases, front-end apps, and any other components it has.

When we test the system in its entirety, we are doing what is called **"system testing"**.
In practice, instead of testing small parts of the system in isolation, system tests exercise the system as a whole.
Note that an alternative name for system testing is **black box testing** because the system is a kind of black box.
In other words, we do not care or actually know what goes on inside of the system ("the black box") as long as we get the expected output for a given input.

#### Advantages

* The obvious advantage of system testing is **how realistic the tests are**.
After all,
the more realistic the tests are, the greater the chance that the system works when shipped.

* System tests also **capture the user's perspective** better than unit tests.
In other words, system tests are a better simulation of how the final user interacts with the system.

#### Disadvantages

System testing does, however, have its downsides.

* **System tests are often slow when compared to unit tests**.
Although we have not written any system tests up until now, try to imagine what all a system test has to do, including starting and running the whole system with all its components. The test also has to interact with the real application and actions might take a few seconds to happen.
Imagine a test that starts a container with a web application and another container with a database. It then submits an HTTP request to a webservice that is exposed by this web app. This webservice
then retrieves data from the database and writes a JSON response to the test.
This obviously takes more time than running a simple unit test, which has virtually no dependencies.

* System tests are also **harder to write**.
Some of the components (e.g., databases) might require complex setup before they can be used in a testing scenario. Think of not only connection and authentication, but also making sure that the database has all the data that is required by that
test case. This takes additional code that is needed just for automating the tests.

* Lastly, **system tests tend to become flaky**.
A flaky test is a test that presents an erratic behaviour: if you run it, it might pass or it might fail for the same configuration. Flaky tests are an important problem for software development teams. After all, having a test that might pass when there is a bug or one that might fail when there are none harms the productivity of the development team.
It is easy to imagine how a system test can become flaky. Think of a system test that exercises a web app. After clicking a button, the HTTP POST request to the web app took half a second more than usual (due to small variations we often do not control in real-life scenarios; Tomcat decided to do a full garbage collection at that very second, for example). The test was not expecting it to happen and thus, it failed. If the test is executed again, the web app might now take its usual time to respond and the test will pass on this try. There are just too many uncertainties in a system test that
can lead to unexpected behaviour.

{% hint style='tip' %} We discuss flaky tests more formally later in the book. {% endhint %}

{% set video_id = "5q_ZjIM_9PQ" %}
{% include "/includes/youtube.md" %}

## Integration testing

Unit and system testing are the two extremes of test levels.
As we saw, unit tests focus on the smallest parts of the system 
while system tests focus on the whole system at once.
However, sometimes we need something in between.

**Integration testing** is the test level we use when we need something more
integrated (or less isolated) than a unit test but without the need of exercising
the entire system.

Let us look at a real-world integration test example:

> Software systems commonly rely on database systems. To communicate with the database, developers often create a class whose only responsibility is to interact with this external component (think of _Data Access Objects_ - DAO - classes). These DAOs might contain complicated SQL code. Thus, a tester feels the need to
test these SQL queries.
>
> However, note that the tester does not want to test the entire system, only the integration between the DAO class and the database. The tester also does not want to test the DAO class in complete isolation; after all, the best way to know whether a SQL query works is to actually submit it to the database and see what the database returns back. This is an example of an integration test.

The goal of integration testing is to test multiple components of a system together, focusing on the interactions between them instead of testing the system as a whole.
Are they communicating correctly? What happens if component A sends message X to component B? 
Do they still present correct behaviour?

#### Advantages

The advantage of integration tests is that, while not fully isolated, devising tests 
just for a specific integration is easier than devising tests for all the components together. 
Therefore, the effort of writing such tests
is a little more than the effort required for unit tests but less than the effort for system tests.

#### Disadvantages

Note that the more integrated our tests are, the more difficult they are to write. 
In the example, setting up a database for the test requires effort.
Tests that involve databases usually need to:
* make use of an isolated instance of the database just for testing purposes (as you 
probably do not want your tests to mess with production data),
* update the database schema (in fast companies, database schemas are changing all the time, and the test database needs to keep up),
* put the database into a state expected by the test by adding or removing rows,
* and clean everything afterwards (so that the next tests do not fail because of the data that was left behind by the previous test).

The same effort happens to any other type of integration test you can imagine (e.g., web services, file reads and writes, etc.).

{% set video_id = "MPjQXVYqadQ" %}
{% include "/includes/youtube.md" %}

## The Testing Pyramid

We discussed three different test levels: unit, system, and integration. 
A question that pragmatic software developers might ask themselves is:

_How much should I do of each?_

Testers have to decide whether to invest more in unit testing or in system testing as well as determine which components should be tested via unit testing and which components should be tested via system testing. A wrong decision might have a considerable impact on the quality of the system: a wrong level might cost too much resources and might not find sufficient bugs.

While we still have no clear empirical answer to this question, practitioners have been
proposing different ways to make this decision.

One of the most famous diagrams that help us in this discussion 
is the so-called **testing pyramid**.

![Testing pyramid, extracted from Fowler's wiki](img/testing-pyramid/testing_pyramid.svg)

The diagram indicates all the test levels we discussed, plus **manual testing**. 
Note that as you climb the levels in the diagram, the tests become more realistic. At the same time, the tests also become more complex on the higher levels.

**How much should we do of each then?** 

The common practice in industry is also represented by the diagram. The size of the pyramid slice represents the number of tests one would want to carry out at each test level. 

Unit testing is at the bottom of the pyramid and has the largest area of them all. This means that testers should favour unit testing.
The reasons for this have been discussed before: they are fast, require less effort to be written, and give developers more control.

As we climb up the levels on the diagram, we see that the next level is integration testing. The area is a bit smaller, indicating that in practice, we should do integration tests less than unit tests.
Given the extra effort that integration tests require,
testers should make sure to write tests only for the integrations they really need.

The diagram continues, showing that testers should then favour system tests less than integration tests and have even fewer manual tests.

It is clear that this diagram focuses on **costs**. Unit tests are cheaper than system tests (and manual tests), and therefore they should be preferred.

The next question is: **how do I decide whether a component should be tested at the unit- or system-level?** Practitioners have devised guidelines, which we present below, but it should be noted that this is not a universal rule as every software system is different from other systems, and might require specific guidelines.

#### When to write unit tests?

> When the component is about an algorithm or a single piece of business logic of the software system.

If we think of enterprise/business systems, most of them are about "transforming data". Such business logics is often expressed by means of entity classes (e.g., an _Invoice_ class and an _Order_ class) exchanging messages.
Business logic often does not depend on external services and so it can easily be tested and fully exercised by means of unit tests. Unit tests give testers full control in terms of the input data, as well as full observability in terms of asserting that the behaviour was as expected.

If you have a piece of code that deals with specific business logic but you are not able to test it via unit tests (e.g., it is only possible to exercise that business logic with the full system running), it is probably because of previous design or architectural decisions that prevent you from writing unit tests.
The way you design your classes has a high impact on how easy it is to write unit tests for your code. We discuss more about design for testability in a future chapter.

#### When to write integration tests?

> Whenever the component under test interacts with an external component (e.g., a database or a web service) integration tests are appropriate.

Following our example in the integration testing section, a Data Access Object class is better tested at the integration level.

Again, note that integration tests are more expensive and harder to set up than a unit test. Therefore, making sure that the component that performs the integration is _solely_ responsible for that integration and nothing else (i.e., no business rules together with integration code), will reduce the cost of the testing.

#### When to write system tests?

As we know, system tests are very costly. This makes it impossible for testers to re-test their entire system at system level. Therefore, the suggestion here is to use a risk-based approach. What are the absolutely critical parts of the software system under test? In other words, what are the parts of the system on which a bug would have a high impact? These are the ones where the tester should focus on with system tests.

Of course, such critical parts must also be tested at other levels. Remember the _pesticide paradox_: a single technique is usually not enough to identify all the bugs.

#### When to perform manual tests?

Manual testing has lots of disadvantages, but is sometimes impossible to avoid. Even in cases where automation is fully possible, manual exploratory testing can be useful. 
The Wikipedia page on [Exploratory Testing](https://en.wikipedia.org/wiki/Exploratory_testing) is well written and we point the reader to it.

On the other hand, those who apply the _testing pyramid_ try to avoid the so-called *ice-cream cone* anti-pattern. Imagine the testing pyramid upside down.
In this new version, manual testing has the largest area, which means more effort on manual testing (!!).

![Ice cream cone](img/testing-pyramid/ice_cream_cone.svg)

At this point, we do not have to explain why relying fully on manual testing is a bad thing. Unfortunately, it is common to see development teams relying mostly on manual tests in their quality assurance processes. Often, these teams also have a large number of system tests. This is not because they believe system tests are more efficient, but because the system was badly designed, so that it is impossible to carry out unit and integration tests.
We will discuss design for testability in future chapters.

{% set video_id = "YpKxAicxasU" %}
{% include "/includes/youtube.md" %}

## The community of practice and the testing pyramid

We have no scientific evidence that the testing pyramid or the idea
of prioritising design for testability and focusing on unit tests is efficient.
However, the software development community has relied on it for years, and small and larger companies have been advocating it.

That being said, in our point of view, the testing pyramid approach is viable in most enterprise / business systems. Imagine an [ERP](https://en.wikipedia.org/wiki/Enterprise_resource_planning) or a [CRM](https://en.wikipedia.org/wiki/Customer_relationship_management) system. Most business rules there can be expressed by classes/units which exchange messages and transform data. Unit tests will then deliver benefits as testers can easily gain control and observability of the actions of the system.

However, in many other systems, unit testing might not be enough. Imagine the development of a database management system itself (e.g., MySQL, Oracle, or a distributed computing system like Hadoop). While MySQL's code probably contains lots of individual components that can be unit tested, a lot happens at "low-level" - like disk I/O or socket communication. In these cases, system tests may be the ones that would reveal most of the important bugs. 

The same might happen with cyber-physical systems. Imagine a water management station.
Although a lot of software is used there, the system depends highly on physical constraints, such as the dynamics of water that affect the reading of the water level sensors. In such situations, it can be challenging or even unrealistic to write unit tests.

The message here is that although the testing pyramid makes sense in lots of systems that are developed in industry, for some others it might not be the best way of making trade-offs. You, as a tester, should understand the advantages and the disadvantages of each test level, their benefits and costs,
and then decide which test levels to use, how much, and when. There is no silver bullet.

## The testing pyramid at Google

In "Software Engineering at Google", authors mention that Google often opts for unit tests, as they tend to be cheaper to be developed, and execute faster. Similarly, to the testing pyramid, integration and system tests also happen, although to a lesser extent. According to the authors, around 80% of their tests are unit tests.

Google has also an interesting definition of "test sizes", which engineers also take into consideration when designing test cases:

* A "small test" is a test that can be executed in a single process. In other words, imagine that a tester wants to test a method in a class. This method makes no use of external components and/or threads and parallelism. This is a "small test". Their advantages are that such tests do not have access to main sources of test slowness or determinism; in other words, they are fast and not flaky.

* A "medium test" can span multiple processes, use threads, and can make external calls (like network calls) to localhost. Integration tests to databases, as we discussed before, could be classified as a medium test, if the database also runs in the same machine as the tests. Clearly, medium tests tend to be somewhat slower and more flaky than small tests.

* Finally, "large tests" remove the localhost restriction. Large tests can then require and make calls to multiple machines. Google reserves large tests for full end-to-end tests.


## Exercises

**Exercise 1.**
Here is a skeleton for the testing pyramid.
Fill in the correct corresponding terms.

![Testing Pyramid exercise skeleton](img/testing-pyramid/exercises/pyramid_skeleton.svg)

(Try to answer the question without scrolling up!)


**Exercise 2.**
As a tester, you have to decide which test level (i.e., unit, integration, or system test) you will apply.
Which of the following statements is true?

1. Integration tests, although more complicated (in terms of automation) than unit tests, would provide more help in finding bugs in the communication with the webservice and/or the communication with the database.
2. Given that unit tests could be easily written (by using mocks) and they would cover as much as integration tests would, unit tests are the best option for this problem.
3. The most effective way to find bugs in this code is through system tests. In this case, the tester should run the entire system and exercise the batch process. Given that this code can be easily mocked, system tests would also be cheap.
4. While all the test levels can be used for this problem, testers are more likely to find more bugs if they choose one level and explore all the possibilities and corner cases there.


**Exercise 3.**
The tester should now write an integration test for the `OrderDao` class below.

```java
public class OrderDeliveryBatch {

  public void runBatch() {

    OrderDao dao = new OrderDao();
    DeliveryStartProcess delivery = new DeliveryStartProcess();

    List<Order> orders = dao.paidButNotDelivered();

    for(Order order : orders) {
      delivery.start(order);

      if(order.isInternational()) {
        order.setDeliveryDate("5 days from now");
      } else {
        order.setDeliveryDate("2 days from now");
      }
    }
  }
}

class OrderDao {
  // accesses a database
}

class DeliveryStartProcess {
  // communicates with a third-party webservice
}
```

Which one of the following statements **is not required** when writing
an integration test for this class?

1. Reset the database state before each test.
2. Apply the correct schema to the database under test.
3. Assert that all database constraints are met.
4. Set the transaction auto-commit to true.




**Exercise 4.**
A newly developed product started off with some basic unit tests but later on decided to add only integration and system tests for the new code that was written. This was because a user interacts with the system as a whole and therefore these types of tests were considered more valuable. Therefore unit tests became less prevalent, while integration and system tests became a more crucial part of the test suite. Which of the following describes this transition?

1. Transitioning from a testing pyramid to an ice-cream cone pattern
2. Transitioning from an ice-cream cone anti-pattern to a testing pyramid
3. Transitioning form an ice-cream cone pattern to a testing pyramid
4. Transitioning from a testing pyramid to an ice-cream cone anti-pattern


**Exercise 5.**
TU Delft just built in-house software to handle the payroll of its employees. The application makes use of Java web technologies and stores data in a Postgres database. The application frequently retrieves, modifies, and inserts large amounts of data into the database. All this communication is made by Java classes that send (complex) SQL queries to the database. 

As testers, we know that a bug can be anywhere, including in the SQL queries themselves. We also know that there are many ways to exercise our system. Which one of the following **is not** a good option to detect bugs in the SQL queries?
  
1. Unit testing
2. Integration testing
3. System testing
4. Stress testing


**Exercise 6.**
Choosing the level of a test involves a trade-off. After all, each 
test level has advantages and disadvantages.
Which one of the following is the **main advantage** of a test at system level?


1. The interaction with the system is much closer to reality.
2. In a continuous integration environment, system tests provide real feedback to developers.
3. Given that system tests are never flaky, they provide developers with more stable feedback.
4. A system test is written by product owners, making it closer to reality.



**Exercise 7.**
What is the main reason for the number of recommended system tests in the testing pyramid to be smaller than the number of unit tests?


1. Unit tests are as good as system tests.
2. System tests do not provide developers with enough quality feedback.
3. There are no good tools for system tests.
4. System tests tend to be slow and are difficult to make deterministic.



**Exercise 8.**
How would you test code that make use of databases and SQL queries?


## References

* Chapter 2 of the Foundations of software testing: ISTQB certification. Graham, Dorothy, Erik Van Veenendaal, and Isabel Evans, Cengage Learning EMEA, 2008.

* Vocke, Ham. The Practical Test Pyramid (2018), https://martinfowler.com/articles/practical-test-pyramid.html.

* Fowler, Martin. TestingPyramid (2012). https://martinfowler.com/bliki/TestPyramid.html

* Wikipedia. Exploratory testing. https://en.wikipedia.org/wiki/Exploratory_testing. Last access on March, 2020.

* Winters, T., Manshreck, T., Wright, H. Software Engineering at Google: Lessons Learned from Programming Over Time. O'Reilly, 2020. Chapters 11 and 12.

# Web testing

Web applications are ubiquitous nowadays: whether you check your e-mail, file your tax return or play a simple online game, chances are high that you are using a web application.
They are all client-server applications, where the server is located somewhere on the Internet, and the client runs in your web browser. 
Testing such web applications is called *web testing*.

<!-- FM: the examples above may not work very well for today's students. They are all used to smartphone apps. Should we have a separate chapter on app testing? ... -->

A simplified architecture diagram of a web application may look as follows:

![Client-server communication in a web application](img/web-testing/webapp.svg)

What makes web testing different from what we have seen so far? 
How can you apply the principles you have learned so far to testing web applications? 
In this chapter, we will answer these questions using concrete examples.

## Characteristics of web applications
We will now take a look at some characteristics of web applications and how they influence our testing process. These consequences for testing are then discussed in more detail after this section.

### The front end is usually written in JavaScript
JavaScript is still the de facto programming language for applications that run in a browser.
(This may change in the future now that WebAssembly is supported by all major browsers, but WebAssembly has not gained much traction yet.)
This means that you either need to write the front end in JavaScript, or use a different language like TypeScript and "transpile" it into JavaScript before having your code executed by a browser.
(In fact, even JavaScript code is often transpiled from a newer language version to an older one, so that you can use JavaScript features that are not yet supported in all browsers.)
Even though you will be able to apply many of the testing principles you have probably applied in a Java context, you will now also have to get acquainted with **JavaScript unit testing frameworks**.

The **programming paradigms** may be different, so principles like "inversion of control" may be implemented differently in a JavaScript context.

Also, you will have to pay special attention to the structure of your code.
It is very easy to mix JavaScript with HTML in such a way that you end up with something that is difficult to test.
Make sure you **design for testability** and apply some kind of modular design, creating small, independent components that are easily tested.
When starting a new project from scratch, it is helpful to use a JavaScript library or framework (like Vue.js, React or Angular) in order to achieve such a structure.
If you are dealing with existing code for which there are no unit tests, you will probably have to do a significant amount of refactoring to make the code testable. 

### The application follows the client-server model
Having to separate your application into a client side (front end) and a server side (back end) which communicate with each other over HTTP may be beneficial, because it forces you to think of some kind of interface and can help to reduce coupling.
Also, the fact that the server side can be written in any programming language you like, means that you can stick to your familiar testing ecosystem there.
At the same time, it poses challenges, like possibly having **different programming languages** and corresponding ecosystems on the client and server side.
(Of course, you could write the server side in Node.js if you wanted to use JavaScript on both sides.)
You also have to realise that just testing the front and back end separately will probably not cut it.
You will want to reflect how a user uses the application by performing **end-to-end tests**, which in turn means that you have to have a web server running while executing such a test.
That server needs to be in the right state (especially if it uses a database), and the versions of front and back end need to be compatible.

### Everyone can access your application
Web applications are usually available to everyone who is connected to the Internet.
First of all, this means that your audience will probably be very diverse: the users will have very different backgrounds. This makes **usability testing** and **accessibility testing** testing a priority.
Secondly, the number of users of your application may become very high at any given point in time, so **load testing** is a wise thing to do.
Finally, some of those users may have malicious intent.
Therefore **security testing** is of utmost importance (and is discussed in the separate Security Testing chapter).

### The front end runs in a browser
Different users use different versions of different browsers.
**Cross-browser testing** helps to ensure that your application will work in the browsers you support.

In addition, browsers show web pages differently based on the window size.
Web designers use something called **responsive web design** to make sure the application will look good in browsers with different sizes, running on different devices.
This is of course also something you should test.

Another factor to consider is the fact that HTML is used as the markup language for web pages.
Even your HTML should be **designed for testability**, so that you can select elements, and so that you can test different parts of the user interface (UI) independently. 
Speaking of user interfaces: **UI component testing** can be considered  a special case of unit testing: here your "unit" is one part of the Document Object Model (DOM), which you feed with certain inputs, possibly triggering some events (like "click"), after which you check whether the new DOM state is equal to what you expected. 
Even more specifically, **snapshot testing** can help you to make sure your UI does not change unexpectedly.

Finally, when you want to perform **end-to-end tests** automatically, you will have to somehow control the browser and make it simulate user interaction. 
Two well-known tools for this are Selenium WebDriver and Cypress.

### Many web applications are asynchronous
Especially in a single-page application (SPA), most of the requests to the server are done in an **asynchronous** manner: while the browser is waiting for results from the server, the user can continue to use the application (i.e., they do not have to wait for the results). 
When writing unit tests, you have to account for this by "awaiting" the results from the (mocked) server call before you can check whether the output of your unit matches what you expected. 
When performing end-to-end tests, you may have to wait for a certain element to appear on the screen, or even to wait for a certain amount of time (like 1 second) before checking the output. 
This can easily lead to *flaky tests* (as discussed in the chapter on Test Code Quality). 
You either have to write custom code to make your tests more robust, or use a tool like Cypress, which has retry-and-timeout logic built-in.



## JavaScript unit testing (without a framework)
We will now tackle several of the aforementioned challenges that relate to the front end of a web application by showing you how to design your JavaScript and HTML for testability, how to write unit tests for JavaScript functions, and how to unit test UI components. We will demonstrate this using both plain JavaScript and a more modern example that uses a JavaScript framework.

### Design for testability
Let us look at an example of a very simple web application (without a back end) that looks like this: 

![Date incrementing application screenshot](img/web-testing/dateIncrementer.png)

The application does the following:
* When the page is loaded, the current date is shown.
* Every time you click the button, the date is incremented.

We start with the following (low-quality) implementation (`dateIncrementer1.html`):

```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Date incrementer - version 1</title>
</head>

<body>
    <p>Date will appear here.</p>
    <button onclick="incrementDate(this)">+1</button>

    <script>
        function incrementDate(sender) {
            var p = sender.parentNode.children[0];
            var date = new Date(p.innerText);
            date.setDate(date.getDate() + 1);
            p.innerText = date.toISOString().slice(0, 10);
        }

        window.onload = function () {
            var p = document.getElementsByTagName("p")[0];
            p.innerText = new Date().toISOString().slice(0, 10);
        }
    </script>
</body>

</html>
```

Take a minute to think of reasons why it is difficult or impossible to write unit tests for this piece of code.

First of all, the fact that the JavaScript code is inline with the rest of the page means that it is virtually impossible to write unit tests for it. You cannot run the code without also running the rest of the page, so you cannot test the functions separately, as a unit. This is a major issue and should be solved by moving the JavaScript code to one or more separate files.

The `incrementDate()` function is a mix of date logic and user interface (UI) code. 
These parts cannot be tested separately. 
This is exacerbated by the fact that keeping track of the currently shown date is done by updating and reading from the DOM. 
We have even become dependent on the implementation of the date conversion functions (which leads to problems if we decide to use a different date format, for instance). 
The conversion from string that is done should not even be necessary. 
We should solve these problems by splitting up the `incrementDate()` function into different functions and storing the currently shown date in a variable. 

Another problem is that the initial date value is hard-coded: the code always uses the current date (by calling `new Date()`), so you cannot test what happens with cases like "February 29th, 2020".

The `<p>` element was difficult to locate using the standard JavaScript query selectors, and we resorted to abusing the DOM structure to find it. 
We thereby unnecessarily imposed restrictions on the DOM structure (the `<p>` element must now be on the same level as the button, and it must be the first element). 
For your UI tests, you should make sure that you can reliably select the elements you use.
In our simple JavaScript example, we can achieve this by adding an `id` to the element and then selecting it by using `getElementById()`.
In general, it is better to find elements in the same way that users find them (for example, by using labels in a form).


The aforementioned issues are solved by refactoring the code and splitting it up into three files.
The first one (`dateUtils.js`) contains the utility functions for working with dates, which can now nicely be tested as separate units:

```js
// Advances the given date by one day.
function incrementDate(date) {
    date.setDate(date.getDate() + 1);
}

// Returns a string representation of the given date
// in the format yyyy-MM-dd.
function dateToString(date) {
    return date.toISOString().slice(0, 10);
}
```

The second one (`dateIncrementer.js`) contains the code for keeping track of the currently shown date and the UI interaction:

```js
function DateIncrementer(initialDate, dateElement) {
    this.date = new Date(initialDate.getTime());
    this.dateElement = dateElement;
}

DateIncrementer.prototype.increment = function () {
    incrementDate(this.date);
    this.updateView();
};

DateIncrementer.prototype.updateView = function () {
    this.dateElement.innerText = dateToString(this.date);
};
```

The third one is the refactored HTML file (`dateIncrementer2.html`) that uses our newly created JavaScript files:

```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Date incrementer - version 2</title>
</head>

<body>
    <p id="pDate">Date will appear here.</p>
    <button id="btnIncrement">+1</button>

    <script src="dateUtils.js"></script>
    <script src="dateIncrementer.js"></script>
    <script>
        window.onload = function () {
            var incrementer = new DateIncrementer(
                new Date(), document.getElementById("pDate"));

            var btn = document.getElementById("btnIncrement");
            btn.onclick = function () { incrementer.increment(); };

            incrementer.updateView();
        }
    </script>
</body>

</html>
```

The date handling logic can now be tested separately, as well as the UI code. The initial date can now be supplied as an argument. 
The `<p>` element can now be found by its ID.
We should now be ready to write some tests!

{% hint style='tip' %}
Some side notes about the code:
* Even though the code has improved, part of it is still not unit-testable (namely the `onload` handler). This can be solved by factoring out that part to a separate file as well.
* You may want to structure the code even better, for instance by writing a proper MVC (Model-View-Controller) implementation. Especially for larger-scale projects with lots of JavaScript, it makes sense to use a library or framework that provides such a structure for you.
* We have not used any modern JavaScript features (like the `class` syntax) in order to maintain compatibility with older browsers. We will show an example that uses newer JavaScript features later in the chapter.
* As with the other examples in the book, the code can be found in the [code examples](https://github.com/sttp-book/code-examples/) repository.

{% endhint %}


### Writing the tests
At this point, you should apply the testing principles you have learned so far in this book to come up with a good set of tests for the implementation above. 
In this section, we will just show a small number of tests to get you started.

Before we discuss JavaScript unit testing frameworks, let us see how far we can get with a manual implementation. This should give us some insight into how those frameworks work and allow us to think about JavaScript testing without being biased by a particular framework.

We first write some tests for the functions in `dateUtils.js` by creating an HTML file, calling the functions and doing some light-weight assertions:

```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Date utils - Test</title>
</head>

<body>
    <p>View the console output for test results.</p>

    <script src="dateUtils.js"></script>
    <script>
        
        function assertEqual(expected, actual) {
            if (expected != actual) {
                console.log("Expected " + expected + " but was " + actual);
            }
        }

        /*
        Be aware that in JavaScript Date objects, months are zero-based,
        meaning that month 0 is January, month 1 is February, etc.
        So "new Date(2020, 1, 29)" represents February 29th, 2020.
        */

        // Test 1: incrementDate should add 1 day a given date object
        var date1 = new Date(2020, 1, 29);  // February 29th, 2020
        incrementDate(date1);
        // This succeeds:
        assertEqual(new Date(2020, 2, 1).getTime(), date1.getTime());

        // Test 2: dateToString should return the date in the form "yyyy-MM-dd"
        var date2 = new Date(2020, 4, 1);   // May 1st, 2020
        // This fails because of time zone issues 
        // (the actual value is "2020-04-30"):
        assertEqual("2020-05-01", dateToString(date2));
    </script>
</body>

</html>
```

Even without using any frameworks, we could write some tests and actually found a bug in the code.
On the console, the following message is logged: "Expected 2020-05-01 but was 2020-04-30". 
This is because the implementation of `dateToString` returns the date in UTC, whereas the date was created in the local time zone of the user with time 0:00:00. 
So a user who is in time zone UTC+2 and opens the application just after midnight on May 1st, will see April 30th instead.
Now the requirements were not explicit on what was meant by the "current date", but this is probably not what was intended so it can be considered a bug.
For reference, here is an alternative implementation that does not have the time zone issues (as long as the given date has been created in the local time zone of the user); you should of course write tests to verify this:

```js
function dateToString(date) {
    year = date.getFullYear();
    month = ('0' + (date.getMonth() + 1)).slice(-2);
    day = ('0' + date.getDate()).slice(-2);
    return year + "-" + month + "-" + day;
}
```

Next, we take a look at writing UI tests for the `DateIncrementer` class in `dateIncrementer.js`. We create another HTML file (`dateIncrementerTest.html`), manually mock the external functions and check whether `increment()` works as expected:

```html
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <title>Date incrementer - Test</title>
</head>

<body>
    <p>View the console output for test results.</p>
    <p id="pDate" style="display: none">Dummy element for UI test.</p>

    <script src="dateIncrementer.js"></script>
    <script>
        function assertEqual(expected, actual) {
            if (expected != actual) {
                console.log("Expected " + expected + " but was " + actual);
            }
        }

        // Mocks
        var incrementDateCalls = 0;
        var dateToStringCalls = 0;

        function incrementDate(date) {
            incrementDateCalls++;
            date.setDate(1);
        }

        function dateToString(date) {
            dateToStringCalls++;
            return "mock";
        }

        // Test 3: DateIncrementer
        var date3 = new Date(2020, 2, 14);  // March 14th, 2020
        var p = document.getElementById("pDate");

        var dateIncrementer = new DateIncrementer(date3, p);
        dateIncrementer.increment();

        // 3.1: increment() should call incrementDate()
        assertEqual(1, incrementDateCalls);

        // 3.2: increment() should not mutate the original date
        assertEqual(14, date3.getDate());
        assertEqual(2, date3.getMonth());
        assertEqual(2020, date3.getFullYear());

        // 3.3: increment() should use dateToString and update the view
        assertEqual(1, dateToStringCalls);
        assertEqual("mock", p.innerText);
    </script>
</body>

</html>
```

We successfully mocked the external dependencies and were able to check whether the UI was updated properly. 
This time, nothing is logged to the console because all tests succeed.

{% hint style='tip' %}
Even though we showed that it is possible to mock the date utility functions, you can also decide to use their actual implementations. 
(This trade-off was explained in the *Test doubles* chapter.) 
In the example above, you would have to add the line `<script src="dateUtils.js"></script>` (and remove the mock implementations) in order to use the actual implementations.
{% endhint %}

Of course, the manual approach we followed here has obvious limitations.  We had to write the `assertEqual` function ourselves, and we would need to write similar methods for other kinds of assertions. 
Our `assertEqual` implementation only logs to the console and does not provide any context when an assertion has failed (like the line number of the call to `assertEqual`). 
The state of the UI is not reset after the UI tests; this becomes a problem when we create more tests than the single one we created.

You could solve all these issues by hand, but to avoid reinventing the wheel, it is wise to use an existing unit testing framework.

### Choosing a unit testing framework
In the JavaScript world, there is an abundance of tools and frameworks to choose from. The same holds for unit testing frameworks: there are many options, and contrary to Java there is certainly no de facto standard framework. Moreover, new frameworks are developed and adopted every year.

When working on an existing application that already has a testing framework in place, it probably makes sense to stick to that. 
For new projects, it is useful to get yourself up-to-date with the currently popular JavaScript testing frameworks. 
Articles like [An Overview of JavaScript Testing in 2020](https://medium.com/welldone-software/an-overview-of-javascript-testing-7ce7298b9870) give you a nice summary of the current state of affairs.

Choosing a framework that is used by many people means that you are more likely to get support in their online community. 
You should also consider using the testing framework that is recommended by the JavaScript library/framework you use. 
For instance, for React applications the [recommended](https://reactjs.org/docs/testing.html#tools) testing framework is Jest. 
Another factor to consider is continuity. 
Is it just a one man project, or is it maintained by a large organisation? When was the latest version published?

There is no single right answer, and we are not trying to give you one here. 
Choose a framework that seems most suited to your project, with the above considerations in mind.

Just to give you an example of what the tests may look like when using such a framework, the next section demonstrates an example implementation in React with tests in Jest.


## JavaScript unit testing (with React and Jest)
In this example, we will rebuild the "date incrementer" application from the previous section in React.
 We create a new React application using the [Create React App](https://create-react-app.dev/) tool. This creates a project structure with the necessary dependencies and includes the unit testing framework Jest. 

### Implementation
We can mostly reuse the utility functions that we built earlier (`dateUtils.js`):

```js
// Advances the given date by one day.
function incrementDate(date) {
  date.setDate(date.getDate() + 1);
}

// Returns a new date that is one day later than the given date.
export function addOneDay(date) {
  const copy = new Date(date.getTime());
  incrementDate(copy);
  return copy;
}

// Returns a string representation of the given date
// in the format yyyy-MM-dd.
export function dateToString(date) {
  const year = date.getFullYear();
  const month = ('0' + (date.getMonth() + 1)).slice(-2);
  const day = ('0' + date.getDate()).slice(-2);
  return year + "-" + month + "-" + day;
}
```

Compared to what you saw before, we have made the following changes: 

1. We now have the JavaScript module feature at our disposal, so we wrote a `dateUtils` module and exported the functions we use elsewhere.
2. We can now use modern JavaScript features like the `const` keyword (which creates a variable that cannot be changed through reassignment). The code gets transpiled to an older version of JavaScript (using `var` instead of `const`) which is understood by most browsers. (In our plain JavaScript example, we had to stay away from modern features in order to maintain browser compatibility.)
3. As we will see later on in the example, React expects us to [treat state as immutable](https://reactjs.org/docs/react-component.html). Therefore, we should not directly change the `date` with the `incrementDate()` function. We created a new function, `addOneDay()`, which circumvents the issue by creating a new `Date` instance and updating that one.

We use those utility functions in the UI component, `DateIncrementer.js`:

```js
import React, { useState } from 'react';
import { addOneDay, dateToString } from './dateUtils.js';

function DateIncrementer({ initialDate }) {
  const [date, setDate] = useState(initialDate);

  return (
    <div>
      <p>{dateToString(date)}</p>
      <button onClick={() => setDate(addOneDay)}>
        +1
      </button>
    </div>
  );
}

export default DateIncrementer;
```

The component is implemented as a function, with "props" (properties) as input, and a DOM representation as output. 
Our component has one input, `initialDate`, which is used to set the date that is shown when the component is first loaded. 
The component keeps one state variable, `date`, which stores the date as it is updated by the user using the button.

The XML-like syntax you see there is not HTML, but rather a JavaScript extension called JSX. This syntax gets converted to `React.createElement()` calls, of which the result will eventually get rendered to the DOM. 
We defined the `<p>` and `<button>` elements like before, but can now include JavaScript within curly braces to provide the expressions for the date string and the `onClick` handler.

Finally, we alter the file `App.js` that was generated by `create-react-app` to use our `DateIncrementer` component:

```js
import React, { useState } from 'react';
import DateIncrementer from './DateIncrementer.js';

function App() {
  const [today] = useState(new Date());

  return (
    <main>
      <DateIncrementer initialDate={today} />
    </main>
  );
}

export default App;
```

We now have an application that behaves the same as the one we built in the plain JavaScript example. 
We will now turn our attention to writing tests for this.


### Tests for the utility functions
For the utility functions, we can again write similar code to what we did with our ad-hoc unit tests:

```js
import { addOneDay, dateToString } from './dateUtils';

describe('addOneDay', () => {
  test('handles February 29th', () => {
    const oldDate = new Date(2020, 1, 29);  // February 29th, 2020
    const newDate = addOneDay(oldDate);
    expect(newDate).toEqual(new Date(2020, 2, 1));
  });
});

describe('dateToString', () => {
  test('returns the date in the form "yyyy-MM-dd"', () => {
    var date = new Date(2020, 4, 1);   // May 1st, 2020
    expect(dateToString(date)).toEqual("2020-05-01");
  });
});
```

The differences are:
1. We can now take advantage of the module system. We do not have to load the functions into global scope any more, but just import the functions we need.
2. We use the Jest syntax, where `describe` is optionally used to group several tests together, and `test` is used to write a test case, with a string describing the expected behaviour and a function executing the actual test. The body of the test function uses the "fluent" syntax `expect(...).toEqual(...)`.


### Tests for the user interface
For the `DateIncrementer` component, we could write tests like this:

```js
import React from 'react';
import { render, fireEvent } from '@testing-library/react';
import DateIncrementer from './DateIncrementer';

test('renders initial date', () => {
  const { getByText } = render(<DateIncrementer initialDate={new Date(2020, 0, 1)} />);
  const dateElement = getByText("2020-01-01");
  expect(dateElement).toBeInTheDocument();
});

test('updates correctly when clicking the "+1" button', () => {
  const date = new Date(2020, 0, 1);
  const { getByText } = render(<DateIncrementer initialDate={date} />);
  const button = getByText("+1");

  fireEvent.click(button);

  const dateElement = getByText("2020-01-02");
  expect(dateElement).toBeInTheDocument();
});
```

Here you see a similar Jest unit test structure, but we also use `react-testing-library` to render the UI component to a virtual DOM. 
In `react-testing-library`, you are encouraged to test components like a user would test them. 
This is why we use functions like `getByText` to look up elements. 
This also means that we did not have to include any `id`s or other ways of identifying the `<p>` and the `<button>` in the component.

You could also decide to mock the utility functions, like we did in the plain JavaScript example. 
In that case, the test would look like this:

```js
jest.mock('./dateUtils');

import React from 'react';
import { render, fireEvent } from '@testing-library/react';
import { dateToString, addOneDay } from './dateUtils';
import DateIncrementer from './DateIncrementer';

test('renders initial date', () => {
  dateToString.mockReturnValue("mockDateString");

  const date = new Date(2020, 0, 1);
  const { getByText } = render(<DateIncrementer initialDate={date} />);
  const dateElement = getByText("mockDateString");

  expect(dateToString).toHaveBeenCalledWith(date);
  expect(dateElement).toBeInTheDocument();
});

test('updates correctly when clicking the "+1" button', () => {
  const mockDate = new Date(2021, 6, 7);
  addOneDay.mockReturnValueOnce(mockDate);

  const date = new Date(2020, 0, 1);
  const { getByText } = render(<DateIncrementer initialDate={date} />);
  const button = getByText("+1");

  fireEvent.click(button);

  expect(addOneDay).toHaveBeenCalledWith(date);
  expect(dateToString).toHaveBeenCalledWith(mockDate);
});
```

Here, `jest.mock('./dateUtils')` replaces every function that is exported from the `dateUtils` module by a mocked version. 
You can then provide alternative implementations with functions like `mockReturnValue`, and check whether the functions have been called with functions like `expect(...).toHaveBeenCalledWith(...)`.

The version with mocks is less 'real' than the one without. 
The one without mocks is arguably preferable. 
However, you could use the same mechanism for things like HTTP requests to a back end, and in that case mocking would certainly be helpful.

### Snapshot testing
To understand snapshot testing, let us first return to one of the UI tests we wrote above:

```js
test('renders initial date', () => {
  const { getByText } = render(<DateIncrementer initialDate={new Date(2020, 0, 1)} />);
  const dateElement = getByText("2020-01-01");
  expect(dateElement).toBeInTheDocument();
});
```

We gave our component some input and checked whether the rendered component contained what we expected. 
What if the rendered output contained more elements?
Would you just look up all the elements and assert things about them?
This can get quite tedious and time-consuming after a while.

If all you want to do is make sure that your UI does not change unexpectedly, snapshot tests are a good fit.
The first time you run a snapshot test, it takes a snapshot of the component as it is rendered in that initial run.
That first time, the test will always pass.
Then in all subsequent runs, the rendered output is compared to the snapshot.
If the output is different, the test fails, and you are presented with the differences between the two versions.
The test runner allows you to inspect the differences and decide whether the changes are what you intended.
You can then either change the component so that its output corresponds to the snapshot,
or you press a button to update the snapshot and mark the newly rendered output as the correct one.

In Jest, such a test can look like this:

```js
test('renders correctly via snapshot', () => {
  const { container } = render(<DateIncrementer initialDate={new Date(2020, 0, 1)} />);
  expect(container).toMatchSnapshot();
});
```

On the first run, this creates a file called `DateIncrementer.test.js.snap` with the following contents:

```
// Jest Snapshot v1, https://goo.gl/fbAQLP

exports[`renders correctly via snapshot 1`] = `
<div>
  <div>
    <p>
      2020-01-01
    </p>
    <button>
      +1
    </button>
  </div>
</div>
`;
```

On subsequent runs, the output of the test is compared to the corresponding snapshot.

The created snapshot file should be committed to your version control system (like Git), 
so that your tests can also run on your Continuous Integration (CI) system.

## End-to-end testing
The goal of end-to-end testing is to test the flow through the application as a user might follow it, while integrating the various components of the web application (such as the front end, back end and database). For instance, when testing an e-commerce application, you could test the flow of a user searching for a product, adding the product to their cart, going to checkout, logging in, paying for their products and receiving a confirmation. You should make this as realistic as possible, so you use an actual browser and perform the tests on a production-like version of the application components; although of course you will want to simulate the payments to prevent you from having to pay for your own products every time you run the test... Similarly, you should be careful with sending out actual e-mails and can decide to send the e-mails to a drop folder instead of to an actual recipient.

While performing these tests manually is possible and sometimes necessary, this can be automated too. A well-known tool for this is [Selenium WebDriver](https://www.selenium.dev/). It basically acts as a "remote control" for your browser, so you can instruct it to "open this page, click this button, wait for that element to appear", etc. You write these tests in one of the supported languages (such as Java) with your favourite unit testing framework. You can then also run these same tests on different browsers and in this way perform *cross-browser tests*.

The WebDriver API is now a W3C standard, and several implementations of it (other than Selenium) exist, such as [WebDriverIO](https://webdriver.io/).

Another, more recent tool, which does not use WebDriver, is [Cypress](https://www.cypress.io/). It integrates more closely with the browser and eliminates some issues that you encounter when you use WebDriver. For instance, when you perform an asynchronous action (like fetching all matching products from a web service, after clicking a Search button) you have to wait for the results to appear. It is sometimes difficult to program this reliably and testers often resort to insert statements like "wait one second" hoping that that is enough for the web service to respond. Cypress solves this by automatically retrying to find an element until it appears, so you do not have to write that code. It also has many more features which go beyond the scope of this book.

As discussed in the chapter on model-based testing, it is common to create an abstraction layer on top of the web application. In the context of web applications, the abstractions are called **Page Objects**.

### Page Objects
An example of a page object is shown in the diagram, made by Martin Fowler, below:

![Page Objects diagram by Martin Fowler](img/web-testing/page_objects.png)

At the bottom, you can see a certain web page that we want to test.
The tool for communicating through the browser (such as WebDriver), gives an API to access the HTML elements.
Additionally, the tool supports clicking on elements, for example on a certain button.

If we use this API directly in the tests, the tests become unreadable very quickly.
So, we create a page object with just the methods that we need in the tests.
These methods correspond to the application, rather than the HTML elements.
The page objects implement these methods by using the API provided by the tool.

Then, the tests use these methods instead of the ones about the HTML elements.
Because we are using methods that correspond to the application itself, they will be more readable than tests without the page objects.


### State Objects

Page objects give us an abstraction for single pages or even fragments of pages.
This is already better than using the API for the HTML elements in the test, but we can take it a bit further.
We can make the page objects correspond to the states in the navigational state machine.
A navigational state machine is a state machine that describes the flow through a web application.
Each page will be represented as a state.
The events of the transitions between these states show how the user can go from one to another page.

With this approach, the page objects each correspond to one of the states of the state machine.
Now we do not call them page objects anymore, but **state objects**.
In these state objects we have the inspection and trigger methods.
Additionally, we have methods that can help with state **self-checking**.
These methods verify whether the state itself is working correctly, for example by checking if certain buttons can be clicked on the web page.
Now the tests can be expressed in the application's context, using these three types of methods.

#### Behaviour-Driven Design

The state objects are mostly used for end-to-end testing in web development.
Another technique useful in end-to-end testing is behaviour-driven design.

In **behaviour-driven design** the system is designed with scenarios in mind.
These scenarios are written in natural language and describe the system's behaviour in a certain situation.

For these scenarios to be used by tools, an example of a tool for scenarios is [cucumber](https://cucumber.io), we need to follow a certain format.
This is a standard format for scenarios, as it provides a very clear structure.
A scenario consists of the following:

- Title of the scenario
- Given ...: Certain conditions that need to hold at the start of the scenario.
- When ...: The action taken.
- Then ...: The result at the end of the scenario.



Let's look at a scenario for an ATM.
If we have a balance of $100, a valid card and enough money in the machine, we can give a certain amount of money requested by the user.
Together with the money, the card should be given back, and the balance of the account should be decreased.
This can be turned into scenario 1 below:

```text
Story: Account Holder withdraws cash

As an Account Holder
I want to withdraw cash from an ATM
So that I can get money when the bank is closed

Scenario 1: Account has sufficient funds
Given the account balance is $100
 And the card is valid
 And the machine contains enough money
When the Account Holder requests $20
Then the ATM should dispense $20
 And the account balance should be $80
 And the card should be returned
```

The small introduction above the scenario itself is part of the user story.
A user story usually consists of multiple scenarios with respect to the user introduced.
This user is the account holder in this example.


With the general `Given`, `When`, `Then` structure we can describe a state transition as a scenario.
In general the scenario for a state transition looks like this:

```text
Given I have arrived in some state
When  I trigger a particular event
Then  the application conducts an action
 And  the application moves to some other state.
```

Each scenario will be able to cover only one transition.
To get an overview of the system as a whole we will still have to draw the entire state machine.

{% set video_id = "NMGX7TEMXdE" %}
{% include "/includes/youtube.md" %}


{% set video_id = "gijO3mlcMCg" %}
{% include "/includes/youtube.md" %}

## Other types of tests
Although we have covered lots of topics related to web testing in this chapter,
we mentioned even more things to consider in our discussion of the 
characteristics of web applications.

Many of them deserve their own chapter, but for now we will just give a quick summary.

### Usability and accessibility testing
Usability and accessibility testing are very closely related.
Traditionally, usability focuses more on "user-friendliness"; making the application pleasant and easy to use.
According to the W3C, accessibility is about "making content accessible to a wider range of people with disabilities, including blindness and low vision, deafness and hearing loss, learning disabilities, cognitive limitations, limited movement, speech disabilities, photosensitivity and combinations of these."

To know what to test for, it is useful to consult resources like the Web Content Accessibility Guidelines (WCAG).

You can use tools like "axe" to test your web pages for accessibility problems.
However, at this stage, such tests often still need to be done manually.
Install assistive technologies like a screen reader that reads aloud the pages
you load in your browser,
and see whether you can still use the application without using your eyes.
Or better yet, ask a user with a disability to use your application 
and help you with identifying issues in it.

### Load and performance testing
On the web, performance is very important. 
If users have to wait too long for your application to load,
they may lose interest and turn to somewhere else.
Also, you should make sure that your application still functions properly
when used by lots of people.

Various tools exist to put a lot of load on your system 
and help you find the parts with performance issues.


## References
* QUnit's Introduction to Unit Testing. https://qunitjs.com/intro/
* Vitali Zaidman. An Overview of JavaScript Testing in 2020. https://medium.com/welldone-software/an-overview-of-javascript-testing-7ce7298b9870
* React.Component. https://reactjs.org/docs/react-component.html
* React Testing Overview. https://reactjs.org/docs/testing.html#tools
* Jest Documentation. https://jestjs.io/docs/en/getting-started
* Kent C. Dodds. Making your UI tests resilient to change. https://kentcdodds.com/blog/making-your-ui-tests-resilient-to-change
* Kent C. Dodds. The Merits of Mocking. https://kentcdodds.com/blog/the-merits-of-mocking
* Martin Fowler. Page Object. https://martinfowler.com/bliki/PageObject.html
* van Deursen, A. (2015). Beyond Page Objects: Testing Web Applications with State Objects. ACM Queue, 13(6), 20.
* WebDriver (W3C Recommendation). https://www.w3.org/TR/webdriver1/
* Introduction to Cypress. https://docs.cypress.io/guides/core-concepts/introduction-to-cypress.html
* Web Content Accessibility Guidelines (WCAG) 2.1 (W3C Recommendation). https://www.w3.org/TR/WCAG21/
# Testing into context
# SQL Testing

As we discussed in the _Testing Pyramid_ chapter, parts of our system only make sense to be tested by means of integration testing. A common case for integration testing are classes that talk to databases. Business applications are often composed of many [Data Access Objects](https://en.wikipedia.org/wiki/Data_access_object) (DAOs) that perform complex SQL queries. A lot of business knowledge are encapsulated in these queries, requiring testers to spend some energy in making sure that produce the expected outcomes.

In this chapter, we discuss:

* What to test in a SQL query?
* How to write automated test cases for such queries
* Challenges and best practices

This chapter expects the reader to have some basic knowledge on SQL queries.

## What to test in a SQL query?

SQL is a robust language and contains a large number of different functions that developers can make use of. Let us simplify and see queries as a composition of predicates. See the following examples:

* `SELECT * FROM INVOICE WHERE VALUE < 50`
* `SELECT * FROM INVOICE I JOIN CUSTOMER C ON I.CUSTOMER_ID = C.ID WHERE C.COUNTRY = 'NL'`
* `SELECT * FROM INVOICE WHERE VALUE > 50 AND VALUE < 200`

In these examples, `value < 50`, `i.customer_id = c.id`, `c.country = 'NL'`, and `value > 50 and value < 200` are the predicates that compose the different queries. As a tester, a possible criteria is to exercise the different predicates and check whether the SQL query returns the expected results when predicates are evaluated to different results. 

Virtually all the testing techniques we have discussed in the _Testing Techniques_ part of this book can be applied here:

* Specification-based testing: These SQL queries emerge out of a requirement. A tester can analyse the requirements and derive equivalent partitions that need to be tested.
* Boundary analysis: Such programs have boundaries. Given that we can also expect boundaries to be places with a high bug probability, exercise them is therefore also important.
* Structural testing: Structurally-speaking, SQL queries contain predicates, and a tester might use the SQL's structure to derive test cases.

Let us focus on structural testing. If we look close to the third example, and try to make an analogy with what we discussed in structural testing, we see that the SQL query contains a single branch (`value > 50 and value < 200`), composed of two predicates (`value > 50` and `value < 200`). This means that there are four possible combinations of results in these two predicates: (TT), (TF), (FT), (FF). A tester might aim at: 

- Branch coverage: in this case, two tests, one that makes the overall decision to be evaluated to true, and one that makes the overall decision to be evaluated to false would be enough to achieve 100% branch coverage.
- Condition+Branch coverage: in this case, three tests would be enough to achieve 100% condition+branch coverage, e.g., T1=150, T2=40, T3=250.

In [A practical guide to SQL white-box testing](https://dl.acm.org/doi/pdf/10.1145/1147214.1147221), Tuya and colleague suggests five guidelines for designing SQL tests: 

1. **Adopting MC/DC for SQL conditions.** Decisions happen at three places in a SQL query: _join_, _where_ and _having_ conditions. Testers can make use of a criteria such as MC/DC to fully exercise its predicates.

1. **Adapting MC/DC for tackling with nulls**. Given that databases have a special way of handling/returning NULLs, any (coverage) criteria should be adapted to a three-valued logic (i.e., true, false, null). In other words, consider the possibility of values being null in your query.

1. **Category partitioning selected data**. SQL can be considered a sort of declarative specification, of which we can define partitions to be tested. Directly from their text:
	1. Rows that are retrieved: We include a test state to force the query to not select any row.
	1. Rows that are merged: The presence of unwanted duplicate rows in the output is a common failure in some queries. We include a test state in which identical rows are selected.
	1. Rows that are grouped: For each of the group-by columns, we design test states to obtain at least two different groups at the output, such that the value used for the grouping is the same, and all the other are different.
	1. Rows that are selected in a subquery: For each subquery, we include test states that return zero and more rows, with at least one null and two different values in the selected column. 
	1. Values that participate in aggregate functions: For each aggregate function (excluding count), we include at least one test state in which the function computes two equal values and another one that is different. 
	1. Other expressions: We also design test states for expressions involving the like predicate, date management, string management, data type conversions or other functions using category partitioning and boundary checking.

1. **Checking the outputs.** We should check not only the input domain, but also the output domain. SQL queries might return NULL in specific columns or empty sets, for example, which might make the rest of the program to break. 

1. **Checking the database constraints**. Databases have constraints. Testers should make sure these constraints are indeed enforced by the database.

As you can see, many things can go wrong in a SQL query. And it is part of a tester's job to make sure it does not happen.

{% hint style='tip' %}
For interested readers, in the [Full predicate coverage for testing SQL database queries](https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.424) paper, Tuya et al. propose a MC/DC criteria for SQL queries.
{% endhint %}

## How to write automated test cases for SQL queries

We can make use of JUnit to write SQL tests. After all, all we need is to (1) establish a connection with the database, (2) make sure the database is in the right initial state, (3) fire a SQL query, (4) check the output.

Imagine: 
* We have an `Invoice` table that is composed of a `name` (varchar, length 100) and a `value` (double).
* We have an `InvoiceDao` class that makes use of any API to communicate with the database. The precise API does not matter.
* This DAO performs three actions: `save()` that persists an invoice in a database, `all()` which returns all invoices in the database, and `allWithAtLeast` that returns all invoices with at least an specified value.
	* `all()` runs the following SQL query: `select * from invoice`
	* `allWithAtLeast()` runs: `select * from invoice where value >= ?`
	* `save()` runs `insert into invoice (name, value) values (?,?)`.
	* You may see a JDBC implementation of this `InvoiceDao` in our [code examples](https://github.com/sttp-book/code-examples/blob/master/src/main/java/tudelft/mocks/invoice/InvoiceDao.java) repository.

Take a look at this rather long JUnit test snippet (which you can also see in our [code examples](https://github.com/sttp-book/code-examples/blob/master/src/test/java/tudelft/mocks/invoice/InvoiceDaoIntegrationTest.java) repository:

```java
public class InvoiceDaoIntegrationTest {

    private final DatabaseConnection connection = new DatabaseConnection();
    private final InvoiceDao dao = new InvoiceDao(connection);

    @BeforeEach
    void cleanup() throws SQLException {
        /**
         * Let's clean up the table before the test runs.
         * That will avoid possible flaky tests.
         *
         * Note that doing a single 'truncate' here seems simple and enough for this exercise.
         * In large systems, you will probably want to encapsulate the 'reset database' logic
         * somewhere else. Or even make use of specific frameworks for that.
         */
        connection.getConnection().prepareStatement("truncate table invoice").execute();

        /**
         * Maybe you also want to double check if the cleaning operation
         * worked!
         */
        List<Invoice> invoices = dao.all();
        assertThat(invoices).isEmpty();
    }

    @AfterEach
    void close() {
        /**
         * Closing up the connection might also be something you do
         * at the end of each test.
         * Or maybe only at the end of the entire test suite, just to optimize.
         * (In practice, you should also use some connection pool, like C3P0,
         * to handle connections)
         */
        connection.close();
    }

    @Test
    void save() {
        final var inv1 = new InvoiceBuilder().build();
        final var inv2 = new InvoiceBuilder().build();

        dao.save(inv1);

        List<Invoice> afterSaving = dao.all();
        assertThat(afterSaving).containsExactlyInAnyOrder(inv1);

        dao.save(inv2);
        List<Invoice> afterSavingAgain = dao.all();
        assertThat(afterSavingAgain).containsExactlyInAnyOrder(inv1, inv2);
    }

    @Test
    void atLeast() {
        int value = 50;

        /**
         * Explore the boundary: value >= x
         * On point = x
         * Off point = x-1
         * In point = x + 1 (not really necessary, but it's cheap, and makes the
         *   test strategy easier to comprehend)
         */
        final var inv1 = new InvoiceBuilder().withValue(value - 1).build();
        final var inv2 = new InvoiceBuilder().withValue(value).build();
        final var inv3 = new InvoiceBuilder().withValue(value + 1).build();

        dao.save(inv1);
        dao.save(inv2);
        dao.save(inv3);

        List<Invoice> afterSaving = dao.allWithAtLeast(value);
        assertThat(afterSaving).containsExactlyInAnyOrder(inv2, inv3);
    }
}
```

Let us understand it:

* Before each test, a clean up operation happens. We clean the entire database to make sure our tests will not be flaky. It is easy to imagine that, if a database has unkwnown data, a SQL query will return unexpected results. Note that, in here, we are doing a simple `truncate table`. In more complex systems, you might want to extract this "reset database" logic to an specialized class (or even to make use of framework).
* After each class, we close the connection, to avoid connection leaks. In this example, a simple `Connection#close` suffices. In real life, you might want to use some professional connection pool (not only for your test code, but also for your production code!)
* The `save()` test method exercises both `save()` and `all()` methods. It inserts values to the database and ensures they are persisted correctly afterwards.
* The `atLeast` exercises the `allWithAtLeast` method. Note how it also exercises the boundaries of the `value>?` condition. 
* Observe how test data builders (in this case, exemplified by the `InvoiceBuilder` class) helps us in quickly building test data.

This test suite might be considered good enough for the current `InvoiceDao` class. Note that, by basically applying all the ideas we have seen before, we were able to write good SQL testing without much costs.

## Challenges and best practices

The example above was quite simple. Challenges might emerge once your SQL queries are highly complex. Some tips:

* **Make use of test data builders.** They will help you to quickly build the data structures you need.
* **Make use of good assertions APIs.** Asserting was easy in the example above as AssertJ makes our life easier. 
* **Minimize the required data**. Make sure the input data is minimized. You do not want to have to load hundreds of thousands of elements to exercise your SQL query (maybe you will want to do this to exercise other features of your database, like speed, but that is not the case here).
* **Build good test infrastructure**. In our example, it was simple to open a connection, to reset the database state, and etc, but that might become more complicated (or lenghty) once your database schema gets complicated. Invest on a test infrastructure to facilitate your SQL testing.
* **Take into consideration the schema evolution**. In real life, database schemas evolve quite fast. Make sure your test suite is resilient towards these changes (i.e., if an evolution should not break the test suite, it does not; if an evolution should break the test suite, it does break the test suite).
* **Consider an in-memory database**. You should decide whether your tests will communicate with a "real" database (i.e., the same database of your production environment) or a simpler database (e.g., an in-memory database). As always, both sides have advantages and disadvantages. Using a the same database as in production makes your tests more realistic, but probably slower than if you use an in-memory database.

## References

* Tuya, Javier, M. José Suárez-Cabal, and Claudio De La Riva. "A practical guide to SQL white-box testing." ACM SIGPLAN Notices 41, no. 4 (2006): 36-41.
